{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import altair as alt\n",
    "alt.data_transformers.enable('json')\n",
    "\n",
    "from sublimpy import utils\n",
    "import glob\n",
    "import pytz\n",
    "from scipy.signal import welch, csd\n",
    "from scipy.stats import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(\"/Users/elischwat/Development/data/sublimationofsnow/sosqc_fast/*.nc\")\n",
    "# file_list = [ f for f in file_list if '_20230113' in f]\n",
    "file_list = [ f for f in file_list if '_20230404' in f]\n",
    "\n",
    "# file_list = [ f for f in file_list if '_20221224' in f]\n",
    "\n",
    "# file_list = [ f for f in file_list if '_20230224' in f]\n",
    "# file_list = [ f for f in file_list if '_20230313' in f]\n",
    "file_list = sorted(file_list)[16:24]\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_vars = ['base_time']\n",
    "value_vars = [\n",
    "        'u_2m_c',\t'v_2m_c',\t'w_2m_c',\t'h2o_2m_c', 'tc_2m_c',\n",
    "        'u_3m_c',\t'v_3m_c',\t'w_3m_c',\t'h2o_3m_c', 'tc_3m_c',\n",
    "        'u_5m_c',\t'v_5m_c',\t'w_5m_c',\t'h2o_5m_c', 'tc_5m_c',\n",
    "        'u_10m_c',\t'v_10m_c',\t'w_10m_c',\t'h2o_10m_c', 'tc_10m_c',\n",
    "        'u_15m_c',\t'v_15m_c',\t'w_15m_c',\t'h2o_15m_c', 'tc_15m_c',\n",
    "        'u_20m_c',\t'v_20m_c',\t'w_20m_c',\t'h2o_20m_c', 'tc_20m_c',\n",
    "\n",
    "        'u_3m_uw',\t'v_3m_uw',\t'w_3m_uw',\t'h2o_3m_uw', 'tc_3m_uw',\n",
    "        'u_10m_uw',\t'v_10m_uw',\t'w_10m_uw',\t'h2o_10m_uw', 'tc_10m_uw',\n",
    "\n",
    "        'u_3m_ue',\t'v_3m_ue',\t'w_3m_ue',\t'h2o_3m_ue', 'tc_3m_ue',\n",
    "        'u_10m_ue',\t'v_10m_ue',\t'w_10m_ue',\t'h2o_10m_ue', 'tc_10m_ue',\n",
    "\n",
    "        'u_3m_d',\t'v_3m_d',\t'w_3m_d',\t'h2o_3m_d', 'tc_3m_d',\n",
    "        'u_10m_d',\t'v_10m_d',\t'w_10m_d',\t'h2o_10m_d', 'tc_10m_d',\n",
    "    ]\n",
    "VARIABLES = index_vars + value_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(\n",
    "    file_list, concat_dim=\"time\", \n",
    "    combine=\"nested\", \n",
    "    data_vars=VARIABLES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds[VARIABLES].to_dataframe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create timestamp\n",
    "To use the datam, its necessary to combine 3 columns of data from the dataset to get the full timestamp. This is demonstrated below. The 'time' column actually only incudes the second and minute information. For all datapoints, the hour according to the 'time' column is 1.  The 'base_time' column indicates the hour of the day. The 'sample' column indicates the 20hz sample number. \n",
    "\n",
    "We demonstrate this in the plots below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'] = df.apply(lambda row: dt.datetime(\n",
    "        year = row['time'].year,\n",
    "        month = row['time'].month,\n",
    "        day = row['time'].day,\n",
    "        hour = row['base_time'].hour,\n",
    "        minute = row['time'].minute,\n",
    "        second = row['time'].second,\n",
    "        microsecond = int(row['sample'] * (1e6/20))\n",
    "    ),\n",
    "    axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.modify_df_timezone(df, pytz.UTC, \"US/Mountain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolate nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in value_vars:\n",
    "    nans_b4_interp = df[var].isna().sum()\n",
    "    df[var] = df[var].interpolate()\n",
    "    nans_after_interp = df[var].isna().sum()\n",
    "    print(var, len(df), nans_b4_interp, nans_after_interp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate spectra of u'u', v'v', w'w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_ls = []\n",
    "for height in [2,3,5,10,20]:\n",
    "    for var in ['u', 'v', 'w']:\n",
    "        spectrum = pd.DataFrame(dict(zip(\n",
    "            ['frequency', 'power spectrum'],\n",
    "            list(welch(\n",
    "                    df[f\"{var}_{height}m_c\"],\n",
    "                    fs=20, #Hz\n",
    "                    window='hann', #'hann' is the default,\n",
    "                    nperseg=72000 # one hour window\n",
    "            ))\n",
    "        )))\n",
    "        spectrum = spectrum.assign(height = height)\n",
    "        spectrum = spectrum.assign(variance = f\"{var}'{var}'\")\n",
    "        spectrum_ls.append(spectrum)\n",
    "variance_spectrum_df = pd.concat(spectrum_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_study_period_in_seconds = (df.time.max() - df.time.min()).seconds\n",
    "length_of_window_in_seconds = 72000/20\n",
    "n_windows_in_study_period = length_of_study_period_in_seconds/length_of_window_in_seconds\n",
    "edof = 2*n_windows_in_study_period\n",
    "# Calculate confidence interval\n",
    "# what are these?\n",
    "conf_x = 1\n",
    "\n",
    "conf_y0 = 1\n",
    "# Degrees of freedom = 2 DOF per window, multiplied by number of windows\n",
    "conf = conf_y0 * edof / chi2.ppf([0.025, 0.975], edof).reshape((2,1))\n",
    "\n",
    "uncertainty_chart = alt.Chart().transform_calculate(\n",
    "    high = f\"{conf_y0 + conf[0][0]}\",\n",
    "    low = f\"{conf_y0 - conf[1][0]}\",\n",
    "    x = f\"{conf_x}\"\n",
    ").mark_line(color='black').encode(\n",
    "    alt.X(\"x:Q\").title(\"\"),\n",
    "    alt.Y(\"low:Q\").title(\"\"),\n",
    "    alt.Y2(\"high:Q\")\n",
    ")\n",
    "\n",
    "uncertainty_chart_dot = alt.Chart().transform_calculate(\n",
    "    middle = f\"{conf_y0}\",\n",
    "    x = f\"{conf_x}\"\n",
    ").mark_circle(color='black').encode(\n",
    "    alt.X(\"x:Q\").title(\"\"),\n",
    "    alt.Y(\"middle:Q\").title(\"\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a line with slope -5/3 (in log log space) that fits the data\n",
    "fit_chart = alt.Chart(pd.DataFrame({\n",
    "        'x': np.arange(0.01, 10),\n",
    "        'y': 0.01*np.arange(0.01, 10)**(-5/3)\n",
    "})).mark_line(color='black', strokeDash=[4,2]).encode(\n",
    "    alt.X('x:Q').scale(type='log'),\n",
    "    alt.Y('y:Q').scale(type='log'),\n",
    ")\n",
    "\n",
    "spectra_chart = alt.Chart().mark_line().encode(\n",
    "    alt.X(\"frequency:Q\").scale(type='log'),\n",
    "    alt.Y(\"power spectrum:Q\").scale(type='log'),\n",
    "    alt.Color(\"height:N\"),\n",
    ")\n",
    "\n",
    "alt.layer(\n",
    "    spectra_chart,\n",
    "    fit_chart,\n",
    "    uncertainty_chart,\n",
    "    uncertainty_chart_dot,\n",
    "    data=variance_spectrum_df.query(\"frequency > 0\")\n",
    ").properties(\n",
    "    width=200, \n",
    "    height=150\n",
    ").facet(\n",
    "    'variance:O'\n",
    ").configure_axis(grid=False).display(renderer='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_spectrum_df.query(\"frequency > 0\").to_csv(str(df.time.dt.date.iloc[0]) + '-spectra.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare w'w' spectra from different months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multipledays = pd.concat([\n",
    "    pd.read_csv(\"2022-12-24-spectra.csv\").assign(date = '2022-12-24'),\n",
    "    pd.read_csv(\"2023-01-13-spectra.csv\").assign(date = '2023-01-13'),\n",
    "    pd.read_csv(\"2023-02-24-spectra.csv\").assign(date = '2023-02-24'),\n",
    "    pd.read_csv(\"2023-03-13-spectra.csv\").assign(date = '2023-03-13')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt.Chart(\n",
    "#     df_multipledays[df_multipledays.variance == \"w'w'\"].query(\"height < 10\")\n",
    "# ).mark_line().encode(\n",
    "#     alt.X(\"frequency:Q\").scale(type='log'),\n",
    "#     alt.Y(\"power spectrum:Q\").scale(type='log'),\n",
    "#     alt.Color(\"height:N\"),\n",
    "#     alt.Column(\"date:O\"),\n",
    "# ).properties(width = 300, height = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_avg_spectra_df_ls = []\n",
    "for height in df_multipledays.height.unique():\n",
    "    for date in df_multipledays.date.unique():\n",
    "        src = df_multipledays[df_multipledays.variance == \"w'w'\"].query(f\"height == {height}\").query(f\"date == '{date}'\")\n",
    "        src['frequency_binned'] = pd.cut(\n",
    "            src['frequency'],\n",
    "            np.logspace(-5, 1, 100)\n",
    "        )\n",
    "        bin_avg_spectra = src.groupby(\"frequency_binned\")[['power spectrum']].mean().reset_index()\n",
    "        bin_avg_spectra['frequency'] = bin_avg_spectra['frequency_binned'].apply(lambda int: (int.left + int.right)/2)\n",
    "        bin_avg_spectra['height'] = height\n",
    "        bin_avg_spectra['date'] = date\n",
    "        bin_avg_spectra_df_ls.append(bin_avg_spectra)\n",
    "bin_avg_spectra_df = pd.concat(bin_avg_spectra_df_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = bin_avg_spectra_df[bin_avg_spectra_df.date.isin(['2022-12-24', '2023-03-13'])].query(\"height < 10\")\n",
    "src['date'] = src['date'].replace({\n",
    "    '2022-12-24': 'Dec. 24\\n(7cm of snow)',\n",
    "    '2023-03-13': 'Mar. 13\\n(126cm of snow)',\n",
    "})\n",
    "power_spectra_2m_is_too_low_chart = (\n",
    "    uncertainty_chart\n",
    "    +\n",
    "    uncertainty_chart_dot\n",
    "    +\n",
    "    alt.Chart(\n",
    "        src[['power spectrum', 'frequency', 'height', 'date']]\n",
    "    ).mark_line().encode(\n",
    "        alt.X(\"frequency:Q\").scale(domain = [0.001, 10], type='log').title(\"Frequency (Hz)\"),\n",
    "        alt.Y(\"power spectrum:Q\").scale(type='log').title(\"Cospectra(w'q')\"),\n",
    "        alt.Color(\"height:N\"),\n",
    "        alt.StrokeDash(\"date:O\"),\n",
    ").properties(width = 200, height = 200)\n",
    ").configure_axis(grid=False).configure_legend(orient='bottom-left', columns = 1\n",
    ")\n",
    "power_spectra_2m_is_too_low_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_spectra_2m_is_too_low_chart.save(\"../../figures/power_spectra_2m_is_too_low_chart.png\", ppi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = bin_avg_spectra_df[bin_avg_spectra_df.date.isin(['2022-12-24', '2023-03-13'])]\n",
    "src['date'] = src['date'].replace({\n",
    "    '2022-12-24': '2022-12-24 (7cm snowdepth)',\n",
    "    '2023-03-13': '2023-03-13 (126cm snowdepth)',\n",
    "})\n",
    "alt.Chart(\n",
    "    src\n",
    ").mark_line().encode(\n",
    "    alt.X(\"frequency:Q\").scale(type='log'),\n",
    "    alt.Y(\"power spectrum:Q\").scale(type='log'),\n",
    "    alt.Color(\"height:N\"),\n",
    "    alt.Column(\"date:O\"),\n",
    "    # alt.StrokeDash(\"date:O\"),\n",
    ").properties(width = 300, height = 200).configure_axis(grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate cospectra of u'w', w'tc', and w'h2o'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_df_list = []\n",
    "towers = ['c', 'd', 'uw', 'ue']\n",
    "for tower in towers:\n",
    "    if tower == 'c':\n",
    "        heights = [2, 3, 5, 10, 15, 20]\n",
    "    else:\n",
    "        heights = [3, 10]\n",
    "    for height in heights:\n",
    "        local_df = pd.DataFrame(dict(zip(\n",
    "            ['frequency', 'power spectrum'],\n",
    "            list(csd(\n",
    "                    np.sqrt(\n",
    "                        df[f'u_{height}m_{tower}']**2 + df[f'v_{height}m_{tower}']**2\n",
    "                    ),\n",
    "                    df[f'w_{height}m_{tower}'],\n",
    "                    fs=20, #Hz\n",
    "                    window='hann', #'hann' is the default,\n",
    "                    nperseg=72000\n",
    "            ))\n",
    "        ))).assign(height=height).assign(tower=tower)\n",
    "        # local_df['power spectrum'] = np.real(local_df['power spectrum'])\n",
    "        local_df['cospectrum'] = local_df['power spectrum'].apply(lambda complex: complex.real)\n",
    "        local_df['quadrature spectrum'] = local_df['power spectrum'].apply(lambda complex: complex.imag)\n",
    "        local_df_list.append(local_df)\n",
    "momentum_copower_spectrum = pd.concat(local_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_df_list = []\n",
    "towers = ['c', 'd', 'uw', 'ue']\n",
    "for tower in towers:\n",
    "    if tower == 'c':\n",
    "        heights = [2, 3, 5, 10, 15, 20]\n",
    "    else:\n",
    "        heights = [3, 10]\n",
    "    for height in heights:\n",
    "        local_df = pd.DataFrame(dict(zip(\n",
    "            ['frequency', 'power spectrum'],\n",
    "            list(csd(\n",
    "                    df[f'w_{height}m_{tower}'],\n",
    "                    df[f'tc_{height}m_{tower}'],\n",
    "                    fs=20, #Hz\n",
    "                    window='hann', #'hann' is the default,\n",
    "                    nperseg=72000\n",
    "            ))\n",
    "        ))).assign(height=height).assign(tower=tower)\n",
    "        local_df['cospectrum'] = local_df['power spectrum'].apply(lambda complex: complex.real)\n",
    "        local_df['quadrature spectrum'] = local_df['power spectrum'].apply(lambda complex: complex.imag)\n",
    "        local_df_list.append(local_df)\n",
    "sensheat_copower_spectrum = pd.concat(local_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_df_list = []\n",
    "towers = ['c', 'd', 'uw', 'ue']\n",
    "for tower in towers:\n",
    "    if tower == 'c':\n",
    "        heights = [2, 3, 5, 10, 15, 20]\n",
    "    else:\n",
    "        heights = [3, 10]\n",
    "    for height in heights:\n",
    "        local_df = pd.DataFrame(dict(zip(\n",
    "            ['frequency', 'power spectrum'],\n",
    "            list(csd(\n",
    "                    df[f'w_{height}m_{tower}'],\n",
    "                    df[f'h2o_{height}m_{tower}'],\n",
    "                    fs=20, #Hz\n",
    "                    window='hann', #'hann' is the default,\n",
    "                    nperseg=72000\n",
    "            ))\n",
    "        ))).assign(height=height).assign(tower=tower)\n",
    "        local_df['cospectrum'] = local_df['power spectrum'].apply(lambda complex: complex.real)\n",
    "        local_df['quadrature spectrum'] = local_df['power spectrum'].apply(lambda complex: complex.imag)\n",
    "        local_df_list.append(local_df)\n",
    "latheat_copower_spectrum = pd.concat(local_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(\n",
    "    momentum_copower_spectrum.query(\"tower == 'c'\").query(\"frequency > 0\")\n",
    ").mark_line().encode(\n",
    "    alt.X(\"frequency:Q\").scale(type='log'),\n",
    "    alt.Y(\"cospectrum:Q\"),\n",
    "    alt.Color(\"height:N\")\n",
    ").properties(width=300, height=150, title=\"u'w'\") |\\\n",
    "alt.Chart(\n",
    "    sensheat_copower_spectrum.query(\"tower == 'c'\").query(\"frequency > 0\")\n",
    ").mark_line().encode(\n",
    "    alt.X(\"frequency:Q\").scale(type='log'),\n",
    "    alt.Y(\"cospectrum:Q\"),\n",
    "    alt.Color(\"height:N\")\n",
    ").properties(width=300, height=150, title=\"w'tc'\")  |\\\n",
    "alt.Chart(\n",
    "    latheat_copower_spectrum.query(\"tower == 'c'\").query(\"frequency > 0\")\n",
    ").mark_line().encode(\n",
    "    alt.X(\"frequency:Q\").scale(type='log'),\n",
    "    alt.Y(\"cospectrum:Q\"),\n",
    "    alt.Color(\"height:N\")\n",
    ").properties(width=300, height=150, title=\"w'h2o'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latheat_copower_spectrum.query(\"tower == 'c'\").query(\"frequency > 0\").query(\n",
    "        \"frequency < 0.01\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(\n",
    "    latheat_copower_spectrum.query(\"tower == 'c'\").query(\"frequency > 0\").query(\n",
    "        \"frequency < 0.01\"\n",
    "    )\n",
    ").mark_line().encode(\n",
    "    alt.X(\"frequency:Q\").scale(type='log'),\n",
    "    alt.Y(\"cospectrum:Q\").scale(type='symlog'),\n",
    "    alt.Color(\"height:N\")\n",
    ").properties(width=300, height=150, title=\"w'h2o'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(\n",
    "    latheat_copower_spectrum.query(\"tower == 'c'\").query(\"frequency > 0\").query(\"height < 10\")\n",
    ").mark_line().encode(\n",
    "    alt.X(\"frequency:Q\").scale(type='log'),\n",
    "    alt.Y(\"power spectrum:Q\"),\n",
    "    alt.Color(\"height:N\")\n",
    ").properties(width=300, height=150, title=\"w'h2o'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(\n",
    "    sensheat_copower_spectrum[\n",
    "        sensheat_copower_spectrum.height.isin([3,10])\n",
    "    ].query(\"frequency > 0\")\n",
    ").mark_line().encode(\n",
    "    alt.X(\"frequency:Q\").scale(type='log'),\n",
    "    alt.Y(\"power spectrum:Q\"),\n",
    "    alt.Color(\"height:N\"),\n",
    "    alt.Facet(\"tower:N\", columns=2),\n",
    ").properties(width=200, height=100, title=\"w'tc'\").display(renderer='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiresolution decomposition\n",
    "\n",
    "Following Michi Haugeneders PHD Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '20221101'\n",
    "end_date = '20230619'\n",
    "# open files\n",
    "# tidy_df = pd.read_parquet(f'tidy_df_{start_date}_{end_date}_noplanar_fit_clean.parquet')\n",
    "tidy_df = pd.read_parquet(f'tidy_df_{start_date}_{end_date}_planar_fit_multiplane.parquet')\n",
    "# convert time column to datetime\n",
    "tidy_df['time'] = pd.to_datetime(tidy_df['time'])\n",
    "tidy_df = utils.modify_df_timezone(tidy_df, pytz.UTC, 'US/Mountain')\n",
    "# limit data to our dates of interest, based on continuous snow cover at Kettle Ponds\n",
    "tidy_df = tidy_df.set_index('time').sort_index().loc[\n",
    "    '20221130':'20230508'\n",
    "].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify lists of timestamps for different categories\n",
    "bs_times = set(\n",
    "    tidy_df.query(\"variable == 'SF_avg_1m_ue'\").query(\"value > 0\").time\n",
    ").union(\n",
    "    set(tidy_df.query(\"variable == 'SF_avg_2m_ue'\").query(\"value > 0\").time)\n",
    ")\n",
    "nobs_times = set(tidy_df.time).difference(bs_times)\n",
    "\n",
    "ri_stable_times = tidy_df.query(\"variable == 'Ri_3m_c'\").query(\"value > 0.25\").time\n",
    "ri_unstable_times = tidy_df.query(\"variable == 'Ri_3m_c'\").query(\"value < -0.01\").time\n",
    "ri_neutral_times = tidy_df.query(\"variable == 'Ri_3m_c'\").query(\"value >= -0.01\").query(\"value <= 0.25\").time\n",
    "\n",
    "tgrad_stable_times = tidy_df.query(\"variable == 'temp_gradient_3m_c'\").query(\"value > 0.01\").time\n",
    "tgrad_unstable_times = tidy_df.query(\"variable == 'temp_gradient_3m_c'\").query(\"value < -0.01\").time\n",
    "tgrad_neutral_times = tidy_df.query(\"variable == 'temp_gradient_3m_c'\").query(\"value >= -0.01\").query(\"value <= 0.01\").time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = tidy_df.query(\"measurement == 'potential virtual temperature'\")\n",
    "src = src[src.time > '20230404 1000'][src.time < '20230404 1700']\n",
    "alt.Chart(src).mark_line().encode(\n",
    "    alt.X('mean(value):Q').sort('-y'),\n",
    "    alt.Y('height:Q'),\n",
    "    alt.Facet('hours(time):T', columns=4)\n",
    ").properties(width=150, height = 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Michi's/Ivana's implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some brief examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\"\"\"\n",
    "    mrd(data_a::Vector, data_b::Vector, M::Integer, Mx::Integer)\n",
    "\n",
    "Multiresolution Flux Decomposition. Adapted from Ivana Stiperski's code.\n",
    "See Vickers&Mahrt 2003 'The Cospectral Gap and Turbulent Flux Calculations'\n",
    "\"\"\"\n",
    "def mrd(data_a, data_b, M, Mx):\n",
    "    D = np.zeros(M - Mx)\n",
    "    data_a2 = data_a.copy()\n",
    "    data_b2 = data_b.copy()\n",
    "    for ims in range(M - Mx + 1):\n",
    "        ms = M - ims\n",
    "        l = 2 ** ms\n",
    "        nw = round((2 ** M) / l)\n",
    "        sumab = 0.0\n",
    "        for i in range(nw):\n",
    "            k = i * l #startidx of averaging segment\n",
    "            za = data_a2[k]\n",
    "            zb = data_b2[k]\n",
    "            for j in range(k + 1, k + l): #iterate over averaging segment\n",
    "                za += data_a2[j]\n",
    "                zb += data_b2[j]\n",
    "            za /= l\n",
    "            zb /= l\n",
    "            sumab += za * zb\n",
    "            for j in range(k, i * l + 1): #substract mean for next step\n",
    "                data_a2[j] -= za\n",
    "                data_b2[j] -= zb\n",
    "        if nw > 1:\n",
    "            D[ms] = sumab / nw\n",
    "    return D\n",
    "\n",
    "def newmrd(data_a, data_b, M, Mx):\n",
    "    D = np.zeros(M - Mx)\n",
    "    Dstd = np.copy(D)\n",
    "    data_a2 = np.copy(data_a)\n",
    "    data_b2 = np.copy(data_b)\n",
    "    for ims in range(M - Mx + 1):\n",
    "        ms = M - ims\n",
    "        l = 2 ** ms\n",
    "        nw = round((2 ** M) / l)\n",
    "        wmeans_a = np.zeros(nw)\n",
    "        wmeans_b = np.copy(wmeans_a)\n",
    "        for i in range(nw):\n",
    "            k = round(i * l)\n",
    "            wmeans_a[i] = np.mean(data_a2[k:(i+1)*l])\n",
    "            wmeans_b[i] = np.mean(data_b2[k:(i+1)*l])\n",
    "            data_a2[k:(i+1)*l] -= wmeans_a[i]\n",
    "            data_b2[k:(i+1)*l] -= wmeans_b[i]\n",
    "        if nw > 1:\n",
    "            D[ms] = np.mean(wmeans_a * wmeans_b)\n",
    "            Dstd[ms] = np.std(wmeans_a * wmeans_b, ddof=0)\n",
    "    return D, Dstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 19\n",
    "\n",
    "mrd_src = df[['time','u_5m_c', 'v_5m_c', 'w_5m_c', 'tc_5m_c']]\n",
    "mrd_src = mrd_src.rename(columns={\n",
    "    'u_5m_c': 'u',\n",
    "    'v_5m_c': 'v',\n",
    "    'w_5m_c': 'w',\n",
    "    'tc_5m_c': 'T'\n",
    "})\n",
    "mrd_src = mrd_src.head(2**M)\n",
    "assert len(mrd_src) == 2**M\n",
    "\n",
    "mrd_src_interpolated = mrd_src.copy()\n",
    "mrd_src_interpolated['w'] = mrd_src_interpolated['w'].interpolate()\n",
    "mrd_src_interpolated['T'] = mrd_src_interpolated['T'].interpolate()\n",
    "result =        mrd(mrd_src_interpolated['w'], mrd_src_interpolated['T'], M, 0)\n",
    "result_orth =   newmrd(mrd_src_interpolated['w'], mrd_src_interpolated['T'], M, 0)\n",
    "\n",
    "\n",
    "timestep = (mrd_src_interpolated['time'].iloc[1] - mrd_src_interpolated['time'].iloc[0]).total_seconds() * 1000\n",
    "mrd_x = np.array([dt.timedelta(milliseconds=2**i * timestep).total_seconds() for i in range(1, M+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({\n",
    "    'tau':      mrd_x,\n",
    "    'Co':       result,\n",
    "    'Co_orth':  result_orth[0],\n",
    "    'std_orth': result_orth[1]\n",
    "})\n",
    "\n",
    "(alt.Chart(result_df).mark_line().encode(\n",
    "    alt.X('tau:Q').scale(type='log').title('tau (s)'),\n",
    "    alt.Y('Co_orth:Q').title('C_wT (K m/s)')\n",
    ") + alt.Chart(result_df).mark_errorband().transform_calculate(\n",
    "    upper = 'datum.Co_orth + datum.std_orth',\n",
    "    lower = 'datum.Co_orth - datum.std_orth'\n",
    ").encode(\n",
    "    alt.X('tau:Q').scale(type='log'),\n",
    "    alt.Y('lower:Q').title('').scale(domain=[-0.005,0.005], clamp=True),\n",
    "    alt.Y2('upper:Q'),\n",
    ").properties(\n",
    "    title = [f\"{str(mrd_src_interpolated.time.dt.date.iloc[0])}, \" + \n",
    "    f\"0{mrd_src_interpolated.time.dt.hour.min()}00 - \" +\n",
    "    f\"{mrd_src_interpolated.time.dt.hour.max()}00\",\n",
    "    '(shading shows ¬± 1*œÉ)'\n",
    "    ]\n",
    ").properties(width=200, height=200)).display(renderer='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large scale analysis (precomputed spectra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /Users/elischwat/Development/data/sublimationofsnow/mrd/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_mrds = pd.read_parquet(\"/Users/elischwat/Development/data/sublimationofsnow/mrd/0900_1700/sensible_heat\")\n",
    "lh_mrds = pd.read_parquet(\"/Users/elischwat/Development/data/sublimationofsnow/mrd/0900_1700/latent_heat\")\n",
    "\n",
    "sh_mrds = sh_mrds[(sh_mrds['date'] >= '20221130') & (sh_mrds['date'] < '20230509')]\n",
    "lh_mrds = lh_mrds[(lh_mrds['date'] >= '20221130') & (lh_mrds['date'] < '20230509')]\n",
    "sh_mrds.head(3), lh_mrds.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempgrad_3m = tidy_df.query(\"variable == 'temp_gradient_2m_c'\")\n",
    "tempgrad_3m['date_str'] = tempgrad_3m.time.dt.strftime('%Y%m%d')\n",
    "tempgrad_3m = tempgrad_3m[tempgrad_3m.date_str.isin(sh_mrds.date)]\n",
    "tempgrad_3m = tempgrad_3m[tempgrad_3m.time.dt.hour.isin([9,10,11,12,13,14,15,16,17])]\n",
    "tempgrad_3m = tempgrad_3m.groupby('date_str').value.mean()\n",
    "alt.Chart(pd.DataFrame(tempgrad_3m)).mark_bar().encode(\n",
    "    alt.X('value:Q').bin(True), alt.Y('count():Q')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_mrds = sh_mrds.merge(\n",
    "    tempgrad_3m,\n",
    "    how='left',\n",
    "    left_on='date',\n",
    "    right_on='date_str'\n",
    ")\n",
    "lh_mrds = lh_mrds.merge(\n",
    "    tempgrad_3m,\n",
    "    how='left',\n",
    "    left_on='date',\n",
    "    right_on='date_str'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(\n",
    "    sh_mrds[sh_mrds.date > '20221130'][sh_mrds.height.isin([3,10, 20])]\n",
    ").mark_line(opacity=0.25).encode(\n",
    "    alt.X('tau:Q').scale(type='log'),\n",
    "    alt.Y('Co:Q').scale(domain = [-0.02, 0.02], clamp=True),\n",
    "    alt.Color('value:Q').scale(scheme='purpleorange', domain = [-0.8, 0.8]),\n",
    "    alt.Detail('date:O'),\n",
    "    alt.Column('tower:N'),\n",
    "    alt.Row('height:O')\n",
    ").configure_axis(grid=False).properties(height = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time\n",
       "2022-11-30    6\n",
       "2022-12-01    2\n",
       "2022-12-04    1\n",
       "2022-12-06    5\n",
       "2022-12-09    1\n",
       "             ..\n",
       "2023-04-21    4\n",
       "2023-04-22    3\n",
       "2023-04-23    3\n",
       "2023-04-25    7\n",
       "2023-04-29    1\n",
       "Name: time, Length: 89, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ri_unstable_times.groupby(ri_unstable_times.dt.date).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = alt.Chart().transform_calculate(y = '0').mark_rule().encode(y='y:Q')\n",
    "\n",
    "src = sh_mrds[sh_mrds.height.isin([3,5, 10, 15, 20])].query(\"tower == 'c'\")\n",
    "src['stability'] = src.value.apply(lambda delta_t: 'stable' if delta_t > 0 else 'unstable')\n",
    "mean_src = pd.DataFrame(src.groupby(['tau', 'height', 'tower', 'stability'])['Co'].mean())\n",
    "stddev_src = pd.DataFrame(src.groupby(['tau', 'height', 'tower', 'stability'])['Co'].std())\n",
    "quantiles_src = pd.DataFrame(src.groupby(['tau', 'height', 'tower', 'stability'])['Co'].agg([q1, q3]))\n",
    "mean_src = mean_src.rename(columns={'Co': 'Co mean'})\n",
    "stddev_src = stddev_src.rename(columns={'Co': 'Co std'})\n",
    "src = mean_src.join(stddev_src).join(quantiles_src).reset_index()\n",
    "\n",
    "src['Co lower'] = src['Co mean'] - src['Co std']\n",
    "src['Co upper'] = src['Co mean'] + src['Co std']\n",
    "\n",
    "shflux_chart = alt.layer(\n",
    "    alt.Chart().mark_rule(strokeDash=[2,2], color='red').transform_calculate(\n",
    "        x = '1800'\n",
    "    ).encode(alt.X('x:Q').title('')),\n",
    "    alt.Chart().mark_errorband().encode(\n",
    "        alt.X('tau:Q').scale(type='log').title('ùúè (s)'),\n",
    "        alt.Y('q1:Q').scale(domain=[-0.0025,0.0025], clamp=True).title(\"Co(w'T')\"),\n",
    "        alt.Y2('q3:Q'),\n",
    "        alt.Color('stability:N')\n",
    "    ),\n",
    "    alt.Chart().mark_line().encode(\n",
    "        alt.X('tau:Q').scale(type='log'),\n",
    "        alt.Y('Co mean:Q').scale(domain=[-0.0025,0.0025], clamp=True).title(\"Co(w'T')\"),\n",
    "        alt.Color('stability:N')\n",
    "    ),\n",
    "    line,\n",
    "    data = src\n",
    ").properties(height = 100, width = 200).facet(\n",
    "    'height:O'\n",
    ").properties(title='Sensible heat flux, multiresolution decomposition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = alt.Chart().transform_calculate(y = '0').mark_rule().encode(y='y:Q')\n",
    "\n",
    "src = lh_mrds[lh_mrds.height.isin([3,5, 10, 15, 20])].query(\"tower == 'c'\")\n",
    "src['stability'] = src.value.apply(lambda delta_t: 'stable' if delta_t > 0 else 'unstable')\n",
    "mean_src = pd.DataFrame(src.groupby(['tau', 'height', 'tower', 'stability'])['Co'].mean())\n",
    "stddev_src = pd.DataFrame(src.groupby(['tau', 'height', 'tower', 'stability'])['Co'].std())\n",
    "quantiles_src = pd.DataFrame(src.groupby(['tau', 'height', 'tower', 'stability'])['Co'].agg([q1, q3]))\n",
    "mean_src = mean_src.rename(columns={'Co': 'Co mean'})\n",
    "stddev_src = stddev_src.rename(columns={'Co': 'Co std'})\n",
    "src = mean_src.join(stddev_src).join(quantiles_src).reset_index()\n",
    "\n",
    "src['Co lower'] = src['Co mean'] - src['Co std']\n",
    "src['Co upper'] = src['Co mean'] + src['Co std']\n",
    "\n",
    "lhflux_chart = alt.layer(\n",
    "    alt.Chart().mark_rule(strokeDash=[2,2], color='red').transform_calculate(\n",
    "        x = '1800'\n",
    "    ).encode(alt.X('x:Q').title('')),\n",
    "    alt.Chart().mark_errorband().encode(\n",
    "        alt.X('tau:Q').scale(type='log').title('ùúè (s)'),\n",
    "        alt.Y('q1:Q').scale(domain=[-0.0025,0.0025], clamp=True).title(\"Co(w'q')\"),\n",
    "        alt.Y2('q3:Q'),\n",
    "        alt.Color('stability:N')\n",
    "    ),\n",
    "    alt.Chart().mark_line().encode(\n",
    "        alt.X('tau:Q').scale(type='log'),\n",
    "        alt.Y('Co mean:Q').scale(domain=[-0.0025,0.0025], clamp=True).title(\"Co(w'q')\"),\n",
    "        alt.Color('stability:N')\n",
    "    ),\n",
    "    line,\n",
    "    data = src\n",
    ").properties(height = 100, width = 200).facet(\n",
    "    'height:O', \n",
    ").properties(title='Lensible heat flux, multiresolution decomposition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-f1ecceb268d646f5ae269f2d47cc5a9e.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-f1ecceb268d646f5ae269f2d47cc5a9e.vega-embed details,\n",
       "  #altair-viz-f1ecceb268d646f5ae269f2d47cc5a9e.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-f1ecceb268d646f5ae269f2d47cc5a9e\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-f1ecceb268d646f5ae269f2d47cc5a9e\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-f1ecceb268d646f5ae269f2d47cc5a9e\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"vconcat\": [{\"data\": {\"url\": \"altair-data-ee3a9585d4d32af23a661c19d297b4e6.json\", \"format\": {\"type\": \"json\"}}, \"facet\": {\"field\": \"height\", \"type\": \"ordinal\"}, \"spec\": {\"layer\": [{\"mark\": {\"type\": \"rule\", \"color\": \"red\", \"strokeDash\": [2, 2]}, \"encoding\": {\"x\": {\"field\": \"x\", \"title\": \"\", \"type\": \"quantitative\"}}, \"transform\": [{\"calculate\": \"1800\", \"as\": \"x\"}]}, {\"mark\": {\"type\": \"errorband\"}, \"encoding\": {\"color\": {\"field\": \"stability\", \"type\": \"nominal\"}, \"x\": {\"field\": \"tau\", \"scale\": {\"type\": \"log\"}, \"title\": \"\\ud835\\udf0f (s)\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"q1\", \"scale\": {\"domain\": [-0.0025, 0.0025], \"clamp\": true}, \"title\": \"Co(w'T')\", \"type\": \"quantitative\"}, \"y2\": {\"field\": \"q3\"}}}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"stability\", \"type\": \"nominal\"}, \"x\": {\"field\": \"tau\", \"scale\": {\"type\": \"log\"}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"Co mean\", \"scale\": {\"domain\": [-0.0025, 0.0025], \"clamp\": true}, \"title\": \"Co(w'T')\", \"type\": \"quantitative\"}}}, {\"mark\": {\"type\": \"rule\"}, \"encoding\": {\"y\": {\"field\": \"y\", \"type\": \"quantitative\"}}, \"transform\": [{\"calculate\": \"0\", \"as\": \"y\"}]}], \"height\": 100, \"width\": 200}, \"title\": \"Sensible heat flux, multiresolution decomposition\"}, {\"data\": {\"url\": \"altair-data-f7411a044266433ff2d9e880505a5ef7.json\", \"format\": {\"type\": \"json\"}}, \"facet\": {\"field\": \"height\", \"type\": \"ordinal\"}, \"spec\": {\"layer\": [{\"mark\": {\"type\": \"rule\", \"color\": \"red\", \"strokeDash\": [2, 2]}, \"encoding\": {\"x\": {\"field\": \"x\", \"title\": \"\", \"type\": \"quantitative\"}}, \"transform\": [{\"calculate\": \"1800\", \"as\": \"x\"}]}, {\"mark\": {\"type\": \"errorband\"}, \"encoding\": {\"color\": {\"field\": \"stability\", \"type\": \"nominal\"}, \"x\": {\"field\": \"tau\", \"scale\": {\"type\": \"log\"}, \"title\": \"\\ud835\\udf0f (s)\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"q1\", \"scale\": {\"domain\": [-0.0025, 0.0025], \"clamp\": true}, \"title\": \"Co(w'q')\", \"type\": \"quantitative\"}, \"y2\": {\"field\": \"q3\"}}}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"stability\", \"type\": \"nominal\"}, \"x\": {\"field\": \"tau\", \"scale\": {\"type\": \"log\"}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"Co mean\", \"scale\": {\"domain\": [-0.0025, 0.0025], \"clamp\": true}, \"title\": \"Co(w'q')\", \"type\": \"quantitative\"}}}, {\"mark\": {\"type\": \"rule\"}, \"encoding\": {\"y\": {\"field\": \"y\", \"type\": \"quantitative\"}}, \"transform\": [{\"calculate\": \"0\", \"as\": \"y\"}]}], \"height\": 100, \"width\": 200}, \"title\": \"Lensible heat flux, multiresolution decomposition\"}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\"}, {\"renderer\": \"svg\", \"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.VConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(shflux_chart & lhflux_chart).display(renderer='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-bb0c5fa90f9d4cb4973cc6d1e2e2c20b.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-bb0c5fa90f9d4cb4973cc6d1e2e2c20b.vega-embed details,\n",
       "  #altair-viz-bb0c5fa90f9d4cb4973cc6d1e2e2c20b.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-bb0c5fa90f9d4cb4973cc6d1e2e2c20b\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-bb0c5fa90f9d4cb4973cc6d1e2e2c20b\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-bb0c5fa90f9d4cb4973cc6d1e2e2c20b\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"url\": \"altair-data-cc40599dcefd6367dbf60bb12d5600dd.json\", \"format\": {\"type\": \"json\"}}, \"facet\": {\"field\": \"stability\", \"type\": \"nominal\"}, \"spec\": {\"layer\": [{\"mark\": {\"type\": \"line\", \"point\": true}, \"encoding\": {\"color\": {\"field\": \"height\", \"scale\": {\"scheme\": \"turbo\"}, \"type\": \"ordinal\"}, \"x\": {\"field\": \"tau\", \"scale\": {\"type\": \"log\"}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"Co mean\", \"title\": \"Cumulative sum of Co(w'q')\", \"type\": \"quantitative\"}}}, {\"mark\": {\"type\": \"rule\", \"color\": \"red\", \"strokeDash\": [2, 2]}, \"encoding\": {\"x\": {\"field\": \"x\", \"title\": \"\", \"type\": \"quantitative\"}}, \"transform\": [{\"calculate\": \"1800\", \"as\": \"x\"}]}], \"height\": 200, \"width\": 200}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\"}, {\"renderer\": \"svg\", \"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alt.layer(\n",
    "    alt.Chart().mark_line(point=True).encode(\n",
    "        alt.X('tau:Q').scale(type='log'),\n",
    "        alt.Y('Co mean:Q').title(\"Cumulative sum of Co(w'q')\"),\n",
    "        alt.Color('height:O').scale(scheme='turbo'),\n",
    "    ),\n",
    "    alt.Chart().mark_rule(strokeDash=[2,2], color='red').transform_calculate(\n",
    "        x = '1800'\n",
    "    ).encode(alt.X('x:Q').title('')),\n",
    "    data = mean_src.groupby(['height', 'tower', 'stability']).cumsum().reset_index().query(\"tower == 'c'\")\n",
    ").properties(width=200, height = 200).facet('stability:N').display(renderer='svg')\n",
    "# .transform_filter(\n",
    "#         # alt.FieldOneOfPredicate('height', [3,10,20])\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the equations 3.3 and 3.4 in Michi's Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def u_mean(m, M, ures_i):\n",
    "    u_n_m_values =  []\n",
    "    n_values = range(1, 2**(M - m))\n",
    "    for n in n_values:\n",
    "        indices = np.array(list(range(\n",
    "            (n-1)*2**m + 1,\n",
    "            n*2**m\n",
    "        ))) - 1\n",
    "        u_n_m_values.append(\n",
    "            (1 / (2**m))*np.sum(ures_i[indices])\n",
    "        )\n",
    "    return u_n_m_values\n",
    "\n",
    "def cospectra(M, m, u_n_of_m, tc_n_of_m):\n",
    "    indices = np.array(list(range(\n",
    "        1,\n",
    "        2**(M-m)\n",
    "    ))) - 1\n",
    "    product = np.array(u_n_of_m)*np.array(tc_n_of_m)\n",
    "    return (1 / 2**(M - m))*np.sum(product[indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the recursive solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the lists of results we will store.\n",
    "u_n_of_m = {}\n",
    "tc_n_of_m = {}\n",
    "C_of_m = {}\n",
    "\n",
    "# initialize M\n",
    "M = 19\n",
    "# initialize the list of m values, we go from high to low,\n",
    "m_list = list(reversed(range(0, M+1)))\n",
    "#initialize series\n",
    "w_series = mrd_src['w_3m_c']\n",
    "tc_series = mrd_src['tc_3m_c']\n",
    "\n",
    "# Set the first m value\n",
    "m = m_list[0]\n",
    "print(m)\n",
    "\n",
    "# An MRD is obtained recursively starting with the average over the complete time series (m = M ; no CuœÜ(M) is calculated). \n",
    "w_segment_avg = np.mean(w_series)\n",
    "tc_segment_avg = np.mean(tc_series)\n",
    "\n",
    "# Subsequently, the average is removed from the time series to obtain uri(M ‚àí 1) \n",
    "u_res_i = w_series - w_segment_avg\n",
    "tc_res_i = tc_series - tc_segment_avg\n",
    "# and (3.3) and (3.4) are evaluated for m = M ‚àí 1. \n",
    "\n",
    "# calculate the first u_n(m) for m = 19\n",
    "u_n_of_m[m] = u_mean(m, M, u_res_i)\n",
    "tc_n_of_m[m] = u_mean(m, M, tc_res_i)\n",
    "\n",
    "# s.groupby(np.arange(len(s)) // (len(s)/2)).transform('min')\n",
    "\n",
    "# now m = 18, and counting down\n",
    "for m in m_list[1:]:\n",
    "    print(m)\n",
    "    # so we need a new residual term, u_res_i(m) where segment avgs of width >2^m have been subtracted\n",
    "    w_segment_avg = w_series.groupby(np.arange(len(w_series)) // (len(w_series)/2**(M-18))).transform('mean')\n",
    "    tc_segment_avg = tc_series.groupby(np.arange(len(w_series)) // (len(w_series)/2**(M-18))).transform('mean')\n",
    "\n",
    "    # update the residual calculations\n",
    "    u_res_i = u_res_i - w_segment_avg\n",
    "    tc_res_i = tc_res_i - tc_segment_avg\n",
    "    \n",
    "    # calculate both eqwuations now\n",
    "    u_n_of_m[m] = u_mean(m, M, u_res_i)\n",
    "    tc_n_of_m[m] = u_mean(m, M, tc_res_i)\n",
    "    C_of_m[m+1] = cospectra(M, m, u_n_of_m[m],tc_n_of_m[m])\n",
    "\n",
    "    # replace series, so we calculate the residuals at the next level\n",
    "    # w_series = ???\n",
    "    # tc_series = ???\n",
    "\n",
    "\n",
    "    # At last, CuœÜ(0) is determined by the fluctuations of single data points about 2-point means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total time in seconds of the time series we are using:')\n",
    "print(len(mrd_src)/20, 'seconds')\n",
    "print(len(mrd_src)/20/60, 'minutes')\n",
    "print(len(mrd_src)/20/60/60, 'hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrd_series = pd.DataFrame(pd.Series(C_of_m, name='Co'))\n",
    "mrd_series['tau'] = (\n",
    "    len(mrd_src)/20\n",
    ")/2**(M - mrd_series.index+1)\n",
    "mrd_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(mrd_series).mark_line().encode(\n",
    "    alt.X('tau:Q').scale(type='log'),\n",
    "    alt.Y('Co:Q')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls | grep csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"mrd__20230404_16_24.csv\"\n",
    ")\n",
    "df\n",
    "alt.Chart(df).mark_line().encode(\n",
    "    alt.X('tau').scale(type='log'),\n",
    "    alt.Y('Co'),\n",
    "    alt.Color('vars')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.pivot_table(index='tau', values='Co', columns='vars')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(normalized_df.reset_index()).transform_fold(\n",
    "    list(normalized_df.columns),\n",
    "    as_ = ['vars', 'Co']\n",
    ").mark_line().encode(\n",
    "    alt.X('tau:Q').scale(type='log'),\n",
    "    alt.Y('Co:Q'),\n",
    "    alt.Color('vars:N')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2b11a00ad1b97cabcd9cc9209b8824a0fcaf6ffe37b5243943912873b5dcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
