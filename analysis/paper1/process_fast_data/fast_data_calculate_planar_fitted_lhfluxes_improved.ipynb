{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "from sublimpy import utils, extrautils\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize parameters, file inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base path for a number of different directories this script needs\n",
    "DATA_DIR = \"/storage/elilouis/\"\n",
    "# path to directory where daily files are stored\n",
    "OUTPUT_PATH = f\"{DATA_DIR}sublimationofsnow/planar_fit_10sector_processed_30min_despiked_q3.5_test/\"\n",
    "DESPIKE = True\n",
    "FILTERING_q = 3.5\n",
    "# n cores utilized by application\n",
    "PARALLELISM = 20\n",
    "# Reynolds averaging length, in units (1/20) seconds\n",
    "SAMPLES_PER_AVERAGING_LENGTH = 30*60*20\n",
    "\n",
    "file_list = sorted(glob.glob(f\"{DATA_DIR}sublimationofsnow/sosqc_fast/*.nc\"))\n",
    "file_list = [f for f in file_list if '202210' not in f]\n",
    "wind_dir_bins = np.arange(0, 390, 30)\n",
    "\n",
    "# Open planar fit data\n",
    "monthly_file = f\"{DATA_DIR}sublimationofsnow/monthly_planar_fits_10sectors.csv\"\n",
    "fits_df = pd.read_csv(monthly_file, delim_whitespace=True)\n",
    "\n",
    "# Transform planar fit data\n",
    "fits_df['height'] = fits_df['height'].str.replace('_', '.').astype('float')\n",
    "fits_df['W_f'] = fits_df.apply(\n",
    "    lambda row: [row['W_f_1'], row['W_f_2'], row['W_f_3']],\n",
    "    axis=1\n",
    ").drop(columns=['W_f_1', 'W_f_2', 'W_f_3'])\n",
    "fits_df = fits_df.set_index(['month', 'height', 'tower', 'bin_low', 'bin_high'])\n",
    "fits_df = fits_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some convenience variables\n",
    "always_there_vars = [\n",
    "    'base_time',\n",
    "    'u_2m_c',\t'v_2m_c',\t'w_2m_c',\t'h2o_2m_c',\t\t'tc_2m_c',      'irgadiag_2m_c', 'ldiag_2m_c',\n",
    "    'u_3m_c',\t'v_3m_c',\t'w_3m_c',\t'h2o_3m_c',\t\t'tc_3m_c',      'irgadiag_3m_c', 'ldiag_3m_c',\n",
    "    'u_3m_d',\t'v_3m_d',\t'w_3m_d',\t'h2o_3m_d',\t\t'tc_3m_d',      'irgadiag_3m_d', 'ldiag_3m_d',\n",
    "    'u_3m_ue',\t'v_3m_ue',\t'w_3m_ue',\t'h2o_3m_ue',\t'tc_3m_ue',     'irgadiag_3m_ue', 'ldiag_3m_ue',\n",
    "    'u_3m_uw',\t'v_3m_uw',\t'w_3m_uw',\t'h2o_3m_uw',\t'tc_3m_uw',     'irgadiag_3m_uw', 'ldiag_3m_uw',\n",
    "    'u_5m_c',\t'v_5m_c',\t'w_5m_c',\t'h2o_5m_c',\t\t'tc_5m_c',      'irgadiag_5m_c', 'ldiag_5m_c',\n",
    "    'u_10m_c',\t'v_10m_c',\t'w_10m_c',\t'h2o_10m_c',\t'tc_10m_c',     'irgadiag_10m_c', 'ldiag_10m_c',\n",
    "    'u_10m_d',\t'v_10m_d',\t'w_10m_d',\t'h2o_10m_d',\t'tc_10m_d',     'irgadiag_10m_d', 'ldiag_10m_d',\n",
    "    'u_10m_ue',\t'v_10m_ue',\t'w_10m_ue',\t'h2o_10m_ue',\t'tc_10m_ue',    'irgadiag_10m_ue', 'ldiag_10m_ue',\n",
    "    'u_10m_uw',\t'v_10m_uw',\t'w_10m_uw',\t'h2o_10m_uw',\t'tc_10m_uw',    'irgadiag_10m_uw', 'ldiag_10m_uw',\n",
    "    'u_15m_c',\t'v_15m_c',\t'w_15m_c',\t'h2o_15m_c',\t'tc_15m_c',     'irgadiag_15m_c', 'ldiag_15m_c',\n",
    "    'u_20m_c',\t'v_20m_c',\t'w_20m_c',\t'h2o_20m_c',\t'tc_20m_c',     'irgadiag_20m_c', 'ldiag_20m_c',\n",
    "]\n",
    "c_1m_vars = ['u_1m_c',\t'v_1m_c',\t'w_1m_c',\t'h2o_1m_c',\t\t'tc_1m_c', 'irgadiag_1m_c', 'ldiag_1m_c']\n",
    "d_1m_vars = ['u_1m_d',\t'v_1m_d',\t'w_1m_d',\t'h2o_1m_d',\t\t'tc_1m_d', 'irgadiag_1m_d', 'ldiag_1m_d']\n",
    "ue_1m_vars = ['u_1m_ue',\t'v_1m_ue',\t'w_1m_ue',\t'h2o_1m_ue',\t'tc_1m_ue', 'irgadiag_1m_ue', 'ldiag_1m_ue']\n",
    "uw_1m_vars = ['u_1m_uw',\t'v_1m_uw',\t'w_1m_uw',\t'h2o_1m_uw',\t'tc_1m_uw', 'irgadiag_1m_uw', 'ldiag_1m_uw']\n",
    "c_1m_vars = ['u_1m_c',\t'v_1m_c',\t'w_1m_c',\t'h2o_1m_c',\t\t'tc_1m_c', 'irgadiag_1m_c', 'ldiag_1m_c']\n",
    "d_1m_vars = ['u_1m_d',\t'v_1m_d',\t'w_1m_d',\t'h2o_1m_d',\t\t'tc_1m_d', 'irgadiag_1m_d', 'ldiag_1m_d']\n",
    "ue_1m_vars = ['u_1m_ue',\t'v_1m_ue',\t'w_1m_ue',\t'h2o_1m_ue',\t'tc_1m_ue', 'irgadiag_1m_ue', 'ldiag_1m_ue']\n",
    "uw_1m_vars = ['u_1m_uw',\t'v_1m_uw',\t'w_1m_uw',\t'h2o_1m_uw',\t'tc_1m_uw', 'irgadiag_1m_uw', 'ldiag_1m_uw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_variables(ds):\n",
    "    all_vars_in_ds = always_there_vars\n",
    "    if 'u_1m_c' in ds:\n",
    "        all_vars_in_ds = all_vars_in_ds + c_1m_vars\n",
    "    if 'u_1m_d' in ds:\n",
    "        all_vars_in_ds = all_vars_in_ds + d_1m_vars\n",
    "    if 'u_1m_ue' in ds:\n",
    "        all_vars_in_ds = all_vars_in_ds + ue_1m_vars\n",
    "    if 'u_1m_uw' in ds:\n",
    "        all_vars_in_ds = all_vars_in_ds + uw_1m_vars\n",
    "    ds = ds[all_vars_in_ds]\n",
    "    return ds\n",
    "\n",
    "def create_timestamp(ds):\n",
    "    # To use the datum, its necessary to combine 3 columns of data from the dataset to get the full timestamp. \n",
    "    # This is demonstrated below. The 'time' column actually only incudes the second and minute information. For all datapoints, the hour according to the 'time' column is 1.  \n",
    "    # The 'base_time' column indicates the hour of the day. The 'sample' column indicates the 20hz sample number. \n",
    "    time_alt = pd.to_datetime(pd.DataFrame({\n",
    "        'year':         np.repeat(ds.time.dt.year, 20),\n",
    "        'month':        np.repeat(ds.time.dt.month, 20),\n",
    "        'day':          np.repeat(ds.time.dt.day, 20),\n",
    "        'hour' :        np.tile(ds.base_time.dt.hour, 60*60*20),\n",
    "        'minute' :      np.repeat(ds.time.dt.minute, 20),\n",
    "        'second' :      np.repeat(ds.time.dt.second, 20),\n",
    "        'microsecond' : (np.tile(ds.sample, 60*60) * (1e6/20)).astype(int),\n",
    "    }))\n",
    "    ds = ds.stack(new_time = ('time', 'sample'))\n",
    "    ds = ds.drop_vars(['time', 'sample'])\n",
    "    ds = ds.assign_coords(time = time_alt)\n",
    "    return ds\n",
    "\n",
    "def despike(ds, height, tower):\n",
    "    # Despiking using multiples of medians\n",
    "    def block_median(timeseries, window):\n",
    "        return timeseries.groupby(pd.Grouper(freq=window)).transform('median')\n",
    "    def filter_spike(timeseries, q = FILTERING_q, window='30min'):\n",
    "        mad = block_median(np.abs(timeseries - block_median(timeseries, window=window)), window=window)\n",
    "        upper_bound = block_median(timeseries, window=window) + q*mad / 0.6745\n",
    "        lower_bound = block_median(timeseries, window=window) - q*mad / 0.6745\n",
    "        is_valid = (timeseries > lower_bound) & (timeseries < upper_bound)\n",
    "        return timeseries.where(is_valid)\n",
    "\n",
    "    this_df = ds.to_dataframe()\n",
    "\n",
    "    this_df = pd.DataFrame(\n",
    "            filter_spike(\n",
    "                this_df[f'h2o_{height}m_{tower}'].where(this_df[f'irgadiag_{height}m_{tower}'] == 0)\n",
    "            )\n",
    "        ).join(\n",
    "            filter_spike(\n",
    "                this_df[f'w_{height}m_{tower}'].where(this_df[f'ldiag_{height}m_{tower}'] == 0)\n",
    "            )\n",
    "        )\n",
    "    ds = ds.update(\n",
    "        this_df[[f'h2o_{height}m_{tower}', f'w_{height}m_{tower}']].to_xarray()\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "def wind_direction(u, v):\n",
    "    # From: https://www.eol.ucar.edu/content/wind-direction-quick-reference\n",
    "    dir = 270 - np.rad2deg(np.arctan2(v,u))\n",
    "    if dir > 360:\n",
    "        dir = dir - 360\n",
    "    return dir\n",
    "\n",
    "def create_re_avg_ds(ds, var1,  var2, covariance_name):\n",
    "    # Function to do Reynolds Averaging\n",
    "    coarse_ds = ds.coarsen(time=SAMPLES_PER_AVERAGING_LENGTH).mean(skipna=True)\n",
    "    coarse_ds = coarse_ds.assign_coords(time = coarse_ds.time.dt.round('1s'))\n",
    "    coarse_ds = coarse_ds.reindex_like(ds, method='nearest')\n",
    "    ds[f\"{var1}_mean\"] = coarse_ds[f\"{var1}\"]\n",
    "    ds[f\"{var1}_fluc\"] = ds[f\"{var1}\"] - ds[f\"{var1}_mean\"]\n",
    "    ds[f\"{var2}_mean\"] = coarse_ds[f\"{var2}\"]\n",
    "    ds[f\"{var2}_fluc\"] = ds[f\"{var2}\"] - ds[f\"{var2}_mean\"]\n",
    "    ds[covariance_name] = ds[f\"{var2}_fluc\"] * ds[f\"{var1}_fluc\"]\n",
    "    ds = ds.coarsen(time = SAMPLES_PER_AVERAGING_LENGTH).mean()\n",
    "    ds = ds.assign_coords(time = ds.time.dt.round('1s'))\n",
    "    return ds.to_dataframe()\n",
    "\n",
    "def process_hourly_file(input_file, output_file):\n",
    "    # open files, filter by variables, checking if the 1m variables are in the dataset\n",
    "    ds = xr.open_dataset(input_file)\n",
    "    ds = subset_variables(ds)\n",
    "    ds = create_timestamp(ds)\n",
    "\n",
    "    assert len(pd.Series(ds.time.dt.month).unique()) == 1\n",
    "    MONTH = ds.time.dt.month.values[0]\n",
    "    \n",
    "    # Iterate over all height-towers, calculating the corrected variables\n",
    "    df_list = []\n",
    "    for height, tower in [\n",
    "        (1,  'c'),  (2,  'c'),  \n",
    "        (3,  'c'), (5,  'c'), \n",
    "        (10, 'c'), (15, 'c'), (20, 'c'),\n",
    "        (1,  'uw'), (3,  'uw'), (10, 'uw'),\n",
    "        (1,  'ue'), (3,  'ue'), (10, 'ue'),\n",
    "        (1,  'd'),  (3,  'd'),  (10, 'd')\n",
    "    ]:  \n",
    "        # assign convenience variables for the height-tower being operated on\n",
    "        U_VAR, V_VAR, W_VAR = (f\"u_{height}m_{tower}\", f\"v_{height}m_{tower}\", f\"w_{height}m_{tower}\")\n",
    "\n",
    "        # Make sure that height-tower is in this dataset (this time period may not have those measurements)\n",
    "        # and make sure that there is a fit available for this time period and height-tower\n",
    "        if U_VAR in ds and (MONTH, height, tower) in fits_df.index.droplevel([3, 4]):\n",
    "            if DESPIKE:\n",
    "                ds = despike(ds, height, tower)\n",
    "            # isolate the variables we want to operate on\n",
    "            local_df = ds[[U_VAR, V_VAR, W_VAR]].to_dataframe()\n",
    "            # calculate the 30-minute averaged wind direction\n",
    "            local_df['wind_direction_block_mean'] = local_df.groupby(pd.Grouper(freq='30min')).transform('mean').apply(\n",
    "                lambda row: wind_direction(row[U_VAR], row[V_VAR]), axis=1\n",
    "            )\n",
    "            # group the wind directions into discrete bins, we use the lower bound to identify each bin\n",
    "            local_df['wind_direction_block_mean_bin_low'] = pd.cut(\n",
    "                local_df['wind_direction_block_mean'],\n",
    "                wind_dir_bins,\n",
    "                labels = wind_dir_bins[:-1] \n",
    "            )\n",
    "            # merge the planar fit parameters into the data, so each row gets the appropriate parameters\n",
    "            local_df = local_df.reset_index().merge(\n",
    "                fits_df.loc[MONTH, height, tower][['a', 'W_f']],\n",
    "                left_on = 'wind_direction_block_mean_bin_low',\n",
    "                right_on = 'bin_low'\n",
    "            ).set_index('time')\n",
    "            # group by the wind speed bin and apply the planar fit for each subset of data\n",
    "            result = local_df.groupby('wind_direction_block_mean_bin_low').apply(\n",
    "                lambda df: (\n",
    "                    df.index, \n",
    "                    extrautils.apply_planar_fit(df[U_VAR], df[V_VAR], df[W_VAR], df['a'].values[0], df['W_f'].values[0])\n",
    "                )\n",
    "            )\n",
    "            # Wrangle the results from the groupby-apply\n",
    "            new_values_df = pd.DataFrame()\n",
    "            for key, results in result:\n",
    "                new_values_df = pd.concat([\n",
    "                    new_values_df,\n",
    "                    pd.DataFrame({\n",
    "                        'time': key,\n",
    "                        'u':    results[0],\n",
    "                        'v':    results[1],\n",
    "                        'w':    results[2],\n",
    "                    }).set_index('time')\n",
    "                ])\n",
    "            new_values_df = new_values_df.sort_index()\n",
    "\n",
    "            # add the fitted <u,v,w> values to the original xarray dataset\n",
    "            ds[f'u_{height}m_{tower}_fit'] =    new_values_df['u'].to_xarray()\n",
    "            ds[f'v_{height}m_{tower}_fit'] =    new_values_df['v'].to_xarray()\n",
    "            ds[f'w_{height}m_{tower}_fit'] =    new_values_df['w'].to_xarray()\n",
    "\n",
    "            # Calculate un-fitted Reynolds averaged variables\n",
    "            ds_plain_w_h2o =create_re_avg_ds(ds, f'w_{height}m_{tower}', f'h2o_{height}m_{tower}',  f'w_h2o__{height}m_{tower}')[[\n",
    "                f'u_{height}m_{tower}',\n",
    "                f'v_{height}m_{tower}',\n",
    "                f'w_{height}m_{tower}',\n",
    "                f'w_h2o__{height}m_{tower}'\n",
    "            ]]\n",
    "            ds_plain_u_h2o =create_re_avg_ds(ds, f'u_{height}m_{tower}', f'h2o_{height}m_{tower}', f'u_h2o__{height}m_{tower}' )[f'u_h2o__{height}m_{tower}']\n",
    "            ds_plain_v_h2o =create_re_avg_ds(ds, f'v_{height}m_{tower}', f'h2o_{height}m_{tower}', f'v_h2o__{height}m_{tower}' )[f'v_h2o__{height}m_{tower}']\n",
    "            ds_plain_w_w =  create_re_avg_ds(ds, f'w_{height}m_{tower}', f'w_{height}m_{tower}',   f'w_w__{height}m_{tower}'   )[f'w_w__{height}m_{tower}']\n",
    "            ds_plain_u_u =  create_re_avg_ds(ds, f'u_{height}m_{tower}', f'u_{height}m_{tower}',   f'u_u__{height}m_{tower}'   )[f'u_u__{height}m_{tower}']\n",
    "            ds_plain_v_v =  create_re_avg_ds(ds, f'v_{height}m_{tower}', f'v_{height}m_{tower}',   f'v_v__{height}m_{tower}'   )[f'v_v__{height}m_{tower}']\n",
    "            ds_plain_u_w =  create_re_avg_ds(ds, f'u_{height}m_{tower}', f'w_{height}m_{tower}',   f'u_w__{height}m_{tower}'   )[f'u_w__{height}m_{tower}']\n",
    "            ds_plain_v_w =  create_re_avg_ds(ds, f'v_{height}m_{tower}', f'w_{height}m_{tower}',   f'v_w__{height}m_{tower}'   )[f'v_w__{height}m_{tower}']\n",
    "            ds_plain_u_tc = create_re_avg_ds(ds, f'u_{height}m_{tower}', f'tc_{height}m_{tower}',  f'u_tc__{height}m_{tower}'  )[f'u_tc__{height}m_{tower}']\n",
    "            ds_plain_v_tc = create_re_avg_ds(ds, f'v_{height}m_{tower}', f'tc_{height}m_{tower}',  f'v_tc__{height}m_{tower}'  )[f'v_tc__{height}m_{tower}']\n",
    "            ds_plain_w_tc = create_re_avg_ds(ds, f'w_{height}m_{tower}', f'tc_{height}m_{tower}',  f'w_tc__{height}m_{tower}'  )[f'w_tc__{height}m_{tower}']\n",
    "\n",
    "            # Calculate fitted Reynolds averaged variables\n",
    "            ds_fit_w_h2o =create_re_avg_ds(ds,  f'w_{height}m_{tower}_fit', f'h2o_{height}m_{tower}', f'w_h2o__{height}m_{tower}_fit')[[\n",
    "                f'u_{height}m_{tower}_fit',\n",
    "                f'v_{height}m_{tower}_fit',\n",
    "                f'w_{height}m_{tower}_fit',\n",
    "                f'w_h2o__{height}m_{tower}_fit'\n",
    "            ]]\n",
    "            ds_fit_u_h2o =create_re_avg_ds(ds, f'u_{height}m_{tower}_fit', f'h2o_{height}m_{tower}',   f'u_h2o__{height}m_{tower}_fit')[f'u_h2o__{height}m_{tower}_fit']\n",
    "            ds_fit_v_h2o =create_re_avg_ds(ds, f'v_{height}m_{tower}_fit', f'h2o_{height}m_{tower}',   f'v_h2o__{height}m_{tower}_fit')[f'v_h2o__{height}m_{tower}_fit']\n",
    "            ds_fit_w_w =  create_re_avg_ds(ds, f'w_{height}m_{tower}_fit', f'w_{height}m_{tower}_fit', f'w_w__{height}m_{tower}_fit'  )[f'w_w__{height}m_{tower}_fit']\n",
    "            ds_fit_u_u =  create_re_avg_ds(ds, f'u_{height}m_{tower}_fit', f'u_{height}m_{tower}_fit', f'u_u__{height}m_{tower}_fit'  )[f'u_u__{height}m_{tower}_fit']\n",
    "            ds_fit_v_v =  create_re_avg_ds(ds, f'v_{height}m_{tower}_fit', f'v_{height}m_{tower}_fit', f'v_v__{height}m_{tower}_fit'  )[f'v_v__{height}m_{tower}_fit']\n",
    "            ds_fit_u_w =  create_re_avg_ds(ds, f'u_{height}m_{tower}_fit', f'w_{height}m_{tower}_fit', f'u_w__{height}m_{tower}_fit'  )[f'u_w__{height}m_{tower}_fit']\n",
    "            ds_fit_v_w =  create_re_avg_ds(ds, f'v_{height}m_{tower}_fit', f'w_{height}m_{tower}_fit', f'v_w__{height}m_{tower}_fit'  )[f'v_w__{height}m_{tower}_fit']\n",
    "            ds_fit_u_tc = create_re_avg_ds(ds, f'u_{height}m_{tower}_fit', f'tc_{height}m_{tower}',    f'u_tc__{height}m_{tower}_fit' )[f'u_tc__{height}m_{tower}_fit']\n",
    "            ds_fit_v_tc = create_re_avg_ds(ds, f'v_{height}m_{tower}_fit', f'tc_{height}m_{tower}',    f'v_tc__{height}m_{tower}_fit' )[f'v_tc__{height}m_{tower}_fit']\n",
    "            ds_fit_w_tc = create_re_avg_ds(ds, f'w_{height}m_{tower}_fit', f'tc_{height}m_{tower}',    f'w_tc__{height}m_{tower}_fit' )[f'w_tc__{height}m_{tower}_fit']\n",
    "\n",
    "            df_plain = ds_plain_w_h2o.join(ds_plain_u_h2o).join(ds_plain_v_h2o).join(ds_plain_w_w).join(ds_plain_u_u).join(\n",
    "                ds_plain_v_v).join(ds_plain_u_w).join(ds_plain_v_w).join(ds_plain_u_tc).join(ds_plain_v_tc).join(ds_plain_w_tc)\n",
    "\n",
    "            df_fit = ds_fit_w_h2o.join(ds_fit_u_h2o).join(ds_fit_v_h2o).join(ds_fit_w_w).join(ds_fit_u_u).join(\n",
    "                ds_fit_v_v).join(ds_fit_u_w).join(ds_fit_v_w).join(ds_fit_u_tc).join(ds_fit_v_tc).join(ds_fit_w_tc)\n",
    "\n",
    "            merged_df = df_plain.join(df_fit)\n",
    "            df_list.append(merged_df)\n",
    "\n",
    "    combined_df = df_list[0].join(df_list[1:])\n",
    "    combined_df.to_parquet(output_file)\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/elilouis/sublimationofsnow/sosqc_fast/isfs_sos_qc_geo_tiltcor_hr_20221101_02.nc\n",
      "/storage/elilouis/sublimationofsnow/planar_fit_10sector_processed_30min_despiked_q3.5_test/20221101_02.parquet\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "input_file = file_list[i]\n",
    "output_file = os.path.join(OUTPUT_PATH, input_file.split('/')[-1][27:]).replace('.nc', '.parquet')\n",
    "\n",
    "print(input_file)\n",
    "print(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[277], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[275], line 186\u001b[0m, in \u001b[0;36mprocess_files\u001b[0;34m(file_list, output_file)\u001b[0m\n\u001b[1;32m    184\u001b[0m ds_fit_u_tc \u001b[38;5;241m=\u001b[39m create_re_avg_ds(ds, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mu_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtower\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtower\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mu_tc__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtower\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fit\u001b[39m\u001b[38;5;124m'\u001b[39m )[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mu_tc__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtower\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fit\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    185\u001b[0m ds_fit_v_tc \u001b[38;5;241m=\u001b[39m create_re_avg_ds(ds, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtower\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtower\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_tc__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtower\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fit\u001b[39m\u001b[38;5;124m'\u001b[39m )[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_tc__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtower\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fit\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 186\u001b[0m ds_fit_w_tc \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_re_avg_ds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mheight\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mm_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtower\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_fit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtc_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mheight\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mm_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtower\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw_tc__\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mheight\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mm_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtower\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_fit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw_tc__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtower\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fit\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    188\u001b[0m df_plain \u001b[38;5;241m=\u001b[39m ds_plain_w_h2o\u001b[38;5;241m.\u001b[39mjoin(ds_plain_u_h2o)\u001b[38;5;241m.\u001b[39mjoin(ds_plain_v_h2o)\u001b[38;5;241m.\u001b[39mjoin(ds_plain_w_w)\u001b[38;5;241m.\u001b[39mjoin(ds_plain_u_u)\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    189\u001b[0m     ds_plain_v_v)\u001b[38;5;241m.\u001b[39mjoin(ds_plain_u_w)\u001b[38;5;241m.\u001b[39mjoin(ds_plain_v_w)\u001b[38;5;241m.\u001b[39mjoin(ds_plain_u_tc)\u001b[38;5;241m.\u001b[39mjoin(ds_plain_v_tc)\u001b[38;5;241m.\u001b[39mjoin(ds_plain_w_tc)\n\u001b[1;32m    191\u001b[0m df_fit \u001b[38;5;241m=\u001b[39m ds_fit_w_h2o\u001b[38;5;241m.\u001b[39mjoin(ds_fit_u_h2o)\u001b[38;5;241m.\u001b[39mjoin(ds_fit_v_h2o)\u001b[38;5;241m.\u001b[39mjoin(ds_fit_w_w)\u001b[38;5;241m.\u001b[39mjoin(ds_fit_u_u)\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    192\u001b[0m     ds_fit_v_v)\u001b[38;5;241m.\u001b[39mjoin(ds_fit_u_w)\u001b[38;5;241m.\u001b[39mjoin(ds_fit_v_w)\u001b[38;5;241m.\u001b[39mjoin(ds_fit_u_tc)\u001b[38;5;241m.\u001b[39mjoin(ds_fit_v_tc)\u001b[38;5;241m.\u001b[39mjoin(ds_fit_w_tc)\n",
      "Cell \u001b[0;32mIn[275], line 69\u001b[0m, in \u001b[0;36mcreate_re_avg_ds\u001b[0;34m(ds, var1, var2, covariance_name)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_re_avg_ds\u001b[39m(ds, var1,  var2, covariance_name):\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Function to do Reynolds Averaging\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     coarse_ds \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoarsen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAMPLES_PER_AVERAGING_LENGTH\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     coarse_ds \u001b[38;5;241m=\u001b[39m coarse_ds\u001b[38;5;241m.\u001b[39massign_coords(time \u001b[38;5;241m=\u001b[39m coarse_ds\u001b[38;5;241m.\u001b[39mtime\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1s\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     71\u001b[0m     coarse_ds \u001b[38;5;241m=\u001b[39m coarse_ds\u001b[38;5;241m.\u001b[39mreindex_like(ds, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/sublimationofsnow/lib/python3.12/site-packages/xarray/core/rolling.py:1208\u001b[0m, in \u001b[0;36mDatasetCoarsen._reduce_method.<locals>.wrapped_func\u001b[0;34m(self, keep_attrs, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m reduced \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, da \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mdata_vars\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m-> 1208\u001b[0m     reduced[key] \u001b[38;5;241m=\u001b[39m \u001b[43mda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoarsen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboundary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1217\u001b[0m coords \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcoords\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;66;03m# variable.coarsen returns variables not containing the window dims\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;66;03m# unchanged (maybe removes attrs)\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/sublimationofsnow/lib/python3.12/site-packages/xarray/core/variable.py:2228\u001b[0m, in \u001b[0;36mVariable.coarsen\u001b[0;34m(self, windows, func, boundary, side, keep_attrs, **kwargs)\u001b[0m\n\u001b[1;32m   2225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid method.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replace(data\u001b[38;5;241m=\u001b[39m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, attrs\u001b[38;5;241m=\u001b[39m_attrs)\n",
      "File \u001b[0;32m~/mambaforge/envs/sublimationofsnow/lib/python3.12/site-packages/xarray/core/duck_array_ops.py:637\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(array, axis, skipna, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _to_pytimedelta(mean_timedeltas, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mus\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m offset\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/sublimationofsnow/lib/python3.12/site-packages/xarray/core/duck_array_ops.py:404\u001b[0m, in \u001b[0;36m_create_nan_agg_method.<locals>.f\u001b[0;34m(values, axis, skipna, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m    403\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll-NaN slice encountered\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 404\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_duck_dask_array(values):\n",
      "File \u001b[0;32m~/mambaforge/envs/sublimationofsnow/lib/python3.12/site-packages/xarray/core/nanops.py:131\u001b[0m, in \u001b[0;36mnanmean\u001b[0;34m(a, axis, dtype, out)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m    127\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean of empty slice\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mRuntimeWarning\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     )\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/sublimationofsnow/lib/python3.12/site-packages/numpy/lib/nanfunctions.py:1044\u001b[0m, in \u001b[0;36mnanmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(out\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39minexact):\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf a is inexact, then out must be inexact\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1044\u001b[0m cnt \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m             \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1046\u001b[0m tot \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(arr, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, keepdims\u001b[38;5;241m=\u001b[39mkeepdims,\n\u001b[1;32m   1047\u001b[0m              where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[1;32m   1048\u001b[0m avg \u001b[38;5;241m=\u001b[39m _divide_by_count(tot, cnt, out\u001b[38;5;241m=\u001b[39mout)\n",
      "File \u001b[0;32m~/mambaforge/envs/sublimationofsnow/lib/python3.12/site-packages/numpy/core/fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/sublimationofsnow/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output = process_hourly_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** KeyboardInterrupt exception caught in code being profiled."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 530.04 s\n",
      "File: /tmp/ipykernel_14408/4167222075.py\n",
      "Function: process_files at line 81\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    81                                           def process_files(file_list, output_file):\n",
      "    82                                               # open files, filter by variables, checking if the 1m variables are in the dataset\n",
      "    83         1 1774789272.0    2e+09      0.3      ds = xr.open_mfdataset(file_list, concat_dim=\"time\", combine=\"nested\")\n",
      "    84         1    2333389.0    2e+06      0.0      ds = subset_variables(ds)\n",
      "    85         1        2e+10    2e+10      3.2      ds = create_timestamp(ds)\n",
      "    86                                           \n",
      "    87         1    9979513.0    1e+07      0.0      assert len(pd.Series(ds.time.dt.month).unique()) == 1\n",
      "    88         1    7023592.0    7e+06      0.0      MONTH = ds.time.dt.month.values[0]\n",
      "    89                                               \n",
      "    90                                               # Iterate over all height-towers, calculating the corrected variables\n",
      "    91         1       1650.0   1650.0      0.0      df_list = []\n",
      "    92        12     135953.0  11329.4      0.0      for height, tower in [\n",
      "    93                                                   (1,  'c'),  (2,  'c'),  (3,  'c'), (5,  'c'), (10, 'c'), (15, 'c'), (20, 'c'),\n",
      "    94                                                   (1,  'uw'), (3,  'uw'), (10, 'uw'),\n",
      "    95                                                   (1,  'ue'), (3,  'ue'), (10, 'ue'),\n",
      "    96                                                   (1,  'd'),  (3,  'd'),  (10, 'd')\n",
      "    97                                               ]:  \n",
      "    98                                                   # assign convenience variables for the height-tower being operated on\n",
      "    99        12     110826.0   9235.5      0.0          U_VAR, V_VAR, W_VAR = (f\"u_{height}m_{tower}\", f\"v_{height}m_{tower}\", f\"w_{height}m_{tower}\")\n",
      "   100                                           \n",
      "   101                                                   # Make sure that height-tower is in this dataset (this time period may not have those measurements)\n",
      "   102                                                   # and make sure that there is a fit available for this time period and height-tower\n",
      "   103        12   26587648.0    2e+06      0.0          if U_VAR in ds and (MONTH, height, tower) in fits_df.index.droplevel([3, 4]):\n",
      "   104        12      85114.0   7092.8      0.0              if DESPIKE:\n",
      "   105        12 7378286079.0    6e+08      1.4                  ds = despike(ds, height, tower)\n",
      "   106                                                       # isolate the variables we want to operate on\n",
      "   107        12   38942078.0    3e+06      0.0              local_df = ds[[U_VAR, V_VAR, W_VAR]].to_dataframe()\n",
      "   108                                                       # calculate the 30-minute averaged wind direction\n",
      "   109        24        7e+10    3e+09     13.6              local_df['wind_direction_block_mean'] = local_df.groupby(pd.Grouper(freq='30min')).transform('mean').apply(\n",
      "   110        12     126288.0  10524.0      0.0                  lambda row: wind_direction(row[U_VAR], row[V_VAR]), axis=1\n",
      "   111                                                       )\n",
      "   112                                                       # group the wind directions into discrete bins, we use the lower bound to identify each bin\n",
      "   113        24   93116168.0    4e+06      0.0              local_df['wind_direction_block_mean_bin_low'] = pd.cut(\n",
      "   114        12    3782720.0 315226.7      0.0                  local_df['wind_direction_block_mean'],\n",
      "   115        12      72506.0   6042.2      0.0                  wind_dir_bins,\n",
      "   116        12      44610.0   3717.5      0.0                  labels = wind_dir_bins[:-1] \n",
      "   117                                                       )\n",
      "   118                                                       # merge the planar fit parameters into the data, so each row gets the appropriate parameters\n",
      "   119        36  397672081.0    1e+07      0.1              local_df = local_df.reset_index().merge(\n",
      "   120        12   78125288.0    7e+06      0.0                  fits_df.loc[MONTH, height, tower][['a', 'W_f']],\n",
      "   121        12      25855.0   2154.6      0.0                  left_on = 'wind_direction_block_mean_bin_low',\n",
      "   122        12      10574.0    881.2      0.0                  right_on = 'bin_low'\n",
      "   123        12   57331250.0    5e+06      0.0              ).set_index('time')\n",
      "   124                                                       # group by the wind speed bin and apply the planar fit for each subset of data\n",
      "   125        24 1021760406.0    4e+07      0.2              result = local_df.groupby('wind_direction_block_mean_bin_low').apply(\n",
      "   126        12      91429.0   7619.1      0.0                  lambda df: (\n",
      "   127                                                               df.index, \n",
      "   128                                                               extrautils.apply_planar_fit(df[U_VAR], df[V_VAR], df[W_VAR], df['a'].values[0], df['W_f'].values[0])\n",
      "   129                                                           )\n",
      "   130                                                       )\n",
      "   131                                                       # Wrangle the results from the groupby-apply\n",
      "   132        12   15579292.0    1e+06      0.0              new_values_df = pd.DataFrame()\n",
      "   133        26     539460.0  20748.5      0.0              for key, results in result:\n",
      "   134        28   47840049.0    2e+06      0.0                  new_values_df = pd.concat([\n",
      "   135        14       9147.0    653.4      0.0                      new_values_df,\n",
      "   136        28   33052925.0    1e+06      0.0                      pd.DataFrame({\n",
      "   137        14       8167.0    583.4      0.0                          'time': key,\n",
      "   138        14      14325.0   1023.2      0.0                          'u':    results[0],\n",
      "   139        14       9590.0    685.0      0.0                          'v':    results[1],\n",
      "   140        14       9428.0    673.4      0.0                          'w':    results[2],\n",
      "   141        14  137619621.0    1e+07      0.0                      }).set_index('time')\n",
      "   142                                                           ])\n",
      "   143        12   41220883.0    3e+06      0.0              new_values_df = new_values_df.sort_index()\n",
      "   144                                           \n",
      "   145                                                       # add the fitted <u,v,w> values to the original xarray dataset\n",
      "   146        12  888866759.0    7e+07      0.2              ds[f'u_{height}m_{tower}_fit'] =    new_values_df['u'].to_xarray()\n",
      "   147        12  451789951.0    4e+07      0.1              ds[f'v_{height}m_{tower}_fit'] =    new_values_df['v'].to_xarray()\n",
      "   148        12  378887937.0    3e+07      0.1              ds[f'w_{height}m_{tower}_fit'] =    new_values_df['w'].to_xarray()\n",
      "   149                                           \n",
      "   150                                                       # Calculate un-fitted Reynolds averaged variables\n",
      "   151        24        2e+10    8e+08      3.4              ds_plain_w_h2o =create_re_avg_ds(ds, f'w_{height}m_{tower}', f'h2o_{height}m_{tower}',  f'w_h2o__{height}m_{tower}')[[\n",
      "   152        12      75159.0   6263.2      0.0                  f'u_{height}m_{tower}',\n",
      "   153        12      14131.0   1177.6      0.0                  f'v_{height}m_{tower}',\n",
      "   154        12      16070.0   1339.2      0.0                  f'w_{height}m_{tower}',\n",
      "   155        12      19762.0   1646.8      0.0                  f'w_h2o__{height}m_{tower}'\n",
      "   156                                                       ]]\n",
      "   157        12        2e+10    2e+09      3.8              ds_plain_u_h2o =create_re_avg_ds(ds, f'u_{height}m_{tower}', f'h2o_{height}m_{tower}', f'u_h2o__{height}m_{tower}' )[f'u_h2o__{height}m_{tower}']\n",
      "   158        12        2e+10    2e+09      4.0              ds_plain_v_h2o =create_re_avg_ds(ds, f'v_{height}m_{tower}', f'h2o_{height}m_{tower}', f'v_h2o__{height}m_{tower}' )[f'v_h2o__{height}m_{tower}']\n",
      "   159        12        2e+10    2e+09      3.7              ds_plain_w_w =  create_re_avg_ds(ds, f'w_{height}m_{tower}', f'w_{height}m_{tower}',   f'w_w__{height}m_{tower}'   )[f'w_w__{height}m_{tower}']\n",
      "   160        12        2e+10    2e+09      3.7              ds_plain_u_u =  create_re_avg_ds(ds, f'u_{height}m_{tower}', f'u_{height}m_{tower}',   f'u_u__{height}m_{tower}'   )[f'u_u__{height}m_{tower}']\n",
      "   161        12        2e+10    2e+09      3.7              ds_plain_v_v =  create_re_avg_ds(ds, f'v_{height}m_{tower}', f'v_{height}m_{tower}',   f'v_v__{height}m_{tower}'   )[f'v_v__{height}m_{tower}']\n",
      "   162        12        2e+10    2e+09      3.8              ds_plain_u_w =  create_re_avg_ds(ds, f'u_{height}m_{tower}', f'w_{height}m_{tower}',   f'u_w__{height}m_{tower}'   )[f'u_w__{height}m_{tower}']\n",
      "   163        12        2e+10    2e+09      3.6              ds_plain_v_w =  create_re_avg_ds(ds, f'v_{height}m_{tower}', f'w_{height}m_{tower}',   f'v_w__{height}m_{tower}'   )[f'v_w__{height}m_{tower}']\n",
      "   164        12        2e+10    2e+09      3.9              ds_plain_u_tc = create_re_avg_ds(ds, f'u_{height}m_{tower}', f'tc_{height}m_{tower}',  f'u_tc__{height}m_{tower}'  )[f'u_tc__{height}m_{tower}']\n",
      "   165        12        2e+10    2e+09      3.8              ds_plain_v_tc = create_re_avg_ds(ds, f'v_{height}m_{tower}', f'tc_{height}m_{tower}',  f'v_tc__{height}m_{tower}'  )[f'v_tc__{height}m_{tower}']\n",
      "   166        12        2e+10    2e+09      3.7              ds_plain_w_tc = create_re_avg_ds(ds, f'w_{height}m_{tower}', f'tc_{height}m_{tower}',  f'w_tc__{height}m_{tower}'  )[f'w_tc__{height}m_{tower}']\n",
      "   167                                           \n",
      "   168                                                       # Calculate fitted Reynolds averaged variables\n",
      "   169        24        2e+10    8e+08      3.7              ds_fit_w_h2o =create_re_avg_ds(ds,  f'w_{height}m_{tower}_fit', f'h2o_{height}m_{tower}', f'w_h2o__{height}m_{tower}_fit')[[\n",
      "   170        12      83669.0   6972.4      0.0                  f'u_{height}m_{tower}_fit',\n",
      "   171        12      30613.0   2551.1      0.0                  f'v_{height}m_{tower}_fit',\n",
      "   172        12      17656.0   1471.3      0.0                  f'w_{height}m_{tower}_fit',\n",
      "   173        12      18146.0   1512.2      0.0                  f'w_h2o__{height}m_{tower}_fit'\n",
      "   174                                                       ]]\n",
      "   175        12        2e+10    2e+09      3.7              ds_fit_u_h2o =create_re_avg_ds(ds, f'u_{height}m_{tower}_fit', f'h2o_{height}m_{tower}',   f'u_h2o__{height}m_{tower}_fit')[f'u_h2o__{height}m_{tower}_fit']\n",
      "   176        12        2e+10    2e+09      3.5              ds_fit_v_h2o =create_re_avg_ds(ds, f'v_{height}m_{tower}_fit', f'h2o_{height}m_{tower}',   f'v_h2o__{height}m_{tower}_fit')[f'v_h2o__{height}m_{tower}_fit']\n",
      "   177        11        2e+10    2e+09      3.5              ds_fit_w_w =  create_re_avg_ds(ds, f'w_{height}m_{tower}_fit', f'w_{height}m_{tower}_fit', f'w_w__{height}m_{tower}_fit'  )[f'w_w__{height}m_{tower}_fit']\n",
      "   178        11        2e+10    2e+09      3.5              ds_fit_u_u =  create_re_avg_ds(ds, f'u_{height}m_{tower}_fit', f'u_{height}m_{tower}_fit', f'u_u__{height}m_{tower}_fit'  )[f'u_u__{height}m_{tower}_fit']\n",
      "   179        11        2e+10    2e+09      3.4              ds_fit_v_v =  create_re_avg_ds(ds, f'v_{height}m_{tower}_fit', f'v_{height}m_{tower}_fit', f'v_v__{height}m_{tower}_fit'  )[f'v_v__{height}m_{tower}_fit']\n",
      "   180        11        2e+10    2e+09      3.3              ds_fit_u_w =  create_re_avg_ds(ds, f'u_{height}m_{tower}_fit', f'w_{height}m_{tower}_fit', f'u_w__{height}m_{tower}_fit'  )[f'u_w__{height}m_{tower}_fit']\n",
      "   181        11        2e+10    2e+09      3.5              ds_fit_v_w =  create_re_avg_ds(ds, f'v_{height}m_{tower}_fit', f'w_{height}m_{tower}_fit', f'v_w__{height}m_{tower}_fit'  )[f'v_w__{height}m_{tower}_fit']\n",
      "   182        11        2e+10    2e+09      3.7              ds_fit_u_tc = create_re_avg_ds(ds, f'u_{height}m_{tower}_fit', f'tc_{height}m_{tower}',    f'u_tc__{height}m_{tower}_fit' )[f'u_tc__{height}m_{tower}_fit']\n",
      "   183        11        2e+10    2e+09      3.7              ds_fit_v_tc = create_re_avg_ds(ds, f'v_{height}m_{tower}_fit', f'tc_{height}m_{tower}',    f'v_tc__{height}m_{tower}_fit' )[f'v_tc__{height}m_{tower}_fit']\n",
      "   184        11        2e+10    2e+09      3.6              ds_fit_w_tc = create_re_avg_ds(ds, f'w_{height}m_{tower}_fit', f'tc_{height}m_{tower}',    f'w_tc__{height}m_{tower}_fit' )[f'w_tc__{height}m_{tower}_fit']\n",
      "   185                                           \n",
      "   186        33  341275615.0    1e+07      0.1              df_plain = ds_plain_w_h2o.join(ds_plain_u_h2o).join(ds_plain_v_h2o).join(ds_plain_w_w).join(ds_plain_u_u).join(\n",
      "   187        22  288030112.0    1e+07      0.1                  ds_plain_v_v).join(ds_plain_u_w).join(ds_plain_v_w).join(ds_plain_u_tc).join(ds_plain_v_tc).join(ds_plain_w_tc)\n",
      "   188                                           \n",
      "   189        33  282006805.0    9e+06      0.1              df_fit = ds_fit_w_h2o.join(ds_fit_u_h2o).join(ds_fit_v_h2o).join(ds_fit_w_w).join(ds_fit_u_u).join(\n",
      "   190        22  264860896.0    1e+07      0.0                  ds_fit_v_v).join(ds_fit_u_w).join(ds_fit_v_w).join(ds_fit_u_tc).join(ds_fit_v_tc).join(ds_fit_w_tc)\n",
      "   191                                           \n",
      "   192        11   56553509.0    5e+06      0.0              merged_df = df_plain.join(df_fit)\n",
      "   193        11      26605.0   2418.6      0.0              df_list.append(merged_df)\n",
      "   194                                           \n",
      "   195                                               combined_df = df_list[0].join(df_list[1:])\n",
      "   196                                               combined_df.to_parquet(output_file)\n",
      "   197                                               return output_file"
     ]
    }
   ],
   "source": [
    "%lprun -f process_files process_files([input_file], output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_re_avg_ds(ds, var1,  var2, covariance_name):\n",
    "    # Function to do Reynolds Averaging\n",
    "    coarse_ds = ds.coarsen(time=SAMPLES_PER_AVERAGING_LENGTH).mean(skipna=True)\n",
    "    coarse_ds = coarse_ds.assign_coords(time = coarse_ds.time.dt.round('1s'))\n",
    "    coarse_ds = coarse_ds.reindex_like(ds, method='nearest')\n",
    "    ds[f\"{var1}_mean\"] = coarse_ds[f\"{var1}\"]\n",
    "    ds[f\"{var1}_fluc\"] = ds[f\"{var1}\"] - ds[f\"{var1}_mean\"]\n",
    "    ds[f\"{var2}_mean\"] = coarse_ds[f\"{var2}\"]\n",
    "    ds[f\"{var2}_fluc\"] = ds[f\"{var2}\"] - ds[f\"{var2}_mean\"]\n",
    "    ds[covariance_name] = ds[f\"{var2}_fluc\"] * ds[f\"{var1}_fluc\"]\n",
    "    ds = ds.coarsen(time = SAMPLES_PER_AVERAGING_LENGTH).mean()\n",
    "    ds = ds.assign_coords(time = ds.time.dt.round('1s'))\n",
    "    return ds.to_dataframe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sublimationofsnow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
