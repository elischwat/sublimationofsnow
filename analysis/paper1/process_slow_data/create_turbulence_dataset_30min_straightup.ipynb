{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produces a dataset with processed and cleaned latent heat flux measurements.\n",
    "Processing steps are described in Schwat et al., 2024.\n",
    "\n",
    "Three versions of the latent heat flux measurements are included in the output/saved dataset:\n",
    "* `w_h2o__3m_c`\n",
    "    * 20Hz qc steps (flagged measurements removed, despiked) and 30min qc steps applied (30min fluxes with >25% 20hz measurements flagged removed, plausibility limits applied)\n",
    "* `w_h2o__3m_c_raw`\n",
    "    * 20Hz qc steps only (see above)\n",
    "* `w_h2o__3m_c_gapfill`\n",
    "    * 20Hz qc steps and 30min qc steps applied (see above), plus the gaps are filled using mean-diurnal gap filling method with 7 day window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from sublimpy import variables\n",
    "from sublimpy import utils\n",
    "from sublimpy import tidy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import altair as alt\n",
    "alt.data_transformers.enable('json')\n",
    "\n",
    "from metpy.calc import specific_humidity_from_mixing_ratio\n",
    "from metpy.units import units\n",
    "import metpy.constants\n",
    "import pint_pandas\n",
    "import pint_xarray\n",
    "import xarray as xr\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from metpy.constants import density_water"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_data_dir = '/Users/elischwat/Development/data/sublimationofsnow/sosqc/sos_isfs_qc_geo_tiltcor_5min_v20241227/'\n",
    "\n",
    "# Choices:\n",
    "planar_fitted_dir = \"/Users/elischwat/Development/data/sublimationofsnow/planar_fit_10sector_processed_30min_despiked_q7\"\n",
    "filtering_str = 'q7'\n",
    "FILTER_SNOWFALL = False\n",
    "snowfall_mask_file = \"/Users/elischwat/Development/data/sublimationofsnow/precipitation_masks/w23_precipitation_mask_0_mm.csv\"\n",
    "snowfall_mask_str = None\n",
    "PERCENTAGE_DIAG = 9000\n",
    "\n",
    "# Constants\n",
    "PLANAR_FIT = True\n",
    "start_date = '20221101'\n",
    "end_date = '20230619'\n",
    "# STUDY_PERIOD_START_DATE = '20221130'\n",
    "# STUDY_PERIOD_END_DATE = '20230508'\n",
    "STUDY_PERIOD_START_DATE = '20221101'\n",
    "STUDY_PERIOD_END_DATE = '20230619'\n",
    "DATE_FORMAT_STR = '%Y%m%d'\n",
    "# Threshold number of 20hz samples in a 5 minute average determining if the value\n",
    "# is replaced with NaN\n",
    "datelist = pd.date_range(\n",
    "    dt.datetime.strptime(start_date, DATE_FORMAT_STR),\n",
    "    dt.datetime.strptime(end_date, DATE_FORMAT_STR),\n",
    "    freq='d'\n",
    ").strftime(DATE_FORMAT_STR).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out the eddy covariance measurement variable names because they are very repetitive\n",
    "ec_measurement_suffixes = [\n",
    "    '1m_ue',    '2m_ue',    '3m_ue',    '10m_ue', \n",
    "    '1m_d',     '2m_d',     '3m_d',     '10m_d',\n",
    "    '1m_uw',    '2m_uw',    '2_5m_uw',  '3m_uw',    '10m_uw', \n",
    "    '1m_c',     '2m_c',     '3m_c',     '5m_c',     '10m_c',    '15m_c',    '20m_c'\n",
    "]\n",
    "\n",
    "sonic_measurement_prefixes = [\n",
    "    'u_', 'v_', 'w_', 'tc_', 'spd_', 'dir_', \n",
    "    'u_u__', 'v_v__', 'w_w__', 'tc_tc__', \n",
    "    'u_w__', 'v_w__', 'u_v__', \n",
    "    'u_tc__', 'v_tc__', 'w_tc__', \n",
    "    'u_u_u__', 'v_v_v__', 'w_w_w__',\n",
    "    'tc_tc_tc__', \n",
    "]\n",
    "irga_measurement_prefixes = [\n",
    "    'h2o_', 'h2o_h2o__', 'h2o_h2o_h2o__', 'co2_', 'co2_co2__', 'co2_co2_co2__', \n",
    "]\n",
    "sonic_plus_irga_measurement_prefixes = [\n",
    "    'u_h2o__', 'v_h2o__', 'w_h2o__', 'u_co2__', 'v_co2__', 'w_co2__', \n",
    "]\n",
    "ec_measurement_prefixes = sonic_measurement_prefixes + irga_measurement_prefixes + sonic_plus_irga_measurement_prefixes\n",
    "\n",
    "ec_variable_names = [\n",
    "    (prefix + suffix) for prefix in ec_measurement_prefixes for suffix in ec_measurement_suffixes\n",
    "]\n",
    "\n",
    "counts_vars = ['counts_' + suffix for suffix in ec_measurement_suffixes]\n",
    "counts_1_vars = ['counts_' + suffix + '_1' for suffix in ec_measurement_suffixes]\n",
    "counts_2_vars = ['counts_' + suffix + '_2' for suffix in ec_measurement_suffixes]\n",
    "irgadiag_vars = ['irgadiag_' + suffix for suffix in ec_measurement_suffixes]\n",
    "ldiag_vars = ['ldiag_' + suffix for suffix in ec_measurement_suffixes]\n",
    "\n",
    "diagnostic_variable_names = counts_vars + counts_1_vars + counts_2_vars + irgadiag_vars + ldiag_vars\n",
    "\n",
    "VARIABLE_NAMES = ec_variable_names + diagnostic_variable_names + [\n",
    "    # Temperature & Relative Humidity Array \n",
    "    'T_1m_c', 'T_2m_c', 'T_3m_c', 'T_4m_c', 'T_5m_c', 'T_6m_c', 'T_7m_c', 'T_8m_c', 'T_9m_c', 'T_10m_c',\n",
    "    'T_11m_c', 'T_12m_c', 'T_13m_c', 'T_14m_c', 'T_15m_c', 'T_16m_c', 'T_17m_c', 'T_18m_c', 'T_19m_c', 'T_20m_c',\n",
    "\n",
    "    'RH_1m_c', 'RH_2m_c', 'RH_3m_c', 'RH_4m_c', 'RH_5m_c', 'RH_6m_c', 'RH_7m_c', 'RH_8m_c', 'RH_9m_c', 'RH_10m_c',\n",
    "    'RH_11m_c','RH_12m_c','RH_13m_c','RH_14m_c','RH_15m_c','RH_16m_c','RH_17m_c','RH_18m_c','RH_19m_c','RH_20m_c',\n",
    "\n",
    "    # Pressure Sensors\n",
    "    'P_20m_c',\n",
    "    'P_10m_c', 'P_10m_d', 'P_10m_uw', 'P_10m_ue',\n",
    "\n",
    "    # Blowing snow/FlowCapt Sensors\n",
    "    'SF_avg_1m_ue', 'SF_avg_2m_ue',\n",
    "\n",
    "    # Apogee sensors\n",
    "    \"Vtherm_c\", \"Vtherm_d\", \"Vtherm_ue\", \"Vtherm_uw\", \n",
    "    \"Vpile_c\", \"Vpile_d\", \"Vpile_ue\", \"Vpile_uw\",\n",
    "    \"IDir_c\", \"IDir_d\", \"IDir_ue\", \"IDir_uw\",\n",
    "\n",
    "    # Snow-level temperature arrays (towers D and UW)\n",
    "    'Tsnow_0_4m_d', 'Tsnow_0_5m_d', 'Tsnow_0_6m_d', 'Tsnow_0_7m_d', 'Tsnow_0_8m_d', 'Tsnow_0_9m_d', 'Tsnow_1_0m_d', 'Tsnow_1_1m_d', 'Tsnow_1_2m_d', 'Tsnow_1_3m_d', 'Tsnow_1_4m_d', 'Tsnow_1_5m_d',\n",
    "    'Tsnow_0_4m_uw', 'Tsnow_0_5m_uw', 'Tsnow_0_6m_uw', 'Tsnow_0_7m_uw', 'Tsnow_0_8m_uw', 'Tsnow_0_9m_uw', 'Tsnow_1_0m_uw', 'Tsnow_1_1m_uw', 'Tsnow_1_2m_uw', 'Tsnow_1_3m_uw', 'Tsnow_1_4m_uw', 'Tsnow_1_5m_uw',\n",
    "    \n",
    "    # Downward/Upward Facing Longwave Radiometers\n",
    "    'Rpile_out_9m_d','Tcase_out_9m_d',    \n",
    "    'Rpile_in_9m_d', 'Tcase_in_9m_d',\n",
    "    'Tcase_uw', 'Rpile_in_uw', 'Rpile_out_uw',\n",
    "    \n",
    "    # Upward facing shortwave radiometer (tower D) - for measuring incoming solar radiation!\n",
    "    'Rsw_in_9m_d', 'Rsw_out_9m_d',\n",
    "\n",
    "    # Snow Pillow SWE\n",
    "    'SWE_p1_c', 'SWE_p2_c', 'SWE_p3_c', 'SWE_p4_c',\n",
    "\n",
    "    # Soil Moisture\n",
    "    'Qsoil_d',\n",
    "\n",
    "    # Ground heat flux\n",
    "    'Gsoil_d',\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open SoS datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file_paths = [\n",
    "    os.path.join(\n",
    "        sos_data_dir,\n",
    "        f'isfs_sos_qc_geo_tiltcor_5min_v2_{date}.nc'\n",
    "    ) for date in datelist\n",
    "]\n",
    "datasets = []\n",
    "for file in all_file_paths:\n",
    "    ds = xr.open_dataset(file)\n",
    "    # this ensures we don't access variables that aren't in this dataset, which would throw an error\n",
    "    ds_new = ds[set(ds.data_vars).intersection(VARIABLE_NAMES)]\n",
    "    datasets.append(ds_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds = xr.concat(datasets, dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure time index is evenly spaced by filling in any missing timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds = utils.fill_missing_timestamps(sos_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds.to_netcdf(\"sos_ds_temp_storage_30min_straightup.cdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sos_ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msos_ds_temp_storage_30min_straightup.cdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xr' is not defined"
     ]
    }
   ],
   "source": [
    "sos_ds = xr.open_dataset(\"sos_ds_temp_storage_30min_straightup.cdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace flag variables with my calculated aggregated flags\n",
    "(sonic anemometer and irgason flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_counts_df = pd.read_parquet(\"/Users/elischwat/Development/data/sublimationofsnow/sosqc_fast_flagcounts\").loc[start_date: end_date]\n",
    "assert all(sos_ds.time == flag_counts_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irga_vars = [\n",
    "    'irgadiag_10m_c',\t 'irgadiag_10m_d',\t 'irgadiag_10m_ue',\t 'irgadiag_10m_uw',\t 'irgadiag_15m_c',\t 'irgadiag_1m_c',\t 'irgadiag_1m_d',\t 'irgadiag_1m_ue',\t \n",
    "    'irgadiag_1m_uw',\t 'irgadiag_20m_c',\t 'irgadiag_2m_c',\t 'irgadiag_3m_c',\t 'irgadiag_3m_d',\t 'irgadiag_3m_ue',\t 'irgadiag_3m_uw',\t 'irgadiag_5m_c',\t\n",
    "]\n",
    "\n",
    "ldiag_vars = [\n",
    "    'ldiag_10m_c', 'ldiag_10m_d', 'ldiag_10m_ue', 'ldiag_10m_uw', 'ldiag_15m_c', 'ldiag_1m_c', 'ldiag_1m_d', 'ldiag_1m_ue', \n",
    "    'ldiag_1m_uw', 'ldiag_20m_c', 'ldiag_2m_c', 'ldiag_3m_c', 'ldiag_3m_d', 'ldiag_3m_ue', 'ldiag_3m_uw', 'ldiag_5m_c',\n",
    "]\n",
    "\n",
    "for var in irga_vars + ldiag_vars:\n",
    "    sos_ds[var] = flag_counts_df[var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dictionary defining the resampling function to use for each variable\n",
    "\n",
    "Covariances are resampled according to the rules of **Reynold averaging** (https://www.eol.ucar.edu/content/combining-short-term-moments-longer-time-periods).\n",
    "\n",
    "Meteorological and turbulence measurements (other than covariances) are resampled using the **mean**.\n",
    "\n",
    "EC count variables are **summed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vars_processing_dict = {\n",
    "    'reynolds_average': [\n",
    "        'u_u__1m_uw',    'v_v__1m_uw',    'w_w__1m_uw', 'w_w_w__1m_uw',    'u_w__1m_uw',    'u_v__1m_uw',     'v_w__1m_uw',  'u_tc__1m_uw',  'v_tc__1m_uw',   'u_co2__1m_uw', 'u_h2o__1m_uw',  'v_co2__1m_uw', 'v_h2o__1m_uw',   'w_tc__1m_uw',   'w_co2__1m_uw', 'w_h2o__1m_uw',\n",
    "        'u_u__2m_uw',    'v_v__2m_uw',    'w_w__2m_uw', 'w_w_w__2m_uw',    'u_w__2m_uw',    'u_v__2m_uw',     'v_w__2m_uw',  'u_tc__2m_uw',  'v_tc__2m_uw',   'u_co2__2m_uw', 'u_h2o__2m_uw',  'v_co2__2m_uw', 'v_h2o__2m_uw',   'w_tc__2m_uw',   'w_co2__2m_uw', 'w_h2o__2m_uw',\n",
    "        'u_u__2_5m_uw', 'v_v__2_5m_uw',   'w_w__2_5m_uw', 'w_w_w__2_5m_uw',  'u_w__2_5m_uw',  'u_v__2_5m_uw',   'v_w__2_5m_uw','u_tc__2_5m_uw','v_tc__2_5m_uw', 'u_co2__2_5m_uw', 'u_h2o__2_5m_uw','v_h2o__2_5m_uw', 'w_tc__2_5m_uw', 'w_co2__2_5m_uw', 'w_h2o__2_5m_uw',\n",
    "        'u_u__3m_uw',    'v_v__3m_uw',    'w_w__3m_uw', 'w_w_w__3m_uw',    'u_w__3m_uw',    'u_v__3m_uw',     'v_w__3m_uw',  'u_tc__3m_uw',  'v_tc__3m_uw',   'u_co2__3m_uw', 'u_h2o__3m_uw',  'v_co2__3m_uw', 'v_h2o__3m_uw',   'w_tc__3m_uw',   'w_co2__3m_uw', 'w_h2o__3m_uw',\n",
    "        'u_u__10m_uw',   'v_v__10m_uw',   'w_w__10m_uw', 'w_w_w__10m_uw',   'u_w__10m_uw',   'u_v__10m_uw',    'v_w__10m_uw', 'u_tc__10m_uw', 'v_tc__10m_uw',  'u_co2__10m_uw', 'u_h2o__10m_uw', 'v_co2__10m_uw', 'v_h2o__10m_uw',  'w_tc__10m_uw',  'w_co2__10m_uw', 'w_h2o__10m_uw',\n",
    "        'u_u__1m_ue',    'v_v__1m_ue',    'w_w__1m_ue', 'w_w_w__1m_ue',    'u_w__1m_ue',    'u_v__1m_ue',     'v_w__1m_ue',  'u_tc__1m_ue',  'v_tc__1m_ue',   'u_co2__1m_ue', 'u_h2o__1m_ue',  'v_co2__1m_ue', 'v_h2o__1m_ue',   'w_tc__1m_ue',   'w_co2__1m_ue', 'w_h2o__1m_ue',\n",
    "        'u_u__2m_ue',    'v_v__2m_ue',    'w_w__2m_ue', 'w_w_w__2m_ue',    'u_w__2m_ue',    'u_v__2m_ue',     'v_w__2m_ue',  'u_tc__2m_ue',  'v_tc__2m_ue',   'u_co2__2m_ue', 'u_h2o__2m_ue',  'v_co2__2m_ue', 'v_h2o__2m_ue',   'w_tc__2m_ue',   'w_co2__2m_ue', 'w_h2o__2m_ue',\n",
    "        'u_u__3m_ue',    'v_v__3m_ue',    'w_w__3m_ue', 'w_w_w__3m_ue',    'u_w__3m_ue',    'u_v__3m_ue',     'v_w__3m_ue',  'u_tc__3m_ue',  'v_tc__3m_ue',   'u_co2__3m_ue', 'u_h2o__3m_ue',  'v_co2__3m_ue', 'v_h2o__3m_ue',   'w_tc__3m_ue',   'w_co2__3m_ue', 'w_h2o__3m_ue',\n",
    "        'u_u__10m_ue',   'v_v__10m_ue',   'w_w__10m_ue', 'w_w_w__10m_ue',   'u_w__10m_ue',   'u_v__10m_ue',    'v_w__10m_ue', 'u_tc__10m_ue', 'v_tc__10m_ue',  'u_co2__10m_ue', 'u_h2o__10m_ue', 'v_co2__10m_ue', 'v_h2o__10m_ue',  'w_tc__10m_ue',  'w_co2__10m_ue', 'w_h2o__10m_ue',\n",
    "        'u_u__1m_d',     'v_v__1m_d',     'w_w__1m_d', 'w_w_w__1m_d',     'u_w__1m_d',     'u_v__1m_d',      'v_w__1m_d',   'u_tc__1m_d',   'v_tc__1m_d',    'u_co2__1m_d', 'u_h2o__1m_d',   'v_co2__1m_d', 'v_h2o__1m_d',    'w_tc__1m_d',    'w_co2__1m_d', 'w_h2o__1m_d',\n",
    "        'u_u__2m_d',     'v_v__2m_d',     'w_w__2m_d', 'w_w_w__2m_d',     'u_w__2m_d',     'u_v__2m_d',      'v_w__2m_d',   'u_tc__2m_d',   'v_tc__2m_d',    'u_co2__2m_d', 'u_h2o__2m_d',   'v_co2__2m_d', 'v_h2o__2m_d',    'w_tc__2m_d',    'w_co2__2m_d', 'w_h2o__2m_d',\n",
    "        'u_u__3m_d',     'v_v__3m_d',     'w_w__3m_d', 'w_w_w__3m_d',     'u_w__3m_d',     'u_v__3m_d',      'v_w__3m_d',   'u_tc__3m_d',   'v_tc__3m_d',    'u_co2__3m_d', 'u_h2o__3m_d',   'v_co2__3m_d', 'v_h2o__3m_d',    'w_tc__3m_d',    'w_co2__3m_d', 'w_h2o__3m_d',\n",
    "        'u_u__10m_d',    'v_v__10m_d',    'w_w__10m_d', 'w_w_w__10m_d',    'u_w__10m_d',    'u_v__10m_d',     'v_w__10m_d',  'u_tc__10m_d',  'v_tc__10m_d',   'u_co2__10m_d', 'u_h2o__10m_d',  'v_co2__10m_d', 'v_h2o__10m_d',   'w_tc__10m_d',   'w_co2__10m_d', 'w_h2o__10m_d',\n",
    "        'u_u__1m_c',     'v_v__1m_c',     'w_w__1m_c', 'w_w_w__1m_c',     'u_w__1m_c',     'u_v__1m_c',      'v_w__1m_c',   'u_tc__1m_c',   'v_tc__1m_c',    'u_co2__1m_c', 'u_h2o__1m_c',   'v_co2__1m_c', 'v_h2o__1m_c',    'w_tc__1m_c',    'w_co2__1m_c', 'w_h2o__1m_c',\n",
    "        'u_u__2m_c',     'v_v__2m_c',     'w_w__2m_c', 'w_w_w__2m_c',     'u_w__2m_c',     'u_v__2m_c',      'v_w__2m_c',   'u_tc__2m_c',   'v_tc__2m_c',    'u_co2__2m_c', 'u_h2o__2m_c',   'v_co2__2m_c', 'v_h2o__2m_c',    'w_tc__2m_c',    'w_co2__2m_c', 'w_h2o__2m_c',\n",
    "        'u_u__3m_c',     'v_v__3m_c',     'w_w__3m_c', 'w_w_w__3m_c',     'u_w__3m_c',     'u_v__3m_c',      'v_w__3m_c',   'u_tc__3m_c',   'v_tc__3m_c',    'u_co2__3m_c', 'u_h2o__3m_c',   'v_co2__3m_c', 'v_h2o__3m_c',    'w_tc__3m_c',    'w_co2__3m_c', 'w_h2o__3m_c',\n",
    "        'u_u__5m_c',     'v_v__5m_c',     'w_w__5m_c', 'w_w_w__5m_c',     'u_w__5m_c',     'u_v__5m_c',      'v_w__5m_c',   'u_tc__5m_c',   'v_tc__5m_c',    'u_co2__5m_c', 'u_h2o__5m_c',   'v_co2__5m_c', 'v_h2o__5m_c',    'w_tc__5m_c',    'w_co2__5m_c', 'w_h2o__5m_c',\n",
    "        'u_u__10m_c',    'v_v__10m_c',    'w_w__10m_c', 'w_w_w__10m_c',    'u_w__10m_c',    'u_v__10m_c',     'v_w__10m_c',  'u_tc__10m_c',  'v_tc__10m_c',   'u_co2__10m_c', 'u_h2o__10m_c',  'v_co2__10m_c', 'v_h2o__10m_c',   'w_tc__10m_c',   'w_co2__10m_c', 'w_h2o__10m_c',\n",
    "        'u_u__15m_c',    'v_v__15m_c',    'w_w__15m_c', 'w_w_w__15m_c',    'u_w__15m_c',    'u_v__15m_c',     'v_w__15m_c',  'u_tc__15m_c',  'v_tc__15m_c',   'u_co2__15m_c', 'u_h2o__15m_c',  'v_co2__15m_c', 'v_h2o__15m_c',   'w_tc__15m_c',   'w_co2__15m_c', 'w_h2o__15m_c',\n",
    "        'u_u__20m_c',    'v_v__20m_c',    'w_w__20m_c', 'w_w_w__20m_c',    'u_w__20m_c',    'u_v__20m_c',     'v_w__20m_c',  'u_tc__20m_c',  'v_tc__20m_c',   'u_co2__20m_c', 'u_h2o__20m_c',  'v_co2__20m_c', 'v_h2o__20m_c',   'w_tc__20m_c',   'w_co2__20m_c', 'w_h2o__20m_c',\n",
    "    ],\n",
    "    'average': [\n",
    "        # Sonic anemometer data\n",
    "        'co2_1m_uw', 'h2o_1m_uw' ,       'tc_1m_uw',     'spd_1m_uw',    'u_1m_uw',  'v_1m_uw',   'w_1m_uw',  \n",
    "        'co2_3m_uw', 'h2o_3m_uw' ,       'tc_3m_uw',     'spd_3m_uw',    'u_3m_uw',  'v_3m_uw',   'w_3m_uw',  \n",
    "        'co2_10m_uw', 'h2o_10m_uw' ,      'tc_10m_uw',    'spd_10m_uw',   'u_10m_uw', 'v_10m_uw',  'w_10m_uw',  \n",
    "        'co2_1m_ue', 'h2o_1m_ue' ,       'tc_1m_ue',     'spd_1m_ue',    'u_1m_ue',  'v_1m_ue',   'w_1m_ue',  \n",
    "        'co2_3m_ue', 'h2o_3m_ue' ,       'tc_3m_ue',     'spd_3m_ue',    'u_3m_ue',  'v_3m_ue',   'w_3m_ue',  \n",
    "        'co2_10m_ue', 'h2o_10m_ue' ,      'tc_10m_ue',    'spd_10m_ue',   'u_10m_ue', 'v_10m_ue',  'w_10m_ue',  \n",
    "        'co2_1m_d', 'h2o_1m_d' ,        'tc_1m_d',      'spd_1m_d',     'u_1m_d',   'v_1m_d',    'w_1m_d',  \n",
    "        'co2_3m_d', 'h2o_3m_d' ,        'tc_3m_d',      'spd_3m_d',     'u_3m_d',   'v_3m_d',    'w_3m_d',  \n",
    "        'co2_10m_d', 'h2o_10m_d' ,       'tc_10m_d',     'spd_10m_d',    'u_10m_d',  'v_10m_d',   'w_10m_d',  \n",
    "        'co2_1m_c', 'h2o_1m_c' ,        'tc_1m_c',      'spd_1m_c',     'u_1m_c',   'v_1m_c',    'w_1m_c',  \n",
    "        'co2_2m_c', 'h2o_2m_c' ,        'tc_2m_c',      'spd_2m_c',     'u_2m_c',   'v_2m_c',    'w_2m_c',  \n",
    "        'co2_3m_c', 'h2o_3m_c' ,        'tc_3m_c',      'spd_3m_c',     'u_3m_c',   'v_3m_c',    'w_3m_c',  \n",
    "        'co2_5m_c', 'h2o_5m_c' ,        'tc_5m_c',      'spd_5m_c',     'u_5m_c',   'v_5m_c',    'w_5m_c',  \n",
    "        'co2_10m_c', 'h2o_10m_c' ,       'tc_10m_c',     'spd_10m_c',    'u_10m_c',  'v_10m_c',   'w_10m_c',  \n",
    "        'co2_15m_c', 'h2o_15m_c' ,       'tc_15m_c',     'spd_15m_c',    'u_15m_c',  'v_15m_c',   'w_15m_c',  \n",
    "        'co2_20m_c', 'h2o_20m_c' ,       'tc_20m_c',     'spd_20m_c',    'u_20m_c',  'v_20m_c',   'w_20m_c',  \n",
    "\n",
    "        # Temperature & Relative Humidity Array \n",
    "        'T_1m_c', 'T_2m_c', 'T_3m_c', 'T_4m_c', 'T_5m_c', 'T_6m_c', 'T_7m_c', 'T_8m_c', 'T_9m_c', 'T_10m_c',\n",
    "        'T_11m_c', 'T_12m_c', 'T_13m_c', 'T_14m_c', 'T_15m_c', 'T_16m_c', 'T_17m_c', 'T_18m_c', 'T_19m_c', 'T_20m_c',\n",
    "\n",
    "        'RH_1m_c', 'RH_2m_c', 'RH_3m_c', 'RH_4m_c', 'RH_5m_c', 'RH_6m_c', 'RH_7m_c', 'RH_8m_c', 'RH_9m_c', 'RH_10m_c',\n",
    "        'RH_11m_c','RH_12m_c','RH_13m_c','RH_14m_c','RH_15m_c','RH_16m_c','RH_17m_c','RH_18m_c','RH_19m_c','RH_20m_c',\n",
    "\n",
    "        # Pressure Sensors\n",
    "        'P_20m_c',\n",
    "        'P_10m_c', 'P_10m_d', 'P_10m_uw', 'P_10m_ue',\n",
    "\n",
    "        # Blowing snow/FlowCapt Sensorsv\n",
    "        'SF_avg_1m_ue', 'SF_avg_2m_ue',\n",
    "\n",
    "        # Apogee sensors\n",
    "        \"Vtherm_c\", \"Vtherm_d\", \"Vtherm_ue\", \"Vtherm_uw\", \n",
    "        \"Vpile_c\", \"Vpile_d\", \"Vpile_ue\", \"Vpile_uw\",\n",
    "        \"IDir_c\", \"IDir_d\", \"IDir_ue\", \"IDir_uw\",\n",
    "\n",
    "        # Snow-level temperature arrays (towers D and UW)\n",
    "        'Tsnow_0_4m_d', 'Tsnow_0_5m_d', 'Tsnow_0_6m_d', 'Tsnow_0_7m_d', 'Tsnow_0_8m_d', 'Tsnow_0_9m_d', 'Tsnow_1_0m_d', 'Tsnow_1_1m_d', 'Tsnow_1_2m_d', 'Tsnow_1_3m_d', 'Tsnow_1_4m_d', 'Tsnow_1_5m_d',\n",
    "        'Tsnow_0_4m_uw', 'Tsnow_0_5m_uw', 'Tsnow_0_6m_uw', 'Tsnow_0_7m_uw', 'Tsnow_0_8m_uw', 'Tsnow_0_9m_uw', 'Tsnow_1_0m_uw', 'Tsnow_1_1m_uw', 'Tsnow_1_2m_uw', 'Tsnow_1_3m_uw', 'Tsnow_1_4m_uw', 'Tsnow_1_5m_uw',\n",
    "        \n",
    "        # Downward Facing Longwave Radiometer (tower D) - for measuring snow surface temperature\n",
    "        'Rpile_out_9m_d',\n",
    "        'Tcase_out_9m_d',    \n",
    "        # Upward Facing Longwave Radiometer (tower D)\n",
    "        'Rpile_in_9m_d',\n",
    "        'Tcase_in_9m_d',\n",
    "        # Downward Facing Longwave Radiometer (tower UW) - for measuring snow surface temperature\n",
    "        'Tcase_uw', 'Rpile_in_uw', 'Rpile_out_uw',\n",
    "        \n",
    "        # Upward facing shortwave radiometer (tower D) - for measuring incoming solar radiation!\n",
    "        'Rsw_in_9m_d',\n",
    "        'Rsw_out_9m_d',\n",
    "\n",
    "        # Snow Pillow SWE\n",
    "        'SWE_p1_c', 'SWE_p2_c', 'SWE_p3_c', 'SWE_p4_c',\n",
    "\n",
    "        # Soil Moisture\n",
    "        'Qsoil_d',\n",
    "\n",
    "        # Soil Moisture\n",
    "        'Gsoil_d',\n",
    "    ],\n",
    "    'median' : [\n",
    "        'dir_1m_uw',    \n",
    "        'dir_3m_uw',    \n",
    "        'dir_10m_uw',   \n",
    "        'dir_1m_ue',    \n",
    "        'dir_3m_ue',    \n",
    "        'dir_10m_ue',   \n",
    "        'dir_1m_d',     \n",
    "        'dir_3m_d',     \n",
    "        'dir_10m_d',    \n",
    "        'dir_1m_c',     \n",
    "        'dir_2m_c',     \n",
    "        'dir_3m_c',     \n",
    "        'dir_5m_c',     \n",
    "        'dir_10m_c',    \n",
    "        'dir_15m_c',    \n",
    "        'dir_20m_c',    \n",
    "    ],\n",
    "    'sum' : [\n",
    "        # Counts of UNflagged instantaneous (20hz) eddy covariance measurements\n",
    "        'counts_1m_c',    'counts_1m_c_1',    'counts_1m_c_2',    \n",
    "        'counts_2m_c',    'counts_2m_c_1',    'counts_2m_c_2',    \n",
    "        'counts_3m_c',    'counts_3m_c_1',    'counts_3m_c_2',    \n",
    "        'counts_5m_c',    'counts_5m_c_1',    'counts_5m_c_2',    \n",
    "        'counts_10m_c',   'counts_10m_c_1',   'counts_10m_c_2',   \n",
    "        'counts_15m_c',   'counts_15m_c_1',   'counts_15m_c_2',   \n",
    "        'counts_20m_c',   'counts_20m_c_1',   'counts_20m_c_2',   \n",
    "        'counts_1m_uw',   'counts_1m_uw_1',   'counts_1m_uw_2',   \n",
    "        'counts_3m_uw',   'counts_3m_uw_1',   'counts_3m_uw_2',   \n",
    "        'counts_10m_uw',  'counts_10m_uw_1',  'counts_10m_uw_2',  \n",
    "        'counts_1m_ue',   'counts_1m_ue_1',   'counts_1m_ue_2',   \n",
    "        'counts_3m_ue',   'counts_3m_ue_1',   'counts_3m_ue_2',   \n",
    "        'counts_10m_ue',  'counts_10m_ue_1',  'counts_10m_ue_2',  \n",
    "        'counts_1m_d',    'counts_1m_d_1',    'counts_1m_d_2',    \n",
    "        'counts_3m_d',    'counts_3m_d_1',    'counts_3m_d_2',    \n",
    "        'counts_10m_d',   'counts_10m_d_1',   'counts_10m_d_2',   \n",
    "\n",
    "        # Counts of FLAGGED 20hz measurements \n",
    "        'irgadiag_1m_c',    'ldiag_1m_c',\n",
    "        'irgadiag_2m_c',    'ldiag_2m_c',\n",
    "        'irgadiag_3m_c',    'ldiag_3m_c',\n",
    "        'irgadiag_5m_c',    'ldiag_5m_c',\n",
    "        'irgadiag_10m_c',   'ldiag_10m_c',\n",
    "        'irgadiag_15m_c',   'ldiag_15m_c',\n",
    "        'irgadiag_20m_c',   'ldiag_20m_c',\n",
    "        'irgadiag_1m_uw',   'ldiag_1m_uw',\n",
    "        'irgadiag_3m_uw',   'ldiag_3m_uw',\n",
    "        'irgadiag_10m_uw',  'ldiag_10m_uw',\n",
    "        'irgadiag_1m_ue',   'ldiag_1m_ue',\n",
    "        'irgadiag_3m_ue',   'ldiag_3m_ue',\n",
    "        'irgadiag_10m_ue',  'ldiag_10m_ue',\n",
    "        'irgadiag_1m_d',    'ldiag_1m_d',\n",
    "        'irgadiag_3m_d',    'ldiag_3m_d',\n",
    "        'irgadiag_10m_d',   'ldiag_10m_d',\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function for resampling covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_covariance_variable_name(cov_name):\n",
    "    \"\"\"Get the names of the two mean variables associated with a covariance variable. Built to use\n",
    "    with SOS datasets. For example, one might provide `w_h2o__3m_c` and this function will return\n",
    "    `w_3m_c` and `h2o_3m_c`.\n",
    "\n",
    "    Args:\n",
    "        cov_name (str): name of variable that you want to separate into the two names of the \n",
    "        asssociated mean variables.\n",
    "\n",
    "    Returns:\n",
    "        var1, var 2 (str, str): two strings with the names of the two mean variables\n",
    "    \"\"\"\n",
    "    [first_parts, second_part] = cov_name.split('__')\n",
    "    [var1, var2] = first_parts.split('_')\n",
    "    [var1, var2] = [\n",
    "        var1 +'_' + second_part,\n",
    "        var2 +'_' + second_part,\n",
    "    ]\n",
    "    return var1, var2\n",
    "\n",
    "def resample_moment(df, cov, mean1, mean2, new_frequency, n_in_new_re_length, skipna=True):\n",
    "    \"\"\"Combines moments into longer time periods, using reynolds averaging. Built to use with SOS \n",
    "    datasets. Resampling covariances which have been calculated for a specific Reynolds\n",
    "    averaging length (e.g. the SOS datasets are averaged to 5minutes), you need both the mean\n",
    "    values and covariance. For example, the variable `w_h2o__3m_c` is associated with mean values\n",
    "    `w_3m_c` and `h2o_3m_c`. To reasmple `w_h2o__3m_c` to another averaging length, we need the three\n",
    "    variables.\n",
    "\n",
    "    Args:\n",
    "        df (pd.Dataframe): Dataframe containing the three columns required for calculations (contains)\n",
    "                    the names supplied as parameters `cov`, `mean1`, and `mean2`.\n",
    "        cov (str): Name of covariance variable to resample using Reynolds averaing\n",
    "        mean1 (str): Name of one of the two mean variables associated with `cov`\n",
    "        mean2 (str): Name of the other mean variable associated with `cov`\n",
    "        new_frequency (str): String interpretable by pandas/xarray that describes the reynolds length you \n",
    "            are resampling to. EG: '60Min'\n",
    "        n_in_new_re_length (_type_): Number of 5 minute intervals that fit in the new_frequency. E.G. for\n",
    "            new_frequency='60Min', you would provide 12.\n",
    "        skipna (bool, optional): Whether to skip NaNs when calculating the new variables. Providing True\n",
    "            will allow more moments to be calculated, but those moments may be inaccurate/non-representative.\n",
    "            Providing False will result in more missing data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with resampled data.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame({\n",
    "            cov: df.groupby(pd.Grouper(freq=new_frequency)).apply(\n",
    "                lambda row: \n",
    "                    (1/n_in_new_re_length)*(row[cov] + row[mean1]*row[mean2]).sum(skipna=skipna)\n",
    "                    - (\n",
    "                        (1/n_in_new_re_length)*row[mean1].sum(skipna=skipna)\n",
    "                        * (1/n_in_new_re_length)*row[mean2].sum(skipna=skipna)\n",
    "                    )\n",
    "            )\n",
    "        })\n",
    "\n",
    "def resample(ds, new_frequency, n_in_new_re_length, skipna=True):\n",
    "    \"\"\"Resample SOS xarray datasets, applying the proper aggregation function\n",
    "    for different variables. Some are resampled by taking the mean, some by \n",
    "    summing, and others by Reynolds averaging. \n",
    "    \"\"\"\n",
    "    # Resample data vars that need to be averaged (plain old averaging)\n",
    "    # Use built in xarray functionality\n",
    "    resampled_averages = ds[\n",
    "        data_vars_processing_dict['average']\n",
    "    ].to_dataframe().resample(new_frequency).mean().to_xarray()\n",
    "\n",
    "    resampled_medians = ds[\n",
    "        data_vars_processing_dict['median']\n",
    "    ].to_dataframe().resample(new_frequency).median().to_xarray()\n",
    "    \n",
    "    # Resample data vars that need to be summed\n",
    "    # Use built in xarray functionality\n",
    "    resampled_sums = ds[\n",
    "        data_vars_processing_dict['sum']\n",
    "    ].to_dataframe().resample(new_frequency).sum().to_xarray()\n",
    "    \n",
    "    # Resample data vars that need to be summed using the rules of Reynolds Averaging\n",
    "    # Use our custom function defined above\n",
    "    resampled_reynolds_averages_list = []\n",
    "    def split_covariance_name_and_resample(name):\n",
    "        mean_var1, mean_var2 = separate_covariance_variable_name(name)\n",
    "        resampled = resample_moment(\n",
    "            ds[[mean_var1, mean_var2, name]].to_dataframe(), \n",
    "            name, \n",
    "            mean_var1, \n",
    "            mean_var2, \n",
    "            new_frequency, \n",
    "            n_in_new_re_length, \n",
    "            skipna=skipna\n",
    "        )\n",
    "        return resampled.to_xarray()\n",
    "    resampled_reynolds_averages_list =  Parallel(n_jobs = 8)(\n",
    "        delayed(split_covariance_name_and_resample)(name) \n",
    "        for name in tqdm(data_vars_processing_dict['reynolds_average'])\n",
    "    )\n",
    "    \n",
    "    new_ds = xr.merge(\n",
    "        [\n",
    "            resampled_sums, \n",
    "            resampled_medians,\n",
    "            resampled_averages\n",
    "        ] + resampled_reynolds_averages_list\n",
    "    )\n",
    "\n",
    "    ## Copy attributes from the original dataset\n",
    "    new_ds.attrs = ds.attrs\n",
    "    for var in new_ds:\n",
    "        new_ds[var].attrs = ds[var].attrs\n",
    "    return new_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds30min = resample(sos_ds, '30Min', 6, skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds5min = sos_ds\n",
    "sos_ds = sos_ds30min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recalculate wind direction as the sin(u_30min, w_30min) (instead of median(wind_dir_30min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wind_dir_values = np.rad2deg(np.arctan2(\n",
    "    -sos_ds['u_3m_c'],\n",
    "    -sos_ds['v_3m_c'],\n",
    "))\n",
    "new_wind_dir_values = np.array([dir if dir > 0 else (360 + dir) for dir in new_wind_dir_values.values])\n",
    "assert not any(new_wind_dir_values > 360)\n",
    "new_wind_dir_values[:5]\n",
    "\n",
    "\n",
    "for uvar, vvar, dirvar in [\n",
    "        ('u_1m_uw',    'v_1m_uw',    'dir_1m_uw'),  \n",
    "        ('u_3m_uw',    'v_3m_uw',    'dir_3m_uw'),  \n",
    "        ('u_10m_uw',   'v_10m_uw',   'dir_10m_uw'), \n",
    "        ('u_1m_ue',    'v_1m_ue',    'dir_1m_ue'),  \n",
    "        ('u_3m_ue',    'v_3m_ue',    'dir_3m_ue'),  \n",
    "        ('u_10m_ue',   'v_10m_ue',   'dir_10m_ue'), \n",
    "        ('u_1m_d',     'v_1m_d',     'dir_1m_d'),   \n",
    "        ('u_3m_d',     'v_3m_d',     'dir_3m_d'),   \n",
    "        ('u_10m_d',    'v_10m_d',    'dir_10m_d'),  \n",
    "        ('u_1m_c',     'v_1m_c',     'dir_1m_c'),   \n",
    "        ('u_2m_c',     'v_2m_c',     'dir_2m_c'),   \n",
    "        ('u_3m_c',     'v_3m_c',     'dir_3m_c'),   \n",
    "        ('u_5m_c',     'v_5m_c',     'dir_5m_c'),   \n",
    "        ('u_10m_c',    'v_10m_c',    'dir_10m_c'),  \n",
    "        ('u_15m_c',    'v_15m_c',    'dir_15m_c'),  \n",
    "        ('u_20m_c',    'v_20m_c',    'dir_20m_c'),  \n",
    "    ]:\n",
    "    new_wind_dir_values = np.rad2deg(np.arctan2(-sos_ds[uvar], -sos_ds[vvar]))\n",
    "    new_wind_dir_values = np.array([dir if dir > 0 else (360 + dir) for dir in new_wind_dir_values.values])\n",
    "    assert not any(new_wind_dir_values > 360)\n",
    "    old_attrs = sos_ds[dirvar].attrs\n",
    "    sos_ds[dirvar] = ('time', new_wind_dir_values)\n",
    "    sos_ds[dirvar].attrs = old_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace fluxes with planar fitted fluxes\n",
    "\n",
    "These fluxes have been filtered for sonic and irga flags, and despiked. \n",
    "\n",
    "See 'analysis/paper1/calculate_planar_fitted_lhfluxes.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = sos_ds[[\n",
    "    'w_1m_c',   'w_2m_c','w_3m_c', 'w_5m_c', 'w_10m_c', 'w_15m_c', 'w_20m_c',\n",
    "    'w_1m_ue',  'w_3m_ue','w_10m_ue',\n",
    "    'w_1m_uw',  'w_3m_uw','w_10m_uw',\n",
    "    'w_1m_d',   'w_3m_d','w_10m_d',\n",
    "]]\n",
    "src = tidy.get_tidy_dataset(src, list(src.data_vars))\n",
    "src = utils.modify_df_timezone(src, 'UTC', 'US/Mountain')\n",
    "alt.Chart(\n",
    "    src\n",
    ").mark_line().encode(\n",
    "    alt.X('hoursminutes(time):T'),\n",
    "    alt.Y('median(value):Q').title('Wind speed (m/s)'),\n",
    "    alt.Color('tower:N'),\n",
    "    alt.Facet('height:O', columns=4),\n",
    "    tooltip='variable',\n",
    ").properties(width = 125, height = 125, title=    'Streamwise').display(renderer='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planar_fitted_data_df = pd.read_parquet(planar_fitted_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLANAR_FIT:\n",
    "    planar_fitted_data_df = planar_fitted_data_df[[c for c in planar_fitted_data_df.columns if c.endswith('_fit')]]\n",
    "    planar_fitted_data_df.columns = [c.replace('_fit', '') for c in planar_fitted_data_df.columns]\n",
    "    planar_fitted_data_df = planar_fitted_data_df.loc[ start_date : end_date ]\n",
    "    planar_fitted_data_df.index = planar_fitted_data_df.index - dt.timedelta(minutes=15)\n",
    "    planar_fitted_ds = planar_fitted_data_df.to_xarray()\n",
    "    sos_ds = sos_ds.assign(planar_fitted_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = sos_ds[[\n",
    "    'w_1m_c',   'w_2m_c',   'w_3m_c', 'w_5m_c', 'w_10m_c', 'w_15m_c', 'w_20m_c',\n",
    "    'w_1m_ue',  'w_3m_ue',  'w_10m_ue',\n",
    "    'w_1m_uw',  'w_3m_uw',  'w_10m_uw',\n",
    "    'w_1m_d',   'w_3m_d',   'w_10m_d',\n",
    "]]\n",
    "src = tidy.get_tidy_dataset(src, list(src.data_vars))\n",
    "src = utils.modify_df_timezone(src, 'UTC', 'US/Mountain')\n",
    "alt.Chart(\n",
    "    src\n",
    ").mark_line().encode(\n",
    "    alt.X('hoursminutes(time):T'),\n",
    "    alt.Y('median(value):Q').title('Wind speed (m/s)').scale(domain = [-0.02,0.02]),\n",
    "    alt.Color('tower:N'),\n",
    "    alt.Facet('height:O', columns=4),\n",
    "    tooltip='variable',\n",
    ").properties(width = 125, height = 125, title='Streamwise').display(renderer='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(\n",
    "    src\n",
    ").mark_line().encode(\n",
    "    alt.X('hoursminutes(time):T'),\n",
    "    alt.Y('median(value):Q').title('Wind speed (m/s)').scale(domain = [-0.02,0.02]),\n",
    "    alt.Facet('tower:N', columns=4),\n",
    "    alt.Color('height:O').scale(scheme='turbo'),\n",
    "    tooltip='variable',\n",
    ").properties(width = 125, height = 125, title='Streamwise').display(renderer='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy latent heat fluxes into variables named \"raw\" so that we keep the raw fluxes around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_ec_variables = [\n",
    "    'w_h2o__3m_ue', 'w_h2o__10m_ue', \n",
    "    'w_h2o__3m_d', 'w_h2o__10m_d',\n",
    "    'w_h2o__3m_uw', 'w_h2o__10m_uw', \n",
    "    'w_h2o__2m_c', 'w_h2o__3m_c', 'w_h2o__5m_c', 'w_h2o__10m_c', 'w_h2o__15m_c', 'w_h2o__20m_c'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in important_ec_variables:\n",
    "    sos_ds[\n",
    "        var + '_raw'\n",
    "    ] = sos_ds[var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC1: Instrument flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVE EXTRA DATA SO WE CAN CREATE THE TABLE FOR OUR PAPER 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src = sos_ds.sel(time=slice(\n",
    "#     '20221130 0700',\n",
    "#     '20230509 0530'\n",
    "# ))\n",
    "# display(pd.to_datetime(src.time.values[0]).tz_localize('US/Mountain'))\n",
    "# display(pd.to_datetime(src.time.values[-1]).tz_localize('US/Mountain'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     utils.modify_xarray_timezone(src, 'UTC', 'US/Mountain').time.values[0],\n",
    "#     utils.modify_xarray_timezone(src, 'UTC', 'US/Mountain').time.values[-1]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sos_ds = src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine what caused instrument flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestamps during issue-causing conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowfall_mask_df = pd.read_csv(snowfall_mask_file, index_col=0)\n",
    "snowfall_mask_df.index.name = 'time'\n",
    "snowfall_mask_df.index = pd.to_datetime(snowfall_mask_df.index)\n",
    "snowfall_mask_df\n",
    "snowfall_mask_df = utils.modify_df_timezone(snowfall_mask_df.reset_index(), 'UTC', \"US/Mountain\").set_index('time')\n",
    "times_with_snowfall = snowfall_mask_df[~snowfall_mask_df.SAIL_gts_pluvio].index\n",
    "times_with_snowfall = set(times_with_snowfall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_with_bs = sos_ds[['SF_avg_1m_ue', 'SF_avg_2m_ue']].to_dataframe()\n",
    "times_with_bs['is_bs'] = (times_with_bs['SF_avg_1m_ue'] > 0) | (times_with_bs['SF_avg_2m_ue'] > 0)\n",
    "times_with_bs = times_with_bs[times_with_bs.is_bs].index\n",
    "times_with_bs = set(times_with_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_with_rh_above_78 = sos_ds[['RH_3m_c']].to_dataframe()\n",
    "times_with_rh_above_78 = times_with_rh_above_78[times_with_rh_above_78['RH_3m_c'] > 78].index\n",
    "times_with_rh_above_78 = set(times_with_rh_above_78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_daytime_after_april1 = sos_ds.time.to_series()\n",
    "times_daytime_after_april1 = times_daytime_after_april1[(times_daytime_after_april1.dt.hour > 9) & (times_daytime_after_april1.dt.hour < 17)]\n",
    "times_daytime_after_april1 = times_daytime_after_april1[times_daytime_after_april1 > '20230301']\n",
    "times_daytime_after_april1 = set(times_daytime_after_april1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(times_with_snowfall), len(times_with_bs), len(times_with_rh_above_78), len(times_daytime_after_april1), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irga flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_ec_variables_suffixes = [c[7:] for c in important_ec_variables]\n",
    "important_irgadiag_variables = ['irgadiag_' + c[7:] for c in important_ec_variables]\n",
    "\n",
    "irgadiag_filtering_df   = sos_ds[important_irgadiag_variables].to_dataframe()\n",
    "lhfluxes_df             = sos_ds[important_ec_variables].to_dataframe()\n",
    "irgadiag_filtering_df   = irgadiag_filtering_df.sort_index().loc[STUDY_PERIOD_START_DATE: STUDY_PERIOD_END_DATE]\n",
    "lhfluxes_df             = lhfluxes_df.sort_index().loc[STUDY_PERIOD_START_DATE: STUDY_PERIOD_END_DATE]\n",
    "\n",
    "# irgadiag > 9000 AND lhflux is not nan\n",
    "lhfluxes_df.columns = [c[7:] for c in lhfluxes_df.columns]\n",
    "irgadiag_filtering_df.columns = [c[9:] for c in irgadiag_filtering_df.columns]\n",
    "removed_by_irgadiag_df = ((~np.isnan(lhfluxes_df)) & (irgadiag_filtering_df > PERCENTAGE_DIAG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_by_irgadiag_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_by_irgadiag_df[removed_by_irgadiag_df.index.isin(\n",
    "    times_with_snowfall\n",
    ")].sum().sum() / removed_by_irgadiag_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_by_irgadiag_df[removed_by_irgadiag_df.index.isin(\n",
    "    times_with_bs\n",
    ")].sum().sum() / removed_by_irgadiag_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_by_irgadiag_df[removed_by_irgadiag_df.index.isin(\n",
    "    times_with_snowfall.union(times_with_bs)\n",
    ")].sum().sum() / removed_by_irgadiag_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_by_irgadiag_df[removed_by_irgadiag_df.index.isin(\n",
    "    times_with_rh_above_78\n",
    ")].sum().sum() / removed_by_irgadiag_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_by_irgadiag_df[removed_by_irgadiag_df.index.isin(\n",
    "    times_daytime_after_april1\n",
    ")].sum().sum() / removed_by_irgadiag_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_by_irgadiag_df[removed_by_irgadiag_df.index.isin(\n",
    "    times_daytime_after_april1\n",
    ")].sum() / removed_by_irgadiag_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_by_irgadiag_df[removed_by_irgadiag_df.index.isin(\n",
    "    times_with_rh_above_78.union(times_daytime_after_april1)\n",
    ")].sum().sum() / removed_by_irgadiag_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_by_irgadiag_df[removed_by_irgadiag_df.index.isin(\n",
    "    times_with_snowfall.union(times_with_bs).union(times_with_rh_above_78).union(times_daytime_after_april1)    \n",
    ")].sum().sum() / removed_by_irgadiag_df.sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sonic flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_ec_variables_suffixes = [c[7:] for c in important_ec_variables]\n",
    "important_ldiag_variables = ['ldiag_' + c[7:] for c in important_ec_variables]\n",
    "\n",
    "ldiag_filtering_df   = sos_ds[important_ldiag_variables].to_dataframe()\n",
    "lhfluxes_df             = sos_ds[important_ec_variables].to_dataframe()\n",
    "ldiag_filtering_df   = ldiag_filtering_df.sort_index().loc[STUDY_PERIOD_START_DATE: STUDY_PERIOD_END_DATE]\n",
    "lhfluxes_df             = lhfluxes_df.sort_index().loc[STUDY_PERIOD_START_DATE: STUDY_PERIOD_END_DATE]\n",
    "\n",
    "# ldiag > 9000 AND lhflux is not nan\n",
    "lhfluxes_df.columns = [c[7:] for c in lhfluxes_df.columns]\n",
    "ldiag_filtering_df.columns = [c[6:] for c in ldiag_filtering_df.columns]\n",
    "removed_by_ldiag_df = ((~np.isnan(lhfluxes_df)) & (ldiag_filtering_df > PERCENTAGE_DIAG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_by_ldiag_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_by_ldiag_df.loc['20221221': '20221222'].sum().sum() / removed_by_ldiag_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_by_ldiag_df[removed_by_ldiag_df.index.isin(\n",
    "    times_with_snowfall\n",
    ")].sum().sum() / removed_by_ldiag_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_by_ldiag_df[removed_by_ldiag_df.index.isin(\n",
    "    times_with_bs\n",
    ")].sum().sum() / removed_by_ldiag_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_by_ldiag_df[removed_by_ldiag_df.index.isin(\n",
    "    times_with_snowfall.union(times_with_bs)\n",
    ")].sum().sum() / removed_by_ldiag_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_by_ldiag_df[removed_by_ldiag_df.index.isin(\n",
    "    times_with_rh_above_78\n",
    ")].sum().sum() / removed_by_ldiag_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_by_ldiag_df[removed_by_ldiag_df.index.isin(\n",
    "    times_daytime_after_april1\n",
    ")].sum().sum() / removed_by_ldiag_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_by_ldiag_df[removed_by_ldiag_df.index.isin(\n",
    "    times_with_snowfall.union(times_with_bs).union(times_with_rh_above_78).union(times_daytime_after_april1)    \n",
    ")].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_by_ldiag_df[removed_by_ldiag_df.index.isin(\n",
    "    times_with_snowfall.union(times_with_bs).union(times_with_rh_above_78).union(times_daytime_after_april1)    \n",
    ")].sum().sum() / removed_by_ldiag_df.sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set bad Irga measurements to NaN\n",
    "\n",
    "The NCAR report recommends all Irga-related measurements be set to NaN when irgadiag is non-zero.  They did this for some but not all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds_10percent = sos_ds.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('h2o_flux_var', 'irgadiag_var', 'old_nan_count_badirga', 'new_nan_count_badirga', 'old_mean', 'new_mean')\n",
    "var_ls = []\n",
    "old_nan_count_badirga_ls = []\n",
    "new_nan_count_badirga_ls = []\n",
    "old_mean_ls = []\n",
    "new_mean_ls = []\n",
    "old_median_ls = []\n",
    "new_median_ls = []\n",
    "for suffix in ec_measurement_suffixes:\n",
    "    h2o_flux_var = 'w_h2o__' + suffix\n",
    "    irgadiag_var = 'irgadiag_' + suffix\n",
    "\n",
    "    if irgadiag_var in sos_ds.variables and h2o_flux_var in sos_ds.variables:\n",
    "        old_nan_count_badirga = (np.isnan(sos_ds[h2o_flux_var])).sum().item()\n",
    "        old_mean = sos_ds[h2o_flux_var].mean().item()\n",
    "        old_median = sos_ds[h2o_flux_var].median().item()\n",
    "\n",
    "        sos_ds[h2o_flux_var] = sos_ds[h2o_flux_var].where(sos_ds[irgadiag_var] <= PERCENTAGE_DIAG)\n",
    "    \n",
    "        new_nan_count_badirga = (np.isnan(sos_ds[h2o_flux_var])).sum().item()\n",
    "        new_mean = sos_ds[h2o_flux_var].mean().item()\n",
    "        new_median = sos_ds[h2o_flux_var].median().item()\n",
    "        print(h2o_flux_var, irgadiag_var, old_nan_count_badirga, new_nan_count_badirga, round(old_mean,6), round(new_mean,6))\n",
    "        var_ls.append(h2o_flux_var)\n",
    "        old_nan_count_badirga_ls.append(old_nan_count_badirga)\n",
    "        new_nan_count_badirga_ls.append(new_nan_count_badirga)\n",
    "        old_mean_ls.append(old_mean)\n",
    "        new_mean_ls.append(new_mean)\n",
    "        old_median_ls.append(old_median)\n",
    "        new_median_ls.append(new_median)\n",
    "    else:\n",
    "        print(f\"Variable {h2o_flux_var} or {irgadiag_var} not in dataset.\")\n",
    "        var_ls.append(h2o_flux_var)\n",
    "        old_nan_count_badirga_ls.append(np.nan)\n",
    "        new_nan_count_badirga_ls.append(np.nan)\n",
    "        old_mean_ls.append(np.nan)\n",
    "        new_mean_ls.append(np.nan)\n",
    "        old_median_ls.append(np.nan)\n",
    "        new_median_ls.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,1, sharex=True, figsize=(10,5))\n",
    "axes[0].scatter(var_ls, old_nan_count_badirga_ls, label = 'before', color='tab:blue')\n",
    "axes[0].set_ylabel(\"n of nans\")\n",
    "axes[0].scatter(var_ls, new_nan_count_badirga_ls, label = 'after', color='tab:orange')\n",
    "\n",
    "axes[1].scatter(var_ls, old_mean_ls, label = 'Mean, before', color='tab:blue')\n",
    "axes[1].set_ylabel(\"<w'q'>\")\n",
    "axes[1].scatter(var_ls, new_mean_ls, label = 'Mean, after', color='tab:orange')\n",
    "\n",
    "axes[1].scatter(var_ls, old_median_ls, label = 'Median, before', marker='+', color='tab:blue')\n",
    "axes[1].set_ylabel(\"<w'q'>\")\n",
    "axes[1].scatter(var_ls, new_median_ls, label = 'Median, after', marker='+', color='tab:orange')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.tick_params(rotation=90, axis='x')\n",
    "    ax.legend(title='Filtering', bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set bad Sonic measurements to Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('h2o_flux_var', 'ldiag_var', 'old_nan_count_badsonic', 'new_nan_count_badsonic', 'old_mean', 'new_mean')\n",
    "\n",
    "var_ls = []\n",
    "old_nan_count_badsonic_ls = []\n",
    "new_nan_count_badsonic_ls = []\n",
    "old_mean_ls = []\n",
    "new_mean_ls = []\n",
    "for suffix in ec_measurement_suffixes:\n",
    "    w_var = 'w_' + suffix\n",
    "    h2o_flux_var = 'w_h2o__' + suffix\n",
    "    sonicdiag_var = 'ldiag_' + suffix\n",
    "\n",
    "    if h2o_flux_var in sos_ds.variables and sonicdiag_var in sos_ds.variables:\n",
    "        old_nan_count_badsonic = (np.isnan(sos_ds[h2o_flux_var])).sum().item()\n",
    "        old_mean = sos_ds[h2o_flux_var].mean().item()\n",
    "        \n",
    "        sos_ds[h2o_flux_var] = sos_ds[h2o_flux_var].where(sos_ds[sonicdiag_var] <= PERCENTAGE_DIAG)\n",
    "\n",
    "        new_nan_count_badsonic = (np.isnan(sos_ds[h2o_flux_var])).sum().item()\n",
    "        new_mean = sos_ds[h2o_flux_var].mean().item()\n",
    "        print(h2o_flux_var, sonicdiag_var, old_nan_count_badsonic, new_nan_count_badsonic, round(old_mean,6), round(new_mean,6))\n",
    "        var_ls.append(h2o_flux_var)\n",
    "        old_nan_count_badsonic_ls.append(old_nan_count_badsonic)\n",
    "        new_nan_count_badsonic_ls.append(new_nan_count_badsonic)\n",
    "        old_mean_ls.append(old_mean)\n",
    "        new_mean_ls.append(new_mean)\n",
    "    else:\n",
    "        print(f\"Variable {h2o_flux_var} or {sonicdiag_var} not in dataset.\")\n",
    "        var_ls.append(h2o_flux_var)\n",
    "        old_nan_count_badsonic_ls.append(np.nan)\n",
    "        new_nan_count_badsonic_ls.append(np.nan)\n",
    "        old_mean_ls.append(np.nan)\n",
    "        new_mean_ls.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('h2o_flux_var', 'ldiag_var', 'old_nan_count_badsonic', 'new_nan_count_badsonic', 'old_mean', 'new_mean')\n",
    "\n",
    "var_ls = []\n",
    "old_nan_count_badsonic_ls = []\n",
    "new_nan_count_badsonic_ls = []\n",
    "old_mean_ls = []\n",
    "new_mean_ls = []\n",
    "for suffix in ec_measurement_suffixes:\n",
    "    w_var = 'w_' + suffix\n",
    "    h2o_flux_var = 'w_h2o__' + suffix\n",
    "    sonicdiag_var = 'ldiag_' + suffix\n",
    "\n",
    "    if h2o_flux_var in sos_ds.variables and sonicdiag_var in sos_ds.variables:\n",
    "        old_nan_count_badsonic = (np.isnan(sos_ds[h2o_flux_var])).sum().item()\n",
    "        old_mean = sos_ds[h2o_flux_var].mean().item()\n",
    "        \n",
    "        sos_ds[h2o_flux_var] = sos_ds[h2o_flux_var].where(sos_ds[sonicdiag_var] <= PERCENTAGE_DIAG)\n",
    "\n",
    "        new_nan_count_badsonic = (np.isnan(sos_ds[h2o_flux_var])).sum().item()\n",
    "        new_mean = sos_ds[h2o_flux_var].mean().item()\n",
    "        print(h2o_flux_var, sonicdiag_var, old_nan_count_badsonic, new_nan_count_badsonic, round(old_mean,6), round(new_mean,6))\n",
    "        var_ls.append(h2o_flux_var)\n",
    "        old_nan_count_badsonic_ls.append(old_nan_count_badsonic)\n",
    "        new_nan_count_badsonic_ls.append(new_nan_count_badsonic)\n",
    "        old_mean_ls.append(old_mean)\n",
    "        new_mean_ls.append(new_mean)\n",
    "    else:\n",
    "        print(f\"Variable {h2o_flux_var} or {sonicdiag_var} not in dataset.\")\n",
    "        var_ls.append(h2o_flux_var)\n",
    "        old_nan_count_badsonic_ls.append(np.nan)\n",
    "        new_nan_count_badsonic_ls.append(np.nan)\n",
    "        old_mean_ls.append(np.nan)\n",
    "        new_mean_ls.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,1, sharex=True, figsize=(10,5))\n",
    "axes[0].scatter(var_ls, old_nan_count_badsonic_ls, label = 'Before')\n",
    "axes[0].set_ylabel(\"n of nans\")\n",
    "axes[0].scatter(var_ls, new_nan_count_badsonic_ls, label = 'After')\n",
    "\n",
    "axes[1].scatter(var_ls, old_mean_ls, label = 'Before')\n",
    "axes[1].set_ylabel(\"<w'q'>\")\n",
    "axes[1].scatter(var_ls, new_mean_ls, label = 'After')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.tick_params(rotation=45)\n",
    "    ax.legend(title='Filtering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC2: Plausibility limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sos_ds['w_h2o__20m_c'].where(\n",
    "    np.abs(sos_ds['w_h2o__20m_c']) < 0.1\n",
    ")*1.8).sel(time = slice(STUDY_PERIOD_START_DATE, STUDY_PERIOD_END_DATE)).cumsum().plot(label='feasible 0.1')\n",
    "(sos_ds['w_h2o__20m_c'].where(\n",
    "    np.abs(sos_ds['w_h2o__20m_c']) < 0.2\n",
    ")*1.8).sel(time = slice(STUDY_PERIOD_START_DATE, STUDY_PERIOD_END_DATE)).cumsum().plot(label='feasible 0.2')\n",
    "(sos_ds['w_h2o__20m_c'].where(\n",
    "    np.abs(sos_ds['w_h2o__20m_c']) < 0.3\n",
    ")*1.8).sel(time = slice(STUDY_PERIOD_START_DATE, STUDY_PERIOD_END_DATE)).cumsum().plot(label='feasible 0.3')\n",
    "(sos_ds['w_h2o__20m_c'].where(\n",
    "    np.abs(sos_ds['w_h2o__20m_c']) < 0.4\n",
    ")*1.8).sel(time = slice(STUDY_PERIOD_START_DATE, STUDY_PERIOD_END_DATE)).cumsum().plot(label='feasible 0.4')\n",
    "(sos_ds['w_h2o__20m_c'].where(\n",
    "    np.abs(sos_ds['w_h2o__20m_c']) < 0.5\n",
    ")*1.8).sel(time = slice(STUDY_PERIOD_START_DATE, STUDY_PERIOD_END_DATE)).cumsum().plot(label='feasible 0.5')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_heat_of_vaporization = 2838 * units(\"J/g\")\n",
    "PLAUSIBILITY_LIMIT = 0.2\n",
    "\n",
    "(PLAUSIBILITY_LIMIT  * units(\"g/(m^2 * s)\") * latent_heat_of_vaporization).to(\"W/m^2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('h2o_flux_var', 'ldiag_var', 'old_nan_count_badsonic', 'new_nan_count_badsonic', 'old_mean', 'new_mean')\n",
    "\n",
    "var_ls = []\n",
    "old_nan_count_plausibilitylimit_ls = []\n",
    "new_nan_count_plausibilitylimit_ls = []\n",
    "old_mean_ls = []\n",
    "new_mean_ls = []\n",
    "for suffix in ec_measurement_suffixes:\n",
    "    w_var = 'w_' + suffix\n",
    "    h2o_flux_var = 'w_h2o__' + suffix\n",
    "    sonicdiag_var = 'ldiag_' + suffix\n",
    "\n",
    "    if h2o_flux_var in sos_ds.variables and sonicdiag_var in sos_ds.variables:\n",
    "        old_nan_count_plausibilitylimit = (np.isnan(sos_ds[h2o_flux_var])).sum().item()\n",
    "        old_mean = sos_ds[h2o_flux_var].mean().item()\n",
    "        \n",
    "        sos_ds[h2o_flux_var] = sos_ds[h2o_flux_var].where(np.abs(sos_ds[h2o_flux_var]) < PLAUSIBILITY_LIMIT)\n",
    "    \n",
    "        new_nan_count_plausibilitylimit = (np.isnan(sos_ds[h2o_flux_var])).sum().item()\n",
    "        new_mean = sos_ds[h2o_flux_var].mean().item()\n",
    "        print(h2o_flux_var, sonicdiag_var, old_nan_count_plausibilitylimit, new_nan_count_plausibilitylimit, round(old_mean,6), round(new_mean,6))\n",
    "        var_ls.append(h2o_flux_var)\n",
    "        old_nan_count_plausibilitylimit_ls.append(old_nan_count_plausibilitylimit)\n",
    "        new_nan_count_plausibilitylimit_ls.append(new_nan_count_plausibilitylimit)\n",
    "        old_mean_ls.append(old_mean)\n",
    "        new_mean_ls.append(new_mean)\n",
    "    else:\n",
    "        print(f\"Variable {h2o_flux_var} or {sonicdiag_var} not in dataset.\")\n",
    "        var_ls.append(h2o_flux_var)\n",
    "        old_nan_count_plausibilitylimit_ls.append(np.nan)\n",
    "        new_nan_count_plausibilitylimit_ls.append(np.nan)\n",
    "        old_mean_ls.append(np.nan)\n",
    "        new_mean_ls.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,1, sharex=True, figsize=(10,5))\n",
    "axes[0].scatter(var_ls, old_nan_count_plausibilitylimit_ls, label = 'Before')\n",
    "axes[0].set_ylabel(\"n of nans\")\n",
    "axes[0].scatter(var_ls, new_nan_count_plausibilitylimit_ls, label = 'After')\n",
    "\n",
    "axes[1].scatter(var_ls, old_mean_ls, label = 'Before')\n",
    "axes[1].set_ylabel(\"<w'q'>\")\n",
    "axes[1].scatter(var_ls, new_mean_ls, label = 'After')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.tick_params(rotation=45)\n",
    "    ax.legend(title='Filtering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counts_df = pd.DataFrame({\n",
    "    'variable':                         var_ls,\n",
    "    'n':                                len(sos_ds.time),\n",
    "    'original nan count':               old_nan_count_badirga_ls, \n",
    "    'nans after bad irga removed':      new_nan_count_badirga_ls, \n",
    "    'nans after bad sonic removed':     new_nan_count_badsonic_ls, \n",
    "    'nans after plausibility limit':    new_nan_count_plausibilitylimit_ls\n",
    "})\n",
    "limited_nan_counts_df = nan_counts_df[ \n",
    "    (~nan_counts_df.variable.str.contains('__1m_'))\n",
    "    \n",
    "    &\n",
    "    (~nan_counts_df.variable.str.contains('__2_5m_'))\n",
    "]\n",
    "limited_nan_counts_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_nan_counts_df['Valid measurements'] = limited_nan_counts_df['n'] - limited_nan_counts_df['original nan count']\n",
    "limited_nan_counts_df['Data removed by EC150 flag'] = limited_nan_counts_df['nans after bad irga removed'] - limited_nan_counts_df['original nan count']\n",
    "limited_nan_counts_df['Data removed by CSAT3 flag'] = limited_nan_counts_df['nans after bad sonic removed'] - limited_nan_counts_df['nans after bad irga removed']\n",
    "limited_nan_counts_df['Data removed by plausibility limit'] = limited_nan_counts_df['nans after plausibility limit'] - limited_nan_counts_df['nans after bad sonic removed']\n",
    "limited_nan_counts_df[[\n",
    "    'variable',\n",
    "    'n',\n",
    "    'Valid measurements',\n",
    "    'Data removed by EC150 flag',\n",
    "    'Data removed by CSAT3 flag',\n",
    "    'Data removed by plausibility limit'\n",
    "]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = sos_ds[['SF_avg_1m_ue', 'SF_avg_2m_ue', 'ldiag_3m_c']].to_dataframe().reset_index()\n",
    "src['SF_avg_ue'] = src['SF_avg_1m_ue'] + src['SF_avg_2m_ue']\n",
    "src = utils.modify_df_timezone(src, 'UTC', 'US/Mountain')\n",
    "src['on Dec 21/22'] = (src.time.dt.date == dt.date(2022,12,22)) | (src.time.dt.date == dt.date(2022,12,21))\n",
    "src = src.query(\"SF_avg_ue > 0\")\n",
    "rule = alt.Chart().transform_calculate(rule = '0.1').mark_rule(strokeDash=[2,2]).encode(y='rule:Q')\n",
    "bad_sonic_data = (\n",
    "    rule + alt.Chart(src).mark_circle(size=10).encode(\n",
    "        alt.X(\"SF_avg_ue\").title(\"Blowing snow flux (g/m^2/s)\").scale(type='log'),\n",
    "        alt.Y(\"ldiag_3m_c\").title([\"Fraction of 20hz sonic anemometer\", \"measurements flagged (Tower C, 3m)\"]),\n",
    "        alt.Color(\"on Dec 21/22:N\")\n",
    "    ).properties(width=200, height=200)\n",
    ").configure_axis(grid=False).configure_legend(columns=2, orient='top')\n",
    "bad_sonic_data.save(\"bad_sonic_data.png\", ppi=200)\n",
    "display(bad_sonic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = sos_ds[['RH_3m_c', 'SF_avg_1m_ue', 'SF_avg_2m_ue', 'ldiag_3m_c', 'ldiag_5m_c', 'ldiag_10m_c', 'ldiag_15m_c', 'ldiag_20m_c']].to_dataframe().reset_index()\n",
    "src['SF_avg_ue'] = src['SF_avg_1m_ue'] + src['SF_avg_2m_ue']\n",
    "src = utils.modify_df_timezone(src, 'UTC', 'US/Mountain')\n",
    "src['on Dec 21/22'] = (src.time.dt.date == dt.date(2022,12,22)) | (src.time.dt.date == dt.date(2022,12,21))\n",
    "src = src.query(\"SF_avg_ue == 0\")\n",
    "rule = alt.Chart().transform_calculate(rule = '9000').mark_rule(strokeDash=[2,2]).encode(y='rule:Q')\n",
    "bad_sonic_data = (\n",
    "    alt.Chart(src).transform_fold([\n",
    "        'ldiag_3m_c', 'ldiag_5m_c', 'ldiag_10m_c', 'ldiag_15m_c', 'ldiag_20m_c'\n",
    "    ]).mark_circle(size=10).encode(\n",
    "        alt.X(\"RH_3m_c\").title(\"RH (%)\"),\n",
    "        alt.Y(\"value:Q\").title([\"Fraction of 20hz sonic anemometer\", \"measurements flagged (Tower C, 3m)\"]),\n",
    "        alt.Color(\"on Dec 21/22:N\"),\n",
    "        alt.Column('key:N').sort(['ldiag_3m_c', 'ldiag_5m_c', 'ldiag_10m_c', 'ldiag_15m_c', 'ldiag_20m_c'])\n",
    "    ).properties(width=200, height=200)\n",
    ").configure_axis(grid=False).configure_legend(columns=2, orient='top')\n",
    "bad_sonic_data.save(\"bad_sonic_data.png\", ppi=200)\n",
    "display(bad_sonic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(src.query(\"ldiag_3m_c > 0.1\")['on Dec 21/22'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = sos_ds[['RH_3m_c', 'RH_5m_c', 'RH_10m_c', 'RH_15m_c', 'RH_20m_c', \n",
    "                'irgadiag_3m_c', 'irgadiag_5m_c', 'irgadiag_10m_c', 'irgadiag_15m_c', 'irgadiag_20m_c', \n",
    "            ]].to_dataframe().reset_index()\n",
    "src = utils.modify_df_timezone(src, 'UTC', 'US/Mountain')\n",
    "\n",
    "bad_irga_data_3m = (\n",
    "    alt.Chart(src).mark_circle(size=10).encode(\n",
    "        alt.X(\"RH_3m_c:Q\").title(\"Relative humidity, 3m (%)\"),\n",
    "        alt.Y(\"irgadiag_3m_c:Q\").title([\"Sum of EC150 diagnostic flags\", \"3m\"]).scale(type='linear'),\n",
    "    ).properties(width=200, height=200)\n",
    ")\n",
    "bad_irga_data_5m = (\n",
    "    alt.Chart(src).mark_circle(size=10).encode(\n",
    "        alt.X(\"RH_5m_c:Q\").title(\"Relative humidity, 5m (%)\"),\n",
    "        alt.Y(\"irgadiag_5m_c:Q\").title([\"Sum of EC150 diagnostic flags\", \"5m\"]).scale(type='linear'),\n",
    "    ).properties(width=200, height=200)\n",
    ")\n",
    "bad_irga_data_10m = (\n",
    "    alt.Chart(src).mark_circle(size=10).encode(\n",
    "        alt.X(\"RH_10m_c:Q\").title(\"Relative humidity, 10m (%)\"),\n",
    "        alt.Y(\"irgadiag_10m_c:Q\").title([\"Sum of EC150 diagnostic flags\", \"10m\"]).scale(type='linear'),\n",
    "    ).properties(width=200, height=200)\n",
    ")\n",
    "bad_irga_data_15m = (\n",
    "    alt.Chart(src).mark_circle(size=10).encode(\n",
    "        alt.X(\"RH_15m_c:Q\").title(\"Relative humidity, 15m (%)\"),\n",
    "        alt.Y(\"irgadiag_15m_c:Q\").title([\"Sum of EC150 diagnostic flags\", \"15m\"]).scale(type='linear'),\n",
    "    ).properties(width=200, height=200)\n",
    ")\n",
    "bad_irga_data_20m = (\n",
    "    alt.Chart(src).mark_circle(size=10).encode(\n",
    "        alt.X(\"RH_20m_c:Q\").title(\"Relative humidity, 20m (%)\"),\n",
    "        alt.Y(\"irgadiag_20m_c:Q\").title([\"Sum of EC150 diagnostic flags\", \"20m\"]).scale(type='linear'),\n",
    "    ).properties(width=200, height=200)\n",
    ")\n",
    "rule = alt.Chart().transform_calculate(y='9000').mark_rule(color='red', strokeDash=[4,2]).encode(y='y:Q')\n",
    "bad_irga_data = (\n",
    "    ((bad_irga_data_3m+rule) | (bad_irga_data_5m+rule) | (bad_irga_data_10m+rule)  )\n",
    "    & ((bad_irga_data_15m+rule)| (bad_irga_data_20m+rule))\n",
    "    ).configure_axis(grid=False)\n",
    "bad_irga_data.save(\"bad_irga_data.png\", ppi=200)\n",
    "display(bad_irga_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC3: Snowfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if FILTER_SNOWFALL:\n",
    "#     # open the snowfall dataset\n",
    "#     snowfall_mask_df = pd.read_csv(snowfall_mask_file, index_col=0)\n",
    "#     snowfall_mask_df.index.name = 'time'\n",
    "#     snowfall_mask_df.index = pd.to_datetime(snowfall_mask_df.index)\n",
    "\n",
    "#     # add it as a variable too the dataset\n",
    "#     sos_ds = sos_ds.assign({\n",
    "#         'snowfall_mask': snowfall_mask_df.to_xarray()['SAIL_gts_pluvio'].reindex_like(sos_ds).astype('bool')\n",
    "#     })\n",
    "\n",
    "#     for suffix in ec_measurement_suffixes:\n",
    "#         w_var = 'w_' + suffix\n",
    "#         h2o_flux_var = 'w_h2o__' + suffix\n",
    "#         if h2o_flux_var in sos_ds.variables and sonicdiag_var in sos_ds.variables:\n",
    "#             prefix = 'w_h2o__'\n",
    "#             var = prefix + suffix\n",
    "#             if var in sos_ds:\n",
    "#                 sos_ds[var] = sos_ds[var].where(sos_ds['snowfall_mask'].values, 0)\n",
    "#         else:\n",
    "#             print(f\"Variable {h2o_flux_var} or {sonicdiag_var} not in dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC4: Non-stationarity\n",
    "\n",
    "Following https://link.springer.com/10.1007/978-3-030-52171-4_55, http://link.springer.com/10.1007/s10546-015-0103-zz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See equation 55.16 in https://link.springer.com/10.1007/978-3-030-52171-4_55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds5min_localized = utils.modify_xarray_timezone(sos_ds5min, 'UTC', 'US/Mountain')\n",
    "sos_ds5min_localized = sos_ds5min_localized.to_dataframe().sort_index().loc[STUDY_PERIOD_START_DATE: STUDY_PERIOD_END_DATE].to_xarray()\n",
    "sos_ds5min_localized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds['SWE_p1_c'].plot(label = 'SWE_p1_c')\n",
    "sos_ds['SWE_p2_c'].plot(label = 'SWE_p2_c')\n",
    "sos_ds['SWE_p3_c'].plot(label = 'SWE_p3_c')\n",
    "sos_ds['SWE_p4_c'].plot(label = 'SWE_p4_c')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in limited_nan_counts_df.variable:\n",
    "    w_h2o__3m_c_stationarity_stat = 100*np.abs(\n",
    "        (\n",
    "            sos_ds5min_localized[var].to_dataframe().resample('30Min').mean()\n",
    "            -\n",
    "            sos_ds[var].to_dataframe()\n",
    "        ) / sos_ds[var].to_dataframe()\n",
    "    )\n",
    "    print(\n",
    "        var,\n",
    "        (w_h2o__3m_c_stationarity_stat > 30).sum().values, \n",
    "        (w_h2o__3m_c_stationarity_stat <= 30).sum().values,\n",
    "        round((((w_h2o__3m_c_stationarity_stat > 30).sum().values) / (\n",
    "            ((w_h2o__3m_c_stationarity_stat > 30).sum().values + (w_h2o__3m_c_stationarity_stat <= 30).sum().values)\n",
    "        ))[0], 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(\n",
    "    w_h2o__3m_c_stationarity_stat.loc['20221130': '20230508'].dropna().query(\"w_h2o__20m_c < 1000\")\n",
    ").mark_bar().encode(\n",
    "    alt.X('w_h2o__20m_c:Q').bin(step=100),\n",
    "    alt.Y('count():Q')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "df = pd.read_csv(StringIO(\n",
    "\"\"\"\n",
    "height tower dropped1 dropped2 percent_nonstationary\n",
    "2 ue 1399 3208 0.3\n",
    "3 ue 2709 4071 0.4\n",
    "10 ue 4330 2506 0.63\n",
    "2 d 1259 3085 0.29\n",
    "3 d 2487 4223 0.37\n",
    "10 d 4125 2630 0.61\n",
    "2 uw 1124 2236 0.33\n",
    "3 uw 2861 3983 0.42\n",
    "10 uw 4070 2689 0.6\n",
    "2 c 2759 4235 0.39\n",
    "3 c 3045 3956 0.43\n",
    "5 c 3810 3480 0.52\n",
    "10 c 4600 2568 0.64\n",
    "15 c 4870 2512 0.66\n",
    "20 c 4818 2106 0.7\n",
    "\"\"\"\n",
    "), delim_whitespace=True)\n",
    "df.percent_nonstationary = df.percent_nonstationary*100\n",
    "alt.Chart(df).mark_point().encode(\n",
    "    alt.X(\"percent_nonstationary:Q\").title(\"% Non-stationary\"),\n",
    "    alt.Y(\"height\").title(\"height (m)\"),\n",
    "    alt.Color(\"tower:N\")\n",
    ").properties(width = 150, height = 150).display(renderer='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add additional variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add snow depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open snow depth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "towerc_snowdepth_dataset = xr.open_dataset(\"~/Development/data/sublimationofsnow/lidar_snow_depth/C_l2.nc\")\n",
    "# towerc_lidar_snowdepth_da = towerc_snowdepth_dataset.resample(time='1440Min').median()['surface']\n",
    "towerc_lidar_snowdepth_da = towerc_snowdepth_dataset.resample(time='30min').median()['surface']\n",
    "towerc_lidar_snowdepth_da = towerc_lidar_snowdepth_da.interpolate_na(dim = 'time', method='linear')\n",
    "towerc_lidar_snowdepth_da = towerc_lidar_snowdepth_da.where(towerc_lidar_snowdepth_da > 0, 0)\n",
    "towerc_lidar_snowdepth_upsample_da = towerc_lidar_snowdepth_da.resample(time = '30Min').pad()\n",
    "towerc_lidar_snowdepth_da.plot()\n",
    "towerc_lidar_snowdepth_upsample_da.plot()\n",
    "display(plt.show())\n",
    "\n",
    "towerc_snowdepth_df = towerc_lidar_snowdepth_upsample_da.to_dataframe().reset_index()\n",
    "towerc_snowdepth_df = towerc_snowdepth_df.set_index('time').loc[\n",
    "    pd.to_datetime(sos_ds.time.min().values):\n",
    "    pd.to_datetime(sos_ds.time.max().values)\n",
    "]\n",
    "\n",
    "sos_ds['SnowDepth_c'] = (['time'], towerc_snowdepth_df.surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "towerd_snowdepth_dataset = xr.open_dataset(\"~/Development/data/sublimationofsnow/lidar_snow_depth/D_from_D_l6.nc\")\n",
    "# towerd_lidar_snowdepth_da = towerd_snowdepth_dataset.resample(time='1440Min').median()['surface']\n",
    "towerd_lidar_snowdepth_da = towerd_snowdepth_dataset.resample(time='30Min').median()['surface']\n",
    "towerd_lidar_snowdepth_da = towerd_lidar_snowdepth_da.interpolate_na(dim = 'time', method='linear')\n",
    "towerd_lidar_snowdepth_da = towerd_lidar_snowdepth_da.where(towerd_lidar_snowdepth_da > 0, 0)\n",
    "towerd_lidar_snowdepth_upsample_da = towerd_lidar_snowdepth_da.resample(time = '30Min').pad()\n",
    "towerd_lidar_snowdepth_da.plot()\n",
    "towerd_lidar_snowdepth_upsample_da.plot()\n",
    "display(plt.show())\n",
    "\n",
    "towerd_snowdepth_df = towerd_lidar_snowdepth_upsample_da.to_dataframe().reset_index()\n",
    "towerd_snowdepth_df = towerd_snowdepth_df.set_index('time').loc[\n",
    "    pd.to_datetime(sos_ds.time.min().values):\n",
    "    pd.to_datetime(sos_ds.time.max().values)\n",
    "]\n",
    "\n",
    "sos_ds['SnowDepth_d'] = (['time'], towerd_snowdepth_df.surface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add/calculate longwave radiation and surface temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds = variables.add_longwave_radiation(sos_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sublimpy import variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds = sos_ds.drop_duplicates(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds = variables.add_surface_temps(sos_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean $T_s$ variables before proceeding with other calculations\n",
    "\n",
    "(as of Feb 20, 2023, using NCAR's QC data release, we found a single $T_s$ outlier.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tsurf_vars = [v for v in sos_ds.data_vars if v.startswith('Tsurf')]\n",
    "Tsurf_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in Tsurf_vars:\n",
    "    print(f\"{var}\\t {round(sos_ds[var].min().item(), 1)}\\t{round(sos_ds[var].max().item(), 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    (sos_ds['Tsurf_c'].values <= -40)\n",
    "    |\n",
    "    (sos_ds['Tsurf_c'].values >= 40)\n",
    ").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    (sos_ds['Tsurf_c'].values > 0)\n",
    ").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in Tsurf_vars:\n",
    "    sos_ds[var] = sos_ds[var].where(\n",
    "        (sos_ds[var].values > -40)\n",
    "        &\n",
    "        (sos_ds[var].values < 40)\n",
    "    ).interpolate_na(\n",
    "        dim='time', \n",
    "        method='linear'\n",
    "    ).where(\n",
    "        ~ sos_ds[var].isnull()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in Tsurf_vars:\n",
    "    print(f\"{var}\\t {round(sos_ds[var].min().item(), 1)}\\t{round(sos_ds[var].max().item(), 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2,sharex=True,sharey=True)\n",
    "sos_ds['Tsurf_c'].plot(ax=axes[0][0])\n",
    "sos_ds['Tsurf_d'].plot(ax=axes[0][1])\n",
    "sos_ds['Tsurf_ue'].plot(ax=axes[1][0])\n",
    "sos_ds['Tsurf_uw'].plot(ax=axes[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add $T_v, \\theta, \\theta_v, \\textbf{tke}, R_i, L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds = variables.add_potential_virtual_temperatures(sos_ds)\n",
    "sos_ds = variables.add_surface_potential_virtual_temperatures(sos_ds)\n",
    "sos_ds = variables.add_tke(sos_ds)\n",
    "sos_ds = variables.add_gradients_and_ri(sos_ds)\n",
    "sos_ds = variables.add_shear_velocity_and_obukhov_length(sos_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add decoupling metric, from Peltola et al. (2021).\n",
    "\n",
    "We adjust the height value for snow depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoupling_metric(z, sigma_w, N):\n",
    "    \"\"\"Calculate the decoupling metric as described in Peltola et al (2021).\n",
    "\n",
    "    Peltola, O., Lapo, K., & Thomas, C. K. (2021). A PhysicsBased Universal Indicator for Vertical Decoupling and Mixing Across Canopies Architectures and Dynamic Stabilities. Geophysical Research Letters, 48(5), e2020GL091615. https://doi.org/10.1029/2020GL091615\n",
    "    \n",
    "    Args:\n",
    "        z (float): height of measurements \n",
    "        sigma_w (float): standarad deviation of w, vertical velocity\n",
    "        N (float): Brunt-Vaisala frequency\n",
    "    \"\"\"\n",
    "    # Brunt Vaisala frequency estimated using the bulk theta gradient\n",
    "    # N = np.sqrt(\n",
    "    #     g * (theta_e - theta_mean) / theta_mean\n",
    "    # )\n",
    "\n",
    "    Lb = sigma_w / N\n",
    "    omega = Lb / ( np.sqrt(2)*z )\n",
    "    return omega\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "### OLD METHOD USING GRADIENT-BASED N\n",
    "#######################################################################\n",
    "from metpy.calc import brunt_vaisala_frequency\n",
    "\n",
    "sigma_w = np.sqrt(sos_ds['w_w__3m_c']).values\n",
    "pot_temps = sos_ds[[\n",
    "    'Tpot_2m_c', \n",
    "    'Tpot_3m_c', \n",
    "    'Tpot_4m_c', \n",
    "    'Tpot_5m_c', \n",
    "    'Tpot_6m_c'\n",
    "]].to_stacked_array(\n",
    "    'z', ['time']\n",
    ").values\n",
    "\n",
    "snow_depth_values = sos_ds['SnowDepth_c']\n",
    "snow_depth_values_reshaped = np.repeat(sos_ds['SnowDepth_c'].values, 5).reshape(-1, 5)\n",
    "\n",
    "heights = np.full(pot_temps.shape,  [ 2,    3,    4,    5,    6])\n",
    "heights_adjusted = heights - snow_depth_values_reshaped\n",
    "brunt_vaisala_values = [ Ns[1] for Ns in \n",
    "    brunt_vaisala_frequency( \n",
    "        heights_adjusted * units(\"meters\"),\n",
    "        pot_temps * units(\"celsius\"), \n",
    "        vertical_dim=1\n",
    "    ).magnitude   \n",
    "]\n",
    "z = np.full(sigma_w.shape, 3) - snow_depth_values.values\n",
    "\n",
    "# decoupling_metric(z, sigma_w, N)\n",
    "print(len(z))\n",
    "print(len(sigma_w))\n",
    "print(len(brunt_vaisala_values))\n",
    "\n",
    "omegas = decoupling_metric(z, sigma_w, brunt_vaisala_values)\n",
    "sos_ds['omega_3m_c'] = (['time'],  omegas)\n",
    "print(len(omegas))\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "### NEW METHOD USING BULK N\n",
    "#######################################################################\n",
    "# sigma_w = np.sqrt(sos_ds['w_w__3m_c']).values\n",
    "\n",
    "# snow_depth_values = sos_ds['SnowDepth_c'].values\n",
    "\n",
    "# air_temp_height = 3\n",
    "# heights_adjusted = air_temp_height - snow_depth_values\n",
    "# surf_pottemp = sos_ds['Tsurfpot_c'].values\n",
    "# air_pottemp = sos_ds['Tpot_3m_c'].values\n",
    "# surfacealyer_avg_pottemp = 0.5*(air_pottemp + surf_pottemp)\n",
    "# bulk_brunt_vaisala_value = np.sqrt(\n",
    "#     metpy.constants.earth_gravity.magnitude.item()*(air_pottemp - surfacealyer_avg_pottemp) \n",
    "#     /\n",
    "#     (surfacealyer_avg_pottemp * heights_adjusted)\n",
    "# )\n",
    "# z = np.full(sigma_w.shape, 3) - snow_depth_values\n",
    "\n",
    "# # decoupling_metric(z, sigma_w, N)\n",
    "# print(len(z))\n",
    "# print(len(sigma_w))\n",
    "# print(len(bulk_brunt_vaisala_value))\n",
    "\n",
    "# omegas = decoupling_metric(z, sigma_w, bulk_brunt_vaisala_value)\n",
    "# sos_ds['omega_3m_c'] = (['time'],  omegas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(sos_ds['omega_3m_c'])\n",
    "plt.xlim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net LW and Net SW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds['Rlw_net_9m_d'] = sos_ds['Rlw_in_9m_d'] - sos_ds['Rlw_out_9m_d']\n",
    "sos_ds['Rsw_net_9m_d'] = sos_ds['Rsw_in_9m_d'] - sos_ds['Rsw_out_9m_d']\n",
    "\n",
    "sos_ds['Rlw_net_9m_uw'] = sos_ds['Rlw_in_9m_uw'] - sos_ds['Rlw_out_9m_uw']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net Radiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds['Rnet_9m_d'] = (\n",
    "    (sos_ds['Rsw_in_9m_d'] + sos_ds['Rlw_in_9m_d'])\n",
    "    -\n",
    "    (sos_ds['Rsw_out_9m_d'] + sos_ds['Rlw_out_9m_d'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in [\n",
    "    'Tsurfmixingratio_c',\n",
    "    'mixingratio_1m_c',\n",
    "    'mixingratio_2m_c',\n",
    "    'mixingratio_3m_c',\n",
    "    'mixingratio_4m_c',\n",
    "    'mixingratio_5m_c',\n",
    "    'mixingratio_6m_c',\n",
    "    'mixingratio_7m_c',\n",
    "    'mixingratio_8m_c',\n",
    "    'mixingratio_9m_c',\n",
    "    'mixingratio_10m_c',\n",
    "    'mixingratio_11m_c',\n",
    "    'mixingratio_12m_c',\n",
    "    'mixingratio_13m_c',\n",
    "    'mixingratio_14m_c',\n",
    "    'mixingratio_15m_c',\n",
    "    'mixingratio_16m_c',\n",
    "    'mixingratio_17m_c',\n",
    "    'mixingratio_18m_c',\n",
    "    'mixingratio_19m_c',\n",
    "    'mixingratio_20m_c',\n",
    "]:\n",
    "    new_var_name = var.replace('mixingratio', 'specifichumidity')\n",
    "    result = specific_humidity_from_mixing_ratio(\n",
    "        sos_ds[var]*units('g/g')\n",
    "    )\n",
    "    sos_ds[new_var_name] = (['time'], result.values)\n",
    "    sos_ds[new_var_name] = sos_ds[new_var_name].assign_attrs(units=str(result.pint.units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply sensible and latent heat flux corrections (lots of citations, see here: https://www.eol.ucar.edu/content/corrections-sensible-and-latent-heat-flux-measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_d = 1293*units('g/m^3') # density of dry air\n",
    "rho_a = 1287*units('g/m^3') #density of moist air\n",
    "mu = 1/0.622\n",
    "rho_w = metpy.constants.density_water\n",
    "\n",
    "def corrected_sensible_heat_flux(\n",
    "    w_tc_,  #sonic_temperature_flux\n",
    "    w_h2o_, #h2o_flux\n",
    "    Q,      #specific_humidity\n",
    "    T,      #absolute_temperature\n",
    "    A,      #A\n",
    "    Mr,     #mean_mixing_ratio\n",
    "):\n",
    "    Ctc = 0.51*(1 + Q*(mu-1))\n",
    "    part1 = w_tc_ - Ctc*(T/rho_a)*A*w_h2o_\n",
    "    part2 = 1 + Ctc*Q\n",
    "    return part1 / part2\n",
    "\n",
    "def wpl_corrected_latent_heat_flux(\n",
    "    w_tc_,  #sonic_temperature_flux\n",
    "    w_h2o_, #h2o_flux\n",
    "    Q,      #specific_humidity\n",
    "    T,      #absolute_temperature\n",
    "    A,      #A\n",
    "    Mr,     #mean_mixing_ratio,\n",
    "    rho_v,\n",
    "    rho_d     #dry air density\n",
    "):\n",
    "    return (\n",
    "        1 + mu*Mr\n",
    "    )*(\n",
    "        w_h2o_ + (rho_v/T)*w_tc_\n",
    "    ) / rho_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tc__3m_c_corrected = corrected_sensible_heat_flux(\n",
    "    sos_ds['w_tc__3m_c']*units(\"(m/s)(degK)\"),\n",
    "    sos_ds['w_h2o__3m_c']*units(\"(m/s)(g/m^3)\"),\n",
    "    (sos_ds['specifichumidity_3m_c']*units(\"g/g\")).pint.to('g/kg'),\n",
    "    (sos_ds['T_3m_c'] * units.degC).pint.to('degK'),\n",
    "    1, #A\n",
    "    (sos_ds['mixingratio_3m_c']*units(\"g/g\")).pint.to('g/kg'),\n",
    ")\n",
    "\n",
    "# WHY DO I HAVE TO MULTIPLE DENSITY_WATER HERE???\n",
    "w_h2o__3m_c_corrected = wpl_corrected_latent_heat_flux(\n",
    "    w_tc__3m_c_corrected.values*units(\"(m/s)(degK)\"),\n",
    "    sos_ds['w_h2o__3m_c']*units(\"(m/s)(g/m^3)\"),\n",
    "    (sos_ds['specifichumidity_3m_c']*units(\"g/g\")).pint.to('g/kg'),\n",
    "    (sos_ds['T_3m_c'] * units.degC).pint.to('degK'),\n",
    "    1, #A\n",
    "    (sos_ds['mixingratio_3m_c']*units(\"g/g\")).pint.to('g/kg'),\n",
    "    (sos_ds['h2o_3m_c']*units(\"g/m^3\")).pint.to('kg/m^3'),\n",
    "    sos_ds['dryairdensity_3m_c']* units('kilogram / meter ** 3')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(2,1, sharex=True, figsize=(6,6))\n",
    "start = '20221220'\n",
    "end =   '20221225'\n",
    "sos_ds['w_tc__3m_c'].loc[start: end].plot(label='raw', ax = ax1)\n",
    "w_tc__3m_c_corrected.loc[start: end].plot(label='corrected', ax = ax1)\n",
    "ax1.legend()\n",
    "\n",
    "sos_ds['w_h2o__3m_c'].loc[start: end].plot(label='raw', ax = ax2)\n",
    "w_h2o__3m_c_corrected.loc[start: end].plot(label='corrected', ax = ax2)\n",
    "ax2.legend()\n",
    "ax2.set_ylim(-0.005,0.04)\n",
    "ax2.set_ylabel(\"w'q' (g/m/s)\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sos_ds['w_h2o__3m_c'].cumsum()*1.8).plot(label='raw', figsize=(6,3))\n",
    "(w_h2o__3m_c_corrected.cumsum()*1.8).plot(label='corrected')\n",
    "plt.title([\n",
    "    round((sos_ds['w_h2o__3m_c'].cumsum()*1.8)[-1].item(), 2),\n",
    "    round((w_h2o__3m_c_corrected.cumsum()*1.8)[-1].item().magnitude.item(), 2),\n",
    "])\n",
    "plt.legend()\n",
    "plt.ylabel(\"Cumulative w'q' (g/m)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for twr in ['c', 'd', 'ue', 'uw']:\n",
    "    if twr == 'c':\n",
    "        heights = [1,2,3,5,10,15,20]\n",
    "    else:\n",
    "        heights = [1,3,10]\n",
    "    for h in heights:\n",
    "        w_tc_raw_var = f'w_tc__{h}m_{twr}'\n",
    "        w_tc_corrected = corrected_sensible_heat_flux(\n",
    "            sos_ds[w_tc_raw_var]*units(\"(m/s)(degK)\"),\n",
    "            sos_ds[f'w_h2o__{h}m_{twr}']*units(\"(m/s)(g/m^3)\"),\n",
    "            (sos_ds[f'specifichumidity_{h}m_c']*units(\"g/g\")).pint.to('g/kg'),\n",
    "            (sos_ds[f'T_{h}m_c'] * units.degC).pint.to('degK'),\n",
    "            1, #A\n",
    "            (sos_ds[f'mixingratio_{h}m_c']*units(\"g/g\")).pint.to('g/kg'),\n",
    "        )\n",
    "\n",
    "        w_h2o_raw_var = f'w_h2o__{h}m_{twr}'\n",
    "        w_h2o_corrected = wpl_corrected_latent_heat_flux(\n",
    "            w_tc__3m_c_corrected.values*units(\"(m/s)(degK)\"),\n",
    "            sos_ds[w_h2o_raw_var]*units(\"(m/s)(g/m^3)\"),\n",
    "            (sos_ds[f'specifichumidity_{h}m_c']*units(\"g/g\")).pint.to('g/kg'),\n",
    "            (sos_ds[f'T_{h}m_c'] * units.degC).pint.to('degK'),\n",
    "            1, #A\n",
    "            (sos_ds[f'mixingratio_{h}m_c']*units(\"g/g\")).pint.to('g/kg'),\n",
    "            (sos_ds[f'h2o_{h}m_{twr}']*units(\"g/m^3\")).pint.to('kg/m^3'),\n",
    "            sos_ds[f'dryairdensity_{h}m_c']* units('kilogram / meter ** 3')\n",
    "        )\n",
    "        sos_ds[\n",
    "            w_h2o_raw_var] = ('time', w_h2o_corrected.values)\n",
    "        \n",
    "        sos_ds[\n",
    "            w_tc_raw_var] = ('time', w_tc_corrected.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add back attributes from the original sos datasets to the new, augmented sos dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_ds = xr.open_dataset(\"sos_ds_temp_storage_30min_straightup.cdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in sos_ds:\n",
    "    if var in original_ds:\n",
    "        sos_ds[var].attrs = original_ds[var].attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save a netcdf version of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds.to_netcdf(\"sos_full_dataset_30min.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "xxx = xr.open_dataset(\"sos_full_dataset_30min.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('kilogram / meter ** 3', 'dimensionless')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxx['dryairdensity_10m_c'].attrs['units'], xxx['mixingratio_10m_c'].attrs['units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_for_3m_ec_dataset = [var for var in sos_ds.data_vars if '_3m_' in var]\n",
    "sos_ds[vars_for_3m_ec_dataset].to_netcdf(\"sos_3m_dataset_30min.nc\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tidy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sublimpy import tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_df = tidy.get_tidy_dataset(sos_ds, list(sos_ds.data_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_df.time.min(), tidy_df.time.max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which variables did not get a \"measurement\" name assigned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_with_no_measurement = tidy_df[tidy_df.measurement.apply(lambda x: x is None)].variable.unique()\n",
    "variables_with_no_measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(tidy_df.variable.unique()).difference(set(list(sos_ds.data_vars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import altair as alt\n",
    "# seconds_in_timestep = 60*30\n",
    "# density_water = 1000\n",
    "# from sublimpy import utils\n",
    "# tidy_df_localized = utils.modify_df_timezone(tidy_df, 'UTC', 'US/Mountain')\n",
    "# tidy_df_localized = tidy_df_localized[\n",
    "#     (tidy_df_localized.time > STUDY_PERIOD_START_DATE)\n",
    "#     &\n",
    "#     (tidy_df_localized.time < '20230509')\n",
    "# ]\n",
    "\n",
    "# measured_results_mm = tidy_df_localized[tidy_df_localized.variable.isin([\n",
    "#     'w_h2o__2m_c',\n",
    "#     'w_h2o__3m_c',\n",
    "#     'w_h2o__5m_c',\n",
    "#     'w_h2o__10m_c',\n",
    "#     'w_h2o__15m_c',\n",
    "#     'w_h2o__20m_c',\n",
    "\n",
    "#     'w_h2o__3m_ue',\n",
    "#     'w_h2o__10m_ue',\n",
    "\n",
    "#     'w_h2o__3m_uw',\n",
    "#     'w_h2o__10m_uw',\n",
    "\n",
    "#     'w_h2o__3m_d',\n",
    "#     'w_h2o__10m_d',\n",
    "# ])][\n",
    "#     ['time', 'value', 'variable']\n",
    "# ]\n",
    "# measured_results_mm.columns = ['time', 'measured', 'variable']\n",
    "# measured_results_mm = measured_results_mm.pivot(\n",
    "#     index = 'time',\n",
    "#     columns = 'variable',\n",
    "#     values = 'measured'\n",
    "# )\n",
    "# # convert to mm\n",
    "# measured_results_mm = measured_results_mm*seconds_in_timestep/density_water\n",
    "# measured_results_cumsum = measured_results_mm.cumsum().reset_index()\n",
    "# measured_results_cumsum = measured_results_cumsum.melt(id_vars='time')\n",
    "# measured_results_cumsum['height'] = measured_results_cumsum['variable'].apply(lambda s: int(s.split('__')[1].split('m')[0]))\n",
    "# measured_results_cumsum['tower'] = measured_results_cumsum['variable'].apply(lambda s: s.split('__')[1].split('_')[-1])\n",
    "# measured_results_cumsum['tower-height'] = measured_results_cumsum['tower'] + '-' + measured_results_cumsum['height'].astype('str')\n",
    "# measured_results_cumsum\n",
    "\n",
    "\n",
    "# src = measured_results_cumsum.groupby(['height', 'tower'])[['value']].max().reset_index()\n",
    "# sublimation_totals_per_height_tower_chart = alt.Chart(src).mark_point(size=100).encode(\n",
    "#     alt.Y(\"height:O\").sort('-y').title(\"height (m)\"),\n",
    "#     alt.X(\"value:Q\").scale(zero=False).title([\"Seasonal sublimation (mm SWE)\"]),\n",
    "#     # alt.Color(\"height:O\").scale(scheme='turbo').legend(columns=2),\n",
    "#     alt.Shape(\"tower:N\")\n",
    "# ).properties(width = 150, height = 150)\n",
    "# sublimation_totals_per_height_tower_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply mean diurnal cycle gap filling for latent heat fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"variable,n_nans_before,total_sub_before,n_nans_after,total_sub_after\")\n",
    "for lhflux_variable in [\n",
    "    'w_h2o__2m_c',\n",
    "    'w_h2o__3m_c',\n",
    "    'w_h2o__5m_c',\n",
    "    'w_h2o__10m_c',\n",
    "    'w_h2o__15m_c',\n",
    "    'w_h2o__20m_c',\n",
    "    'w_h2o__3m_ue',\n",
    "    'w_h2o__10m_ue',\n",
    "    'w_h2o__3m_uw',\n",
    "    'w_h2o__10m_uw',\n",
    "    'w_h2o__3m_d',\n",
    "    'w_h2o__10m_d',\n",
    "]:\n",
    "    subset = tidy_df[tidy_df.variable == lhflux_variable].set_index('time')\n",
    "    n_nans_before = subset['value'].isna().sum()\n",
    "    total_sub_before = (subset['value']*30*60/1000).sum()    \n",
    "    for i,row in subset.iterrows():\n",
    "        if np.isnan(row['value']):\n",
    "            start_window = i - dt.timedelta(days=3, hours=12)\n",
    "            end_window = i + dt.timedelta(days=3, hours=12)\n",
    "            src = subset.loc[start_window: end_window].reset_index()\n",
    "            means = pd.DataFrame(\n",
    "                src.groupby([src.time.dt.hour, src.time.dt.minute])['value'].mean()\n",
    "            )\n",
    "            subset.loc[i, 'value'] = means.loc[i.hour, i.minute].value\n",
    "    new_values = subset['value'].values\n",
    "    measurement = subset['measurement'].values[0]\n",
    "    height = subset['height'].values[0]\n",
    "    tower = subset['tower'].values[0]\n",
    "    # Add new values for variable\n",
    "    tidy_df = tidy.tidy_df_add_variable(\n",
    "        tidy_df,\n",
    "        new_values,\n",
    "        lhflux_variable + '_gapfill',\n",
    "        measurement,\n",
    "        height,\n",
    "        tower\n",
    "    )\n",
    "    tidy_df.query(\"variable == 'w_h2o__3m_c'\").set_index('time')['value'].isna().sum()\n",
    "    n_nans_after = subset['value'].isna().sum()\n",
    "    total_sub_after = (subset['value']*30*60/1000).sum()\n",
    "    print(lhflux_variable, n_nans_before, round(total_sub_before,1), n_nans_after, round(total_sub_after,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1.8*tidy_df.query(\"variable == 'w_h2o__3m_c_raw'\").set_index('time').value).cumsum().plot(label='raw')\n",
    "(1.8*tidy_df.query(\"variable == 'w_h2o__3m_c'\").set_index('time').value).cumsum().plot(label='cleaned')\n",
    "(1.8*tidy_df.query(\"variable == 'w_h2o__3m_c_gapfill'\").set_index('time').value).cumsum().plot(label='gap filled')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1.8*tidy_df.query(\"variable == 'w_h2o__20m_c_raw'\").set_index('time').value).cumsum().plot(label='raw')\n",
    "(1.8*tidy_df.query(\"variable == 'w_h2o__20m_c'\").set_index('time').value).cumsum().plot(label='cleaned')\n",
    "(1.8*tidy_df.query(\"variable == 'w_h2o__20m_c_gapfill'\").set_index('time').value).cumsum().plot(label='gap filled')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import altair as alt\n",
    "# seconds_in_timestep = 60*30\n",
    "# density_water = 1000\n",
    "# from sublimpy import utils\n",
    "# tidy_df_localized = utils.modify_df_timezone(tidy_df, 'UTC', 'US/Mountain')\n",
    "# tidy_df_localized = tidy_df_localized[\n",
    "#     (tidy_df_localized.time > STUDY_PERIOD_START_DATE)\n",
    "#     &\n",
    "#     (tidy_df_localized.time < '20230509')\n",
    "# ]\n",
    "\n",
    "# measured_results_mm = tidy_df_localized[tidy_df_localized.variable.isin([\n",
    "#     'w_h2o__2m_c',\n",
    "#     'w_h2o__3m_c',\n",
    "#     'w_h2o__5m_c',\n",
    "#     'w_h2o__10m_c',\n",
    "#     'w_h2o__15m_c',\n",
    "#     'w_h2o__20m_c',\n",
    "\n",
    "#     'w_h2o__3m_ue',\n",
    "#     'w_h2o__10m_ue',\n",
    "\n",
    "#     'w_h2o__3m_uw',\n",
    "#     'w_h2o__10m_uw',\n",
    "\n",
    "#     'w_h2o__3m_d',\n",
    "#     'w_h2o__10m_d',\n",
    "# ])][\n",
    "#     ['time', 'value', 'variable']\n",
    "# ]\n",
    "# measured_results_mm.columns = ['time', 'measured', 'variable']\n",
    "# measured_results_mm = measured_results_mm.pivot(\n",
    "#     index = 'time',\n",
    "#     columns = 'variable',\n",
    "#     values = 'measured'\n",
    "# )\n",
    "# # convert to mm\n",
    "# measured_results_mm = measured_results_mm*seconds_in_timestep/density_water\n",
    "# measured_results_cumsum = measured_results_mm.cumsum().reset_index()\n",
    "# measured_results_cumsum = measured_results_cumsum.melt(id_vars='time')\n",
    "# measured_results_cumsum['height'] = measured_results_cumsum['variable'].apply(lambda s: int(s.split('__')[1].split('m')[0]))\n",
    "# measured_results_cumsum['tower'] = measured_results_cumsum['variable'].apply(lambda s: s.split('__')[1].split('_')[-1])\n",
    "# measured_results_cumsum['tower-height'] = measured_results_cumsum['tower'] + '-' + measured_results_cumsum['height'].astype('str')\n",
    "# measured_results_cumsum\n",
    "\n",
    "\n",
    "# src = measured_results_cumsum.groupby(['height', 'tower'])[['value']].max().reset_index()\n",
    "# sublimation_totals_per_height_tower_chart = alt.Chart(src).mark_point(size=100).encode(\n",
    "#     alt.Y(\"height:O\").sort('-y').title(\"height (m)\"),\n",
    "#     alt.X(\"value:Q\").scale(zero=False).title([\"Seasonal sublimation (mm SWE)\"]),\n",
    "#     # alt.Color(\"height:O\").scale(scheme='turbo').legend(columns=2),\n",
    "#     alt.Shape(\"tower:N\")\n",
    "# ).properties(width = 150, height = 150)\n",
    "# sublimation_totals_per_height_tower_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lhflux_variable in [\n",
    "#     'w_h2o__2m_c_gapfill',\n",
    "#     'w_h2o__3m_c_gapfill',\n",
    "#     'w_h2o__5m_c_gapfill',\n",
    "#     'w_h2o__10m_c_gapfill',\n",
    "#     'w_h2o__15m_c_gapfill',\n",
    "#     'w_h2o__20m_c_gapfill',\n",
    "#     'w_h2o__3m_ue_gapfill',\n",
    "#     'w_h2o__10m_ue_gapfill',\n",
    "#     'w_h2o__3m_uw_gapfill',\n",
    "#     'w_h2o__10m_uw_gapfill',\n",
    "#     'w_h2o__3m_d_gapfill',\n",
    "#     'w_h2o__10m_d_gapfill',\n",
    "# ]:\n",
    "#     subset = tidy_df[tidy_df.variable == lhflux_variable].set_index('time')\n",
    "#     subset = utils.modify_df_timezone(subset.reset_index(), 'UTC', 'US/Mountain').set_index('time')\n",
    "#     subset = subset.sort_index().loc[STUDY_PERIOD_START_DATE: STUDY_PERIOD_END_DATE]\n",
    "#     subset.value = (subset.value * 1.8).cumsum()\n",
    "#     subset = subset.reset_index()\n",
    "#     cumsum_chart = alt.Chart(subset).mark_line().encode(\n",
    "#         alt.X('time:T'),\n",
    "#         alt.Y('value:Q').title(subset.variable.iloc[0])\n",
    "#     ).properties(width=800).configure_axis(titleFontSize=16)\n",
    "#     display(cumsum_chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src = tidy_df[tidy_df.variable.isin([    \n",
    "#     'w_h2o__2m_c_raw',\n",
    "#     'w_h2o__3m_c_raw',\n",
    "#     'w_h2o__5m_c_raw',\n",
    "#     'w_h2o__10m_c_raw',\n",
    "#     'w_h2o__15m_c_raw',\n",
    "#     'w_h2o__20m_c_raw',\n",
    "#     'w_h2o__3m_ue_raw',\n",
    "#     'w_h2o__10m_ue_raw',\n",
    "#     'w_h2o__3m_uw_raw',\n",
    "#     'w_h2o__10m_uw_raw',\n",
    "#     'w_h2o__3m_d_raw',\n",
    "#     'w_h2o__10m_d_raw'])]\n",
    "# src = src[(src.time >= '20221221 1200') & (src.time < '20221223')]\n",
    "# alt.Chart(src).mark_line().encode(\n",
    "#     alt.X('time:T'),\n",
    "#     alt.Y('value:Q'),\n",
    "#     alt.Color('height:O'),\n",
    "#     alt.StrokeDash('tower:N')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src = tidy_df[tidy_df.variable.isin([    \n",
    "#     'w_h2o__2m_c_raw',\n",
    "#     'w_h2o__3m_c_raw',\n",
    "#     'w_h2o__5m_c_raw',\n",
    "#     'w_h2o__10m_c_raw',\n",
    "#     'w_h2o__15m_c_raw',\n",
    "#     'w_h2o__20m_c_raw',\n",
    "#     'w_h2o__3m_ue_raw',\n",
    "#     'w_h2o__10m_ue_raw',\n",
    "#     'w_h2o__3m_uw_raw',\n",
    "#     'w_h2o__10m_uw_raw',\n",
    "#     'w_h2o__3m_d_raw',\n",
    "#     'w_h2o__10m_d_raw'])]\n",
    "# src = src[(src.time >= '20230505') & (src.time < '20230506')]\n",
    "# alt.Chart(src).mark_line().encode(\n",
    "#     alt.X('time:T'),\n",
    "#     alt.Y('value:Q'),\n",
    "#     alt.Color('height:O').scale(scheme='turbo'),\n",
    "#     alt.StrokeDash('tower:N')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src = tidy_df.query(\"measurement == 'w'\").query(\"height > 1\").query(\n",
    "#     \"variable != 'w_2m_uw'\"\n",
    "# ).query(\n",
    "#     \"variable != 'w_2_5m_uw'\"\n",
    "# ).query(\n",
    "#     \"variable != 'w_2m_d'\"\n",
    "# ).query(\n",
    "#     \"variable != 'w_2m_ue'\"\n",
    "# )\n",
    "# src = src[(src.time >= '20230505') & (src.time < '20230506')]\n",
    "# alt.Chart(src).transform_window(\n",
    "#     rolling_median = 'mean(value)',\n",
    "#     frame = [-2, 2],\n",
    "#     groupby=['height', 'tower']\n",
    "# ).mark_line().encode(\n",
    "#     alt.X('time:T'),\n",
    "#     alt.Y('rolling_median:Q'),\n",
    "#     alt.Color('height:O').scale(scheme='turbo'),\n",
    "#     alt.StrokeDash('tower:N')\n",
    "# ).properties(height = 150)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lah | grep parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create file name based on processing choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY_PERIOD_START_DATE, STUDY_PERIOD_END_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = None\n",
    "if PLANAR_FIT:    \n",
    "    if 'oneplane' in planar_fitted_dir:\n",
    "        if FILTER_SNOWFALL:\n",
    "            output_filename = f'tidy_df_{STUDY_PERIOD_START_DATE}_{STUDY_PERIOD_END_DATE}_planar_fit_STRAIGHTUP_{filtering_str}_flags{PERCENTAGE_DIAG}_snowfallfiltered{snowfall_mask_str}.parquet'\n",
    "        else:\n",
    "            output_filename = f'tidy_df_{STUDY_PERIOD_START_DATE}_{STUDY_PERIOD_END_DATE}_planar_fit_STRAIGHTUP_{filtering_str}_flags{PERCENTAGE_DIAG}.parquet'\n",
    "                \n",
    "    else:\n",
    "        if FILTER_SNOWFALL:\n",
    "            output_filename = f'tidy_df_{STUDY_PERIOD_START_DATE}_{STUDY_PERIOD_END_DATE}_planar_fit_multiplane_STRAIGHTUP_{filtering_str}_flags{PERCENTAGE_DIAG}_snowfallfiltered{snowfall_mask_str}.parquet'\n",
    "        else:\n",
    "            output_filename = f'tidy_df_{STUDY_PERIOD_START_DATE}_{STUDY_PERIOD_END_DATE}_planar_fit_multiplane_STRAIGHTUP_{filtering_str}_flags{PERCENTAGE_DIAG}.parquet'\n",
    "else:\n",
    "    if FILTER_SNOWFALL:\n",
    "        output_filename = f'tidy_df_{STUDY_PERIOD_START_DATE}_{STUDY_PERIOD_END_DATE}_noplanar_fit_STRAIGHTUP_{filtering_str}_flags{PERCENTAGE_DIAG}_snowfallfiltered{snowfall_mask_str}.parquet',    \n",
    "    else:\n",
    "        output_filename = f'tidy_df_{STUDY_PERIOD_START_DATE}_{STUDY_PERIOD_END_DATE}_noplanar_fit_STRAIGHTUP_{filtering_str}_flags{PERCENTAGE_DIAG}.parquet'\n",
    "output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = output_filename.replace('.parquet', '_pf10.parquet')\n",
    "output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls | grep parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_df.to_parquet(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lah | grep parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(\n",
    "    \"tidy_df_20221101_20230619_planar_fit_multiplane_STRAIGHTUP_nodespiking_flags36000.parquet\"\n",
    ").query(\"variable == 'w_h2o__20m_c'\").set_index('time')['value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(\n",
    "    \"tidy_df_20221101_20230619_planar_fit_multiplane_STRAIGHTUP_nodespiking_flags36000_snowfallfiltered0mm.parquet\"\n",
    ").query(\"variable == 'w_h2o__20m_c'\").set_index('time')['value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sos_ds['w_h2o__3m_c'].loc['20221221':'20221223'].plot(label='w_h2o__3m_c')\n",
    "# sos_ds['w_h2o__5m_c'].loc['20221221':'20221223'].plot(label='w_h2o__5m_c')\n",
    "# sos_ds['w_h2o__10m_c'].loc['20221221':'20221223'].plot(label='w_h2o__10m_c')\n",
    "# sos_ds['w_h2o__15m_c'].loc['20221221':'20221223'].plot(label='w_h2o__15m_c')\n",
    "# sos_ds['w_h2o__20m_c'].loc['20221221':'20221223'].plot(label='w_h2o__20m_c')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls -lah | grep parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_parquet('tidy_df_20221101_20230619_planar_fit_multiplane_STRAIGHTUP.parquet')\n",
    "# df2 = pd.read_parquet('tidy_df_20221101_20230619_planar_fit_multiplane_STRAIGHTUP_dispiked.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src1 = df1[\n",
    "#     ~df1.variable.str.contains('predicted')\n",
    "# ].query(\"measurement == 'w_h2o_'\").query(\"tower == 'c'\").query(\"height > 1\")\n",
    "\n",
    "# src2 = df2[\n",
    "#     ~df2.variable.str.contains('predicted')\n",
    "# ].query(\"measurement == 'w_h2o_'\").query(\"tower == 'c'\").query(\"height > 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt.Chart(\n",
    "#     src1[src1.time < '20221222'][src1.time > '20221220']\n",
    "# ).mark_line().encode(\n",
    "#     alt.X('time:T'),\n",
    "#     alt.Y('value:Q'),\n",
    "#     alt.Color('height:O')\n",
    "# ) | alt.Chart(\n",
    "#     src2[src2.time < '20221222'][src2.time > '20221220']\n",
    "# ).mark_line().encode(\n",
    "#     alt.X('time:T'),\n",
    "#     alt.Y('value:Q'),\n",
    "#     alt.Color('height:O')\n",
    "# ).interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST DESPIKING DIFFERENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# COLUMNS = [\n",
    "#     'w_h2o__2m_c_fit', 'w_h2o__3m_c_fit', 'w_h2o__5m_c_fit', 'w_h2o__10m_c_fit', 'w_h2o__15m_c_fit', 'w_h2o__20m_c_fit',\n",
    "#     'w_h2o__3m_d_fit', 'w_h2o__10m_d_fit',\n",
    "#     'w_h2o__3m_ue_fit', 'w_h2o__10m_ue_fit',\n",
    "#     'w_h2o__3m_uw_fit', 'w_h2o__10m_uw_fit',\n",
    "# ]\n",
    "# srcq3 = pd.read_parquet(glob.glob(\"/Users/elischwat/Development/data/sublimationofsnow/planar_fit_processed_30min_despiked_q3.5/**.parquet\"), columns=COLUMNS).sort_index()\n",
    "# srcq7 = pd.read_parquet(glob.glob(\"/Users/elischwat/Development/data/sublimationofsnow/planar_fit_processed_30min_despiked_q7/**.parquet\"), columns=COLUMNS).sort_index()\n",
    "# srcraw = pd.read_parquet(glob.glob(\"/Users/elischwat/Development/data/sublimationofsnow/planar_fit_processed_30min/**.parquet\"), columns=COLUMNS).sort_index()\n",
    "\n",
    "# def organize_processed_lhflux_datasets(planar_fitted_data_df): \n",
    "#     planar_fitted_data_df = planar_fitted_data_df[[c for c in planar_fitted_data_df.columns if c.endswith('_fit')]]\n",
    "#     planar_fitted_data_df.columns = [c.replace('_fit', '') for c in planar_fitted_data_df.columns]\n",
    "#     planar_fitted_data_df.index = planar_fitted_data_df.index - dt.timedelta(minutes=15)\n",
    "#     return planar_fitted_data_df\n",
    "\n",
    "# srcq3 = organize_processed_lhflux_datasets(srcq3).assign(processing = 'q3')\n",
    "# srcq7 = organize_processed_lhflux_datasets(srcq7).assign(processing = 'q7')\n",
    "# srcraw = organize_processed_lhflux_datasets(srcraw).assign(processing = 'raw')\n",
    "# src = pd.concat([srcq3, srcq7, srcraw])\n",
    "\n",
    "# # src = utils.modify_df_timezone(src.reset_index('time'), 'UTC', 'US/Mountain').set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(srcraw.index.max(), srcraw.index.min())\n",
    "# print(srcq7.index.max(), srcq7.index.min())\n",
    "# print(srcq3.index.max(), srcq3.index.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(round(srcraw['w_h2o__10m_d'].mean(), 5), round(srcraw['w_h2o__10m_d'].min(), 5), round(srcraw['w_h2o__10m_d'].max(), 5))\n",
    "# print(round(srcq7['w_h2o__10m_d'].mean(), 5), round(srcq7['w_h2o__10m_d'].min(), 5), round(srcq7['w_h2o__10m_d'].max(), 5))\n",
    "# print(round(srcq3['w_h2o__10m_d'].mean(), 5), round(srcq3['w_h2o__10m_d'].min(), 5), round(srcq3['w_h2o__10m_d'].max(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local = src.copy()\n",
    "# vars = [C.replace('_fit', '') for C in COLUMNS]\n",
    "# local = local.sort_index().loc[STUDY_PERIOD_START_DATE: STUDY_PERIOD_END_DATE]\n",
    "# local[vars] = local.groupby('processing')[vars].cumsum()\n",
    "# alt.Chart(\n",
    "#     local.reset_index()\n",
    "# ).transform_fold(\n",
    "#     vars\n",
    "# ).mark_line().encode(\n",
    "#     alt.X('time:T'),\n",
    "#     alt.Y('value:Q'),\n",
    "#     alt.Row('key:N'),\n",
    "#     alt.Color('processing').sort(['raw', 'q7', 'q3'])\n",
    "# ).properties(width=600, height = 200).resolve_scale(x='independent').interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local = src.copy()\n",
    "# vars = [C.replace('_fit', '') for C in COLUMNS]\n",
    "# local = local.loc['20221221': '20221222']\n",
    "# alt.Chart(\n",
    "#     local.reset_index()\n",
    "# ).transform_fold(\n",
    "#     vars\n",
    "# ).mark_line().encode(\n",
    "#     alt.X('time:T'),\n",
    "#     alt.Y('value:Q'),\n",
    "#     alt.Row('key:N'),\n",
    "#     alt.Color('processing').sort(['raw', 'q7', 'q3'])\n",
    "# ).properties(width=600, height = 200).interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local = src.copy()\n",
    "# vars = [C.replace('_fit', '') for C in COLUMNS]\n",
    "# local = local.loc['20230110': '20230112']\n",
    "# alt.Chart(\n",
    "#     local.reset_index()\n",
    "# ).transform_fold(\n",
    "#     vars\n",
    "# ).mark_line().encode(\n",
    "#     alt.X('time:T'),\n",
    "#     alt.Y('value:Q'),\n",
    "#     alt.Row('key:N'),\n",
    "#     alt.Color('processing').sort(['raw', 'q7', 'q3'])\n",
    "# ).properties(width=600, height = 200).interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# srcq3 = srcq3.sort_index().loc[:srcq3.index.max()]\n",
    "# srcq7 = srcq7.sort_index().loc[:srcq3.index.max()]\n",
    "# srcraw = srcraw.sort_index().loc[:srcq3.index.max()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seconds_in_timestep = 60*30\n",
    "# density_water = 1000\n",
    "# # convert to mm\n",
    "# def convert_to_mm_cumsum(src):\n",
    "#     measured_results_mm = src.drop(columns='processing')*seconds_in_timestep/density_water\n",
    "#     measured_results_cumsum = measured_results_mm.cumsum().reset_index()\n",
    "#     measured_results_cumsum = measured_results_cumsum.melt(id_vars='time')\n",
    "#     measured_results_cumsum['height'] = measured_results_cumsum['variable'].apply(lambda s: int(s.split('__')[1].split('m')[0]))\n",
    "#     measured_results_cumsum['tower'] = measured_results_cumsum['variable'].apply(lambda s: s.split('__')[1].split('_')[-1])\n",
    "#     measured_results_cumsum['tower-height'] = measured_results_cumsum['tower'] + '-' + measured_results_cumsum['height'].astype('str')\n",
    "#     return measured_results_cumsum\n",
    "\n",
    "# measured_results_cumsum_q3 = convert_to_mm_cumsum(srcq3.loc[STUDY_PERIOD_START_DATE: STUDY_PERIOD_END_DATE]).groupby(['height', 'tower'])[['value']].max().reset_index()\n",
    "# measured_results_cumsum_q7 = convert_to_mm_cumsum(srcq7.loc[STUDY_PERIOD_START_DATE: STUDY_PERIOD_END_DATE]).groupby(['height', 'tower'])[['value']].max().reset_index()\n",
    "# measured_results_cumsum_raw = convert_to_mm_cumsum(srcraw.loc[STUDY_PERIOD_START_DATE: STUDY_PERIOD_END_DATE]).groupby(['height', 'tower'])[['value']].max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def seasonal_total_chart(src):\n",
    "#     return alt.Chart(src).mark_point(size=100).encode(\n",
    "#         alt.Y(\"height:O\").sort('-y').title(\"height (m)\"),\n",
    "#         alt.X(\"value:Q\").scale(zero=False).title([\"Seasonal sublimation (mm SWE)\"]),\n",
    "#         # alt.Color(\"height:O\").scale(scheme='turbo').legend(columns=2),\n",
    "#         alt.Shape(\"tower:N\")\n",
    "#     ).properties(width = 200, height = 100)\n",
    "\n",
    "# (\n",
    "# seasonal_total_chart(measured_results_cumsum_q3).properties(title='Despiked (q = 3, strict)') &\\\n",
    "# seasonal_total_chart(measured_results_cumsum_q7).properties(title='Despiked (q = 7, standard)') &\\\n",
    "# seasonal_total_chart(measured_results_cumsum_raw).properties(title='Raw, (no despiking)')\n",
    "# ).resolve_scale(y='shared', x='shared')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
