{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Take the sos netcdfs, add a bunch of useful variables, save as a tidy dataset.\n",
    "\n",
    "* Concatenate the SoS datasets\n",
    "* Remove flagged irgason and sonic anemometer measurements\n",
    "* Fill EC data gaps up to 1 hour using linear interpolation\n",
    "* Resample the dataset to 30min, testing averaging lengths of 10,30,60,120 along the way\n",
    "* Calculate stationarity of latent heat flux measurements\n",
    "* Add a bunch of new variables\n",
    "* Create a tidy dataset\n",
    "* Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from sublimpy import variables\n",
    "from sublimpy import utils\n",
    "from sublimpy import tidy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import altair as alt\n",
    "alt.data_transformers.enable('json')\n",
    "\n",
    "from metpy.calc import specific_humidity_from_mixing_ratio\n",
    "from metpy.units import units\n",
    "import metpy.constants\n",
    "import pint_pandas\n",
    "import pint_xarray\n",
    "import xarray as xr\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from metpy.constants import density_water"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_data_dir = '/Users/elischwat/Development/data/sublimationofsnow/sosqc/sos_qc_geo_tiltcor_v20240307/'\n",
    "DATE_FORMAT_STR = '%Y%m%d'\n",
    "start_date = '20221130'\n",
    "# end_date = '20230509'\n",
    "end_date = '20230619'\n",
    "PLANAR_FIT = False\n",
    "\n",
    "datelist = pd.date_range(\n",
    "    dt.datetime.strptime(start_date, DATE_FORMAT_STR),\n",
    "    dt.datetime.strptime(end_date, DATE_FORMAT_STR),\n",
    "    freq='d'\n",
    ").strftime(DATE_FORMAT_STR).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out the eddy covariance measurement variable names because they are very repetitive\n",
    "ec_measurement_suffixes = [\n",
    "    '1m_ue',    '2m_ue',    '3m_ue',    '10m_ue', \n",
    "    '1m_d',     '2m_d',     '3m_d',     '10m_d',\n",
    "    '1m_uw',    '2m_uw',    '2_5m_uw',  '3m_uw',    '10m_uw', \n",
    "    '1m_c',     '2m_c',     '3m_c',     '5m_c',     '10m_c',    '15m_c',    '20m_c'\n",
    "]\n",
    "\n",
    "sonic_measurement_prefixes = [\n",
    "    'u_', 'v_', 'w_', 'tc_', 'spd_', 'dir_', \n",
    "    'u_u__', 'v_v__', 'w_w__', 'tc_tc__', \n",
    "    'u_w__', 'v_w__', 'u_v__', \n",
    "    'u_tc__', 'v_tc__', 'w_tc__', \n",
    "    'u_u_u__', 'v_v_v__', 'w_w_w__', \n",
    "    'tc_tc_tc__', \n",
    "]\n",
    "irga_measurement_prefixes = [\n",
    "    'h2o_', 'h2o_h2o__', 'h2o_h2o_h2o__', \n",
    "]\n",
    "sonic_plus_irga_measurement_prefixes = [\n",
    "    'u_h2o__', 'v_h2o__', 'w_h2o__', \n",
    "]\n",
    "ec_measurement_prefixes = sonic_measurement_prefixes + irga_measurement_prefixes + sonic_plus_irga_measurement_prefixes\n",
    "\n",
    "ec_variable_names = [\n",
    "    (prefix + suffix) for prefix in ec_measurement_prefixes for suffix in ec_measurement_suffixes\n",
    "]\n",
    "\n",
    "counts_vars = ['counts_' + suffix for suffix in ec_measurement_suffixes]\n",
    "counts_1_vars = ['counts_' + suffix + '_1' for suffix in ec_measurement_suffixes]\n",
    "counts_2_vars = ['counts_' + suffix + '_2' for suffix in ec_measurement_suffixes]\n",
    "irgadiag_vars = ['irgadiag_' + suffix for suffix in ec_measurement_suffixes]\n",
    "ldiag_vars = ['ldiag_' + suffix for suffix in ec_measurement_suffixes]\n",
    "\n",
    "diagnostic_variable_names = counts_vars + counts_1_vars + counts_2_vars + irgadiag_vars + ldiag_vars\n",
    "\n",
    "VARIABLE_NAMES = ec_variable_names + diagnostic_variable_names + [\n",
    "    # Temperature & Relative Humidity Array \n",
    "    'T_1m_c', 'T_2m_c', 'T_3m_c', 'T_4m_c', 'T_5m_c', 'T_6m_c', 'T_7m_c', 'T_8m_c', 'T_9m_c', 'T_10m_c',\n",
    "    'T_11m_c', 'T_12m_c', 'T_13m_c', 'T_14m_c', 'T_15m_c', 'T_16m_c', 'T_17m_c', 'T_18m_c', 'T_19m_c', 'T_20m_c',\n",
    "\n",
    "    'RH_1m_c', 'RH_2m_c', 'RH_3m_c', 'RH_4m_c', 'RH_5m_c', 'RH_6m_c', 'RH_7m_c', 'RH_8m_c', 'RH_9m_c', 'RH_10m_c',\n",
    "    'RH_11m_c','RH_12m_c','RH_13m_c','RH_14m_c','RH_15m_c','RH_16m_c','RH_17m_c','RH_18m_c','RH_19m_c','RH_20m_c',\n",
    "\n",
    "    # Pressure Sensors\n",
    "    'P_20m_c',\n",
    "    'P_10m_c', 'P_10m_d', 'P_10m_uw', 'P_10m_ue',\n",
    "\n",
    "    # Blowing snow/FlowCapt Sensors\n",
    "    'SF_avg_1m_ue', 'SF_avg_2m_ue',\n",
    "\n",
    "    # Apogee sensors\n",
    "    \"Vtherm_c\", \"Vtherm_d\", \"Vtherm_ue\", \"Vtherm_uw\", \n",
    "    \"Vpile_c\", \"Vpile_d\", \"Vpile_ue\", \"Vpile_uw\",\n",
    "    \"IDir_c\", \"IDir_d\", \"IDir_ue\", \"IDir_uw\",\n",
    "\n",
    "    # Snow-level temperature arrays (towers D and UW)\n",
    "    'Tsnow_0_4m_d', 'Tsnow_0_5m_d', 'Tsnow_0_6m_d', 'Tsnow_0_7m_d', 'Tsnow_0_8m_d', 'Tsnow_0_9m_d', 'Tsnow_1_0m_d', 'Tsnow_1_1m_d', 'Tsnow_1_2m_d', 'Tsnow_1_3m_d', 'Tsnow_1_4m_d', 'Tsnow_1_5m_d',\n",
    "    'Tsnow_0_4m_uw', 'Tsnow_0_5m_uw', 'Tsnow_0_6m_uw', 'Tsnow_0_7m_uw', 'Tsnow_0_8m_uw', 'Tsnow_0_9m_uw', 'Tsnow_1_0m_uw', 'Tsnow_1_1m_uw', 'Tsnow_1_2m_uw', 'Tsnow_1_3m_uw', 'Tsnow_1_4m_uw', 'Tsnow_1_5m_uw',\n",
    "    \n",
    "    # Downward/Upward Facing Longwave Radiometers\n",
    "    'Rpile_out_9m_d','Tcase_out_9m_d',    \n",
    "    'Rpile_in_9m_d', 'Tcase_in_9m_d',\n",
    "    'Tcase_uw', 'Rpile_in_uw', 'Rpile_out_uw',\n",
    "    \n",
    "    # Upward facing shortwave radiometer (tower D) - for measuring incoming solar radiation!\n",
    "    'Rsw_in_9m_d', 'Rsw_out_9m_d',\n",
    "\n",
    "    # Snow Pillow SWE\n",
    "    'SWE_p1_c', 'SWE_p2_c', 'SWE_p3_c', 'SWE_p4_c',\n",
    "\n",
    "    # Soil Moisture\n",
    "    'Qsoil_d',\n",
    "\n",
    "    # Ground heat flux\n",
    "    'Gsoil_d',\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open and concatenate daily SoS datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file_paths = [\n",
    "    os.path.join(\n",
    "        sos_data_dir,\n",
    "        f'isfs_sos_qc_geo_tiltcor_5min_{date}.nc'\n",
    "    ) for date in datelist\n",
    "]\n",
    "datasets = []\n",
    "for file in all_file_paths:\n",
    "    ds = xr.open_dataset(file)\n",
    "    # this ensures we don't access variables that aren't in this dataset, which would throw an error\n",
    "    ds_new = ds[set(ds.data_vars).intersection(VARIABLE_NAMES)]\n",
    "    datasets.append(ds_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds = xr.concat(datasets, dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure time index is evenly spaced by filling in any missing timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds = utils.fill_missing_timestamps(sos_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sos_ds.to_netcdf(\"sos_ds_temp_storage.cdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sos_ds = xr.open_dataset(\"sos_ds_temp_storage.cdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove instrument-flagged data\n",
    "\n",
    "Based on Stiperski and Rotach (2016, http://link.springer.com/10.1007/s10546-015-0103-z), who recommend the following steps as minimum quality criteria:\n",
    "\n",
    "1. The sonic diagnostic flag was set high (malfunctioning of the instrument) inside the averaging period. \n",
    "2. KH20 voltage fell below 5 mV (indication of condensation occurring on the KH20 window).\n",
    "3. Skewness of temperature and wind components fell outside the [-2, 2] range, following Vickers and Mahrt (1997).\n",
    "4. Kurtosis of temperature and wind components was >8, following Vickers and Mahrt (1997).\n",
    "\n",
    "We only implement number #2 and #3. We tried implementing #1, using the ldiag flag to remove sonic data, but it removed a lot of data, and, without using high rate data, we't cannot filter based on a \"high\" diagnostic flag, we can only filtering using the aggregate of all the flags (i.e. ldiag > 0). The 4th moments are not included in the 5-minute averages, so we cannot implement #4 without using the high rate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set bad Irga measurements to NaN\n",
    "\n",
    "The NCAR report recommends all Irga-related measurements be set to NaN when irgadiag is non-zero.  They did this for some but not all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('h2o_flux_var', 'irgadiag_var', 'old_nan_count_badirga', 'new_nan_count_badirga', 'old_mean', 'new_mean')\n",
    "var_ls = []\n",
    "old_nan_count_badirga_ls = []\n",
    "new_nan_count_badirga_ls = []\n",
    "old_mean_ls = []\n",
    "new_mean_ls = []\n",
    "old_median_ls = []\n",
    "new_median_ls = []\n",
    "for suffix in ec_measurement_suffixes:\n",
    "    h2o_flux_var = 'w_h2o__' + suffix\n",
    "    irgadiag_var = 'irgadiag_' + suffix\n",
    "     \n",
    "    old_nan_count_badirga = (np.isnan(sos_ds[h2o_flux_var])).sum().item()\n",
    "    old_mean = sos_ds[h2o_flux_var].mean().item()\n",
    "    old_median = sos_ds[h2o_flux_var].median().item()\n",
    "\n",
    "    for prefix in ec_measurement_prefixes:\n",
    "        var = prefix + suffix\n",
    "        if var in sos_ds:\n",
    "            sos_ds[var] = sos_ds[var].where(sos_ds[irgadiag_var] == 0)\n",
    "    # for prefix in [\n",
    "    #     'h2o_', 'h2o_h2o__', 'u_h2o__', 'v_h2o__', 'w_h2o__',\n",
    "    #     # I'M NOT SURE I WANT TO REMOVE THESE w_ MEASUREMENTS BUT I\"M CURIOUS WHAT HAPPENS IF I DO\n",
    "    #     'w_',\n",
    "    # ]:\n",
    "\n",
    "\n",
    "    new_nan_count_badirga = (np.isnan(sos_ds[h2o_flux_var])).sum().item()\n",
    "    new_mean = sos_ds[h2o_flux_var].mean().item()\n",
    "    new_median = sos_ds[h2o_flux_var].median().item()\n",
    "    print(h2o_flux_var, irgadiag_var, old_nan_count_badirga, new_nan_count_badirga, round(old_mean,6), round(new_mean,6))\n",
    "    var_ls.append(h2o_flux_var)\n",
    "    old_nan_count_badirga_ls.append(old_nan_count_badirga)\n",
    "    new_nan_count_badirga_ls.append(new_nan_count_badirga)\n",
    "    old_mean_ls.append(old_mean)\n",
    "    new_mean_ls.append(new_mean)\n",
    "    old_median_ls.append(old_median)\n",
    "    new_median_ls.append(new_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,1, sharex=True, figsize=(10,5))\n",
    "axes[0].scatter(var_ls, old_nan_count_badirga_ls, label = 'before', color='tab:blue')\n",
    "axes[0].set_ylabel(\"n of nans\")\n",
    "axes[0].scatter(var_ls, new_nan_count_badirga_ls, label = 'after', color='tab:orange')\n",
    "\n",
    "axes[1].scatter(var_ls, old_mean_ls, label = 'Mean, before', color='tab:blue')\n",
    "axes[1].set_ylabel(\"<w'q'>\")\n",
    "axes[1].scatter(var_ls, new_mean_ls, label = 'Mean, after', color='tab:orange')\n",
    "\n",
    "axes[1].scatter(var_ls, old_median_ls, label = 'Median, before', marker='+', color='tab:blue')\n",
    "axes[1].set_ylabel(\"<w'q'>\")\n",
    "axes[1].scatter(var_ls, new_median_ls, label = 'Median, after', marker='+', color='tab:orange')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.tick_params(rotation=90, axis='x')\n",
    "    ax.legend(title='Filtering', bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set bad Sonic measurements to Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('h2o_flux_var', 'ldiag_var', 'old_nan_count_badsonic', 'new_nan_count_badsonic', 'old_mean', 'new_mean')\n",
    "\n",
    "var_ls = []\n",
    "old_nan_count_badsonic_ls = []\n",
    "new_nan_count_badsonic_ls = []\n",
    "old_mean_ls = []\n",
    "new_mean_ls = []\n",
    "for suffix in ec_measurement_suffixes:\n",
    "    w_var = 'w_' + suffix\n",
    "    h2o_flux_var = 'w_h2o__' + suffix\n",
    "    sonicdiag_var = 'ldiag_' + suffix\n",
    "    old_nan_count_badsonic = (np.isnan(sos_ds[h2o_flux_var])).sum().item()\n",
    "    old_mean = sos_ds[h2o_flux_var].mean().item()\n",
    "    \n",
    "    # sos_ds[h2o_flux_var] = sos_ds[h2o_flux_var].where(sos_ds[sonicdiag_var] == 0)\n",
    "    # sos_ds[w_var] = sos_ds[w_var].where(sos_ds[sonicdiag_var] == 0)\n",
    "    for prefix in ec_measurement_prefixes:\n",
    "        var = prefix + suffix\n",
    "        if var in sos_ds:\n",
    "            sos_ds[var] = sos_ds[var].where(sos_ds[sonicdiag_var] <= 0.1)\n",
    "\n",
    "    new_nan_count_badsonic = (np.isnan(sos_ds[h2o_flux_var])).sum().item()\n",
    "    new_mean = sos_ds[h2o_flux_var].mean().item()\n",
    "    print(h2o_flux_var, sonicdiag_var, old_nan_count_badsonic, new_nan_count_badsonic, round(old_mean,6), round(new_mean,6))\n",
    "    var_ls.append(h2o_flux_var)\n",
    "    old_nan_count_badsonic_ls.append(old_nan_count_badsonic)\n",
    "    new_nan_count_badsonic_ls.append(new_nan_count_badsonic)\n",
    "    old_mean_ls.append(old_mean)\n",
    "    new_mean_ls.append(new_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,1, sharex=True, figsize=(10,5))\n",
    "axes[0].scatter(var_ls, old_nan_count_badsonic_ls, label = 'Before')\n",
    "axes[0].set_ylabel(\"n of nans\")\n",
    "axes[0].scatter(var_ls, new_nan_count_badsonic_ls, label = 'After')\n",
    "\n",
    "axes[1].scatter(var_ls, old_mean_ls, label = 'Before')\n",
    "axes[1].set_ylabel(\"<w'q'>\")\n",
    "axes[1].scatter(var_ls, new_mean_ls, label = 'After')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.tick_params(rotation=45)\n",
    "    ax.legend(title='Filtering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolate missing EC variables up to 1 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('h2o_flux_var', 'ldiag_var', 'old_nan_count_afterfilling', 'new_nan_count_afterfilling', 'old_mean', 'new_mean')\n",
    "\n",
    "var_ls = []\n",
    "old_nan_count_afterfilling_ls = []\n",
    "new_nan_count_afterfilling_ls = []\n",
    "old_mean_ls = []\n",
    "new_mean_ls = []\n",
    "for suffix in ec_measurement_suffixes:\n",
    "    w_var = 'w_' + suffix\n",
    "    h2o_flux_var = 'w_h2o__' + suffix\n",
    "    sonicdiag_var = 'ldiag_' + suffix\n",
    "    old_nan_count_afterfilling = (np.isnan(sos_ds[h2o_flux_var])).sum().item()\n",
    "    old_mean = sos_ds[h2o_flux_var].mean().item()\n",
    "    \n",
    "    for prefix in ec_measurement_prefixes:\n",
    "        var = prefix + suffix\n",
    "        if var in sos_ds:\n",
    "            sos_ds[var] = sos_ds[var].interpolate_na(dim='time', method='linear', limit=12)\n",
    "\n",
    "    new_nan_count_afterfilling = (np.isnan(sos_ds[h2o_flux_var])).sum().item()\n",
    "    new_mean = sos_ds[h2o_flux_var].mean().item()\n",
    "    print(h2o_flux_var, sonicdiag_var, old_nan_count_afterfilling, new_nan_count_afterfilling, round(old_mean,6), round(new_mean,6))\n",
    "    var_ls.append(h2o_flux_var)\n",
    "    old_nan_count_afterfilling_ls.append(old_nan_count_afterfilling)\n",
    "    new_nan_count_afterfilling_ls.append(new_nan_count_afterfilling)\n",
    "    old_mean_ls.append(old_mean)\n",
    "    new_mean_ls.append(new_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,1, sharex=True, figsize=(10,5))\n",
    "axes[0].scatter(var_ls, old_nan_count_afterfilling_ls, label = 'Before')\n",
    "axes[0].set_ylabel(\"n of nans\")\n",
    "axes[0].scatter(var_ls, new_nan_count_afterfilling_ls, label = 'After')\n",
    "\n",
    "axes[1].scatter(var_ls, old_mean_ls, label = 'Before')\n",
    "axes[1].set_ylabel(\"<w'q'>\")\n",
    "axes[1].scatter(var_ls, new_mean_ls, label = 'After')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.tick_params(rotation=45)\n",
    "    ax.legend(title='Filtering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counts_df = pd.DataFrame({\n",
    "    'variable':                     var_ls,\n",
    "    'n':                            len(sos_ds.time),\n",
    "    'original nan count':           old_nan_count_badirga_ls, \n",
    "    'nans after bad irga removed':  new_nan_count_badirga_ls, \n",
    "    'nans after bad sonic removed': new_nan_count_badsonic_ls, \n",
    "    'nans after gap filling':       new_nan_count_afterfilling_ls,\n",
    "})\n",
    "limited_nan_counts_df = nan_counts_df[ \n",
    "    (~nan_counts_df.variable.str.contains('__1m_'))\n",
    "    \n",
    "    &\n",
    "    (~nan_counts_df.variable.str.contains('__2_5m_'))\n",
    "]\n",
    "limited_nan_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_nan_counts_df['Data removed by EC150 flag'] = limited_nan_counts_df['nans after bad irga removed'] - limited_nan_counts_df['original nan count']\n",
    "limited_nan_counts_df['Data removed by CSAT3 flag'] = limited_nan_counts_df['nans after bad sonic removed'] - limited_nan_counts_df['nans after bad irga removed']\n",
    "limited_nan_counts_df[[\n",
    "    'variable',\n",
    "    'n',\n",
    "    'Data removed by EC150 flag',\n",
    "    'Data removed by CSAT3 flag'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "src = sos_ds[['SF_avg_1m_ue', 'SF_avg_2m_ue', 'ldiag_3m_c']].to_dataframe().reset_index()\n",
    "src['SF_avg_ue'] = src['SF_avg_1m_ue'] + src['SF_avg_2m_ue']\n",
    "src = utils.modify_df_timezone(src, pytz.UTC, 'US/Mountain')\n",
    "src['on Dec 21/22'] = (src.time.dt.date == dt.date(2022,12,22)) | (src.time.dt.date == dt.date(2022,12,21))\n",
    "src = src.query(\"SF_avg_ue > 0\")\n",
    "rule = alt.Chart().transform_calculate(rule = '0.1').mark_rule(strokeDash=[2,2]).encode(y='rule:Q')\n",
    "bad_sonic_data = (\n",
    "    rule + alt.Chart(src).mark_circle(size=10).encode(\n",
    "        alt.X(\"SF_avg_ue\").title(\"Blowing snow flux (g/m^2/s)\").scale(type='log'),\n",
    "        alt.Y(\"ldiag_3m_c\").title([\"Fraction of 20hz sonic anemometer\", \"measurements flagged (Tower C, 3m)\"]),\n",
    "        alt.Color(\"on Dec 21/22:N\")\n",
    "    ).properties(width=200, height=200)\n",
    ").configure_axis(grid=False).configure_legend(columns=2, orient='top')\n",
    "bad_sonic_data.save(\"bad_sonic_data.png\", ppi=200)\n",
    "bad_sonic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src.query(\"ldiag_3m_c > 0.1\")['on Dec 21/22'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "src = sos_ds[['RH_3m_c', 'RH_20m_c', 'irgadiag_3m_c', 'irgadiag_20m_c']].to_dataframe().reset_index()\n",
    "src = utils.modify_df_timezone(src, pytz.UTC, 'US/Mountain')\n",
    "\n",
    "bad_irga_data_3m = (\n",
    "    alt.Chart(src).mark_circle(size=10).encode(\n",
    "        alt.X(\"RH_3m_c:Q\").title(\"Relative humidity, 3m (%)\"),\n",
    "        alt.Y(\"irgadiag_3m_c:Q\").title([\"Sum of EC150 diagnostic flags\", \"3m\"]).scale(type='symlog').axis(\n",
    "            values=[0,10,100,1000,10000,100000,1000000], format=\"1e\"\n",
    "        ),\n",
    "    ).properties(width=200, height=200)\n",
    ")\n",
    "bad_irga_data_20m = (\n",
    "    alt.Chart(src).mark_circle(size=10).encode(\n",
    "        alt.X(\"RH_20m_c:Q\").title(\"Relative humidity, 20m (%)\"),\n",
    "        alt.Y(\"irgadiag_20m_c:Q\").title([\"Sum of EC150 diagnostic flags\", \"20m\"]).scale(type='symlog').axis(\n",
    "            values=[0,10,100,1000,10000,100000,1000000], format=\"1e\"\n",
    "        ),\n",
    "    ).properties(width=200, height=200)\n",
    ")\n",
    "bad_irga_data = (bad_irga_data_3m | bad_irga_data_20m).configure_axis(grid=False)\n",
    "bad_irga_data.save(\"bad_irga_data.png\", ppi=200)\n",
    "bad_irga_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dictionary defining the resampling function to use for each variable\n",
    "\n",
    "Covariances are resampled according to the rules of **Reynold averaging** (https://www.eol.ucar.edu/content/combining-short-term-moments-longer-time-periods).\n",
    "\n",
    "Meteorological and turbulence measurements (other than covariances) are resampled using the **mean**.\n",
    "\n",
    "EC count variables are **summed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vars_processing_dict = {\n",
    "    'reynolds_average': [\n",
    "        'u_u__1m_uw',    'v_v__1m_uw',    'w_w__1m_uw',    'u_w__1m_uw',    'v_w__1m_uw',  'u_tc__1m_uw',  'v_tc__1m_uw',   'u_h2o__1m_uw',  'v_h2o__1m_uw',   'w_tc__1m_uw',   'w_h2o__1m_uw',\n",
    "        'u_u__2m_uw',    'v_v__2m_uw',    'w_w__2m_uw',    'u_w__2m_uw',    'v_w__2m_uw',  'u_tc__2m_uw',  'v_tc__2m_uw',   'u_h2o__2m_uw',  'v_h2o__2m_uw',   'w_tc__2m_uw',   'w_h2o__2m_uw',\n",
    "        'u_u__2_5m_uw', 'v_v__2_5m_uw',   'w_w__2_5m_uw',  'u_w__2_5m_uw',  'v_w__2_5m_uw','u_tc__2_5m_uw','v_tc__2_5m_uw', 'u_h2o__2_5m_uw','v_h2o__2_5m_uw', 'w_tc__2_5m_uw', 'w_h2o__2_5m_uw',\n",
    "        'u_u__3m_uw',    'v_v__3m_uw',    'w_w__3m_uw',    'u_w__3m_uw',    'v_w__3m_uw',  'u_tc__3m_uw',  'v_tc__3m_uw',   'u_h2o__3m_uw',  'v_h2o__3m_uw',   'w_tc__3m_uw',   'w_h2o__3m_uw',\n",
    "        'u_u__10m_uw',   'v_v__10m_uw',   'w_w__10m_uw',   'u_w__10m_uw',   'v_w__10m_uw', 'u_tc__10m_uw', 'v_tc__10m_uw',  'u_h2o__10m_uw', 'v_h2o__10m_uw',  'w_tc__10m_uw',  'w_h2o__10m_uw',\n",
    "        'u_u__1m_ue',    'v_v__1m_ue',    'w_w__1m_ue',    'u_w__1m_ue',    'v_w__1m_ue',  'u_tc__1m_ue',  'v_tc__1m_ue',   'u_h2o__1m_ue',  'v_h2o__1m_ue',   'w_tc__1m_ue',   'w_h2o__1m_ue',\n",
    "        'u_u__2m_ue',    'v_v__2m_ue',    'w_w__2m_ue',    'u_w__2m_ue',    'v_w__2m_ue',  'u_tc__2m_ue',  'v_tc__2m_ue',   'u_h2o__2m_ue',  'v_h2o__2m_ue',   'w_tc__2m_ue',   'w_h2o__2m_ue',\n",
    "        'u_u__3m_ue',    'v_v__3m_ue',    'w_w__3m_ue',    'u_w__3m_ue',    'v_w__3m_ue',  'u_tc__3m_ue',  'v_tc__3m_ue',   'u_h2o__3m_ue',  'v_h2o__3m_ue',   'w_tc__3m_ue',   'w_h2o__3m_ue',\n",
    "        'u_u__10m_ue',   'v_v__10m_ue',   'w_w__10m_ue',   'u_w__10m_ue',   'v_w__10m_ue', 'u_tc__10m_ue', 'v_tc__10m_ue',  'u_h2o__10m_ue', 'v_h2o__10m_ue',  'w_tc__10m_ue',  'w_h2o__10m_ue',\n",
    "        'u_u__1m_d',     'v_v__1m_d',     'w_w__1m_d',     'u_w__1m_d',     'v_w__1m_d',   'u_tc__1m_d',   'v_tc__1m_d',    'u_h2o__1m_d',   'v_h2o__1m_d',    'w_tc__1m_d',    'w_h2o__1m_d',\n",
    "        'u_u__2m_d',     'v_v__2m_d',     'w_w__2m_d',     'u_w__2m_d',     'v_w__2m_d',   'u_tc__2m_d',   'v_tc__2m_d',    'u_h2o__2m_d',   'v_h2o__2m_d',    'w_tc__2m_d',    'w_h2o__2m_d',\n",
    "        'u_u__3m_d',     'v_v__3m_d',     'w_w__3m_d',     'u_w__3m_d',     'v_w__3m_d',   'u_tc__3m_d',   'v_tc__3m_d',    'u_h2o__3m_d',   'v_h2o__3m_d',    'w_tc__3m_d',    'w_h2o__3m_d',\n",
    "        'u_u__10m_d',    'v_v__10m_d',    'w_w__10m_d',    'u_w__10m_d',    'v_w__10m_d',  'u_tc__10m_d',  'v_tc__10m_d',   'u_h2o__10m_d',  'v_h2o__10m_d',   'w_tc__10m_d',   'w_h2o__10m_d',\n",
    "        'u_u__1m_c',     'v_v__1m_c',     'w_w__1m_c',     'u_w__1m_c',     'v_w__1m_c',   'u_tc__1m_c',   'v_tc__1m_c',    'u_h2o__1m_c',   'v_h2o__1m_c',    'w_tc__1m_c',    'w_h2o__1m_c',\n",
    "        'u_u__2m_c',     'v_v__2m_c',     'w_w__2m_c',     'u_w__2m_c',     'v_w__2m_c',   'u_tc__2m_c',   'v_tc__2m_c',    'u_h2o__2m_c',   'v_h2o__2m_c',    'w_tc__2m_c',    'w_h2o__2m_c',\n",
    "        'u_u__3m_c',     'v_v__3m_c',     'w_w__3m_c',     'u_w__3m_c',     'v_w__3m_c',   'u_tc__3m_c',   'v_tc__3m_c',    'u_h2o__3m_c',   'v_h2o__3m_c',    'w_tc__3m_c',    'w_h2o__3m_c',\n",
    "        'u_u__5m_c',     'v_v__5m_c',     'w_w__5m_c',     'u_w__5m_c',     'v_w__5m_c',   'u_tc__5m_c',   'v_tc__5m_c',    'u_h2o__5m_c',   'v_h2o__5m_c',    'w_tc__5m_c',    'w_h2o__5m_c',\n",
    "        'u_u__10m_c',    'v_v__10m_c',    'w_w__10m_c',    'u_w__10m_c',    'v_w__10m_c',  'u_tc__10m_c',  'v_tc__10m_c',   'u_h2o__10m_c',  'v_h2o__10m_c',   'w_tc__10m_c',   'w_h2o__10m_c',\n",
    "        'u_u__15m_c',    'v_v__15m_c',    'w_w__15m_c',    'u_w__15m_c',    'v_w__15m_c',  'u_tc__15m_c',  'v_tc__15m_c',   'u_h2o__15m_c',  'v_h2o__15m_c',   'w_tc__15m_c',   'w_h2o__15m_c',\n",
    "        'u_u__20m_c',    'v_v__20m_c',    'w_w__20m_c',    'u_w__20m_c',    'v_w__20m_c',  'u_tc__20m_c',  'v_tc__20m_c',   'u_h2o__20m_c',  'v_h2o__20m_c',   'w_tc__20m_c',   'w_h2o__20m_c',\n",
    "    ],\n",
    "    'average': [\n",
    "        # Sonic anemometer data\n",
    "        'h2o_1m_uw' ,       'tc_1m_uw',     'spd_1m_uw',    'u_1m_uw',  'v_1m_uw',   'w_1m_uw',  \n",
    "        'h2o_3m_uw' ,       'tc_3m_uw',     'spd_3m_uw',    'u_3m_uw',  'v_3m_uw',   'w_3m_uw',  \n",
    "        'h2o_10m_uw' ,      'tc_10m_uw',    'spd_10m_uw',   'u_10m_uw', 'v_10m_uw',  'w_10m_uw',  \n",
    "        'h2o_1m_ue' ,       'tc_1m_ue',     'spd_1m_ue',    'u_1m_ue',  'v_1m_ue',   'w_1m_ue',  \n",
    "        'h2o_3m_ue' ,       'tc_3m_ue',     'spd_3m_ue',    'u_3m_ue',  'v_3m_ue',   'w_3m_ue',  \n",
    "        'h2o_10m_ue' ,      'tc_10m_ue',    'spd_10m_ue',   'u_10m_ue', 'v_10m_ue',  'w_10m_ue',  \n",
    "        'h2o_1m_d' ,        'tc_1m_d',      'spd_1m_d',     'u_1m_d',   'v_1m_d',    'w_1m_d',  \n",
    "        'h2o_3m_d' ,        'tc_3m_d',      'spd_3m_d',     'u_3m_d',   'v_3m_d',    'w_3m_d',  \n",
    "        'h2o_10m_d' ,       'tc_10m_d',     'spd_10m_d',    'u_10m_d',  'v_10m_d',   'w_10m_d',  \n",
    "        'h2o_2m_c' ,        'tc_2m_c',      'spd_2m_c',     'u_2m_c',   'v_2m_c',    'w_2m_c',  \n",
    "        'h2o_3m_c' ,        'tc_3m_c',      'spd_3m_c',     'u_3m_c',   'v_3m_c',    'w_3m_c',  \n",
    "        'h2o_5m_c' ,        'tc_5m_c',      'spd_5m_c',     'u_5m_c',   'v_5m_c',    'w_5m_c',  \n",
    "        'h2o_10m_c' ,       'tc_10m_c',     'spd_10m_c',    'u_10m_c',  'v_10m_c',   'w_10m_c',  \n",
    "        'h2o_15m_c' ,       'tc_15m_c',     'spd_15m_c',    'u_15m_c',  'v_15m_c',   'w_15m_c',  \n",
    "        'h2o_20m_c' ,       'tc_20m_c',     'spd_20m_c',    'u_20m_c',  'v_20m_c',   'w_20m_c',  \n",
    "\n",
    "        # Temperature & Relative Humidity Array \n",
    "        'T_1m_c', 'T_2m_c', 'T_3m_c', 'T_4m_c', 'T_5m_c', 'T_6m_c', 'T_7m_c', 'T_8m_c', 'T_9m_c', 'T_10m_c',\n",
    "        'T_11m_c', 'T_12m_c', 'T_13m_c', 'T_14m_c', 'T_15m_c', 'T_16m_c', 'T_17m_c', 'T_18m_c', 'T_19m_c', 'T_20m_c',\n",
    "\n",
    "        'RH_1m_c', 'RH_2m_c', 'RH_3m_c', 'RH_4m_c', 'RH_5m_c', 'RH_6m_c', 'RH_7m_c', 'RH_8m_c', 'RH_9m_c', 'RH_10m_c',\n",
    "        'RH_11m_c','RH_12m_c','RH_13m_c','RH_14m_c','RH_15m_c','RH_16m_c','RH_17m_c','RH_18m_c','RH_19m_c','RH_20m_c',\n",
    "\n",
    "        # Pressure Sensors\n",
    "        'P_20m_c',\n",
    "        'P_10m_c', 'P_10m_d', 'P_10m_uw', 'P_10m_ue',\n",
    "\n",
    "        # Blowing snow/FlowCapt Sensors\n",
    "        'SF_avg_1m_ue', 'SF_avg_2m_ue',\n",
    "\n",
    "        # Apogee sensors\n",
    "        \"Vtherm_c\", \"Vtherm_d\", \"Vtherm_ue\", \"Vtherm_uw\", \n",
    "        \"Vpile_c\", \"Vpile_d\", \"Vpile_ue\", \"Vpile_uw\",\n",
    "        \"IDir_c\", \"IDir_d\", \"IDir_ue\", \"IDir_uw\",\n",
    "\n",
    "        # Snow-level temperature arrays (towers D and UW)\n",
    "        'Tsnow_0_4m_d', 'Tsnow_0_5m_d', 'Tsnow_0_6m_d', 'Tsnow_0_7m_d', 'Tsnow_0_8m_d', 'Tsnow_0_9m_d', 'Tsnow_1_0m_d', 'Tsnow_1_1m_d', 'Tsnow_1_2m_d', 'Tsnow_1_3m_d', 'Tsnow_1_4m_d', 'Tsnow_1_5m_d',\n",
    "        'Tsnow_0_4m_uw', 'Tsnow_0_5m_uw', 'Tsnow_0_6m_uw', 'Tsnow_0_7m_uw', 'Tsnow_0_8m_uw', 'Tsnow_0_9m_uw', 'Tsnow_1_0m_uw', 'Tsnow_1_1m_uw', 'Tsnow_1_2m_uw', 'Tsnow_1_3m_uw', 'Tsnow_1_4m_uw', 'Tsnow_1_5m_uw',\n",
    "        \n",
    "        # Downward Facing Longwave Radiometer (tower D) - for measuring snow surface temperature\n",
    "        'Rpile_out_9m_d',\n",
    "        'Tcase_out_9m_d',    \n",
    "        # Upward Facing Longwave Radiometer (tower D)\n",
    "        'Rpile_in_9m_d',\n",
    "        'Tcase_in_9m_d',\n",
    "        # Downward Facing Longwave Radiometer (tower UW) - for measuring snow surface temperature\n",
    "        'Tcase_uw', 'Rpile_in_uw', 'Rpile_out_uw',\n",
    "        \n",
    "        # Upward facing shortwave radiometer (tower D) - for measuring incoming solar radiation!\n",
    "        'Rsw_in_9m_d',\n",
    "        'Rsw_out_9m_d',\n",
    "\n",
    "        # Snow Pillow SWE\n",
    "        'SWE_p1_c', 'SWE_p2_c', 'SWE_p3_c', 'SWE_p4_c',\n",
    "\n",
    "        # Soil Moisture\n",
    "        'Qsoil_d',\n",
    "\n",
    "        # Soil Moisture\n",
    "        'Gsoil_d',\n",
    "        \n",
    "        # Diagnostic variables for irga and sonic\n",
    "        'irgadiag_1m_c',    'ldiag_1m_c',\n",
    "        'irgadiag_2m_c',    'ldiag_2m_c',\n",
    "        'irgadiag_3m_c',    'ldiag_3m_c',\n",
    "        'irgadiag_5m_c',    'ldiag_5m_c',\n",
    "        'irgadiag_10m_c',   'ldiag_10m_c',\n",
    "        'irgadiag_15m_c',   'ldiag_15m_c',\n",
    "        'irgadiag_20m_c',   'ldiag_20m_c',\n",
    "        'irgadiag_1m_uw',   'ldiag_1m_uw',\n",
    "        'irgadiag_3m_uw',   'ldiag_3m_uw',\n",
    "        'irgadiag_10m_uw',  'ldiag_10m_uw',\n",
    "        'irgadiag_1m_ue',   'ldiag_1m_ue',\n",
    "        'irgadiag_3m_ue',   'ldiag_3m_ue',\n",
    "        'irgadiag_10m_ue',  'ldiag_10m_ue',\n",
    "        'irgadiag_1m_d',    'ldiag_1m_d',\n",
    "        'irgadiag_3m_d',    'ldiag_3m_d',\n",
    "        'irgadiag_10m_d',   'ldiag_10m_d',\n",
    "    ],\n",
    "    'median' : [\n",
    "        'dir_1m_uw',    \n",
    "        'dir_3m_uw',    \n",
    "        'dir_10m_uw',   \n",
    "        'dir_1m_ue',    \n",
    "        'dir_3m_ue',    \n",
    "        'dir_10m_ue',   \n",
    "        'dir_1m_d',     \n",
    "        'dir_3m_d',     \n",
    "        'dir_10m_d',    \n",
    "        'dir_2m_c',     \n",
    "        'dir_3m_c',     \n",
    "        'dir_5m_c',     \n",
    "        'dir_10m_c',    \n",
    "        'dir_15m_c',    \n",
    "        'dir_20m_c',    \n",
    "    ],\n",
    "    'sum' : [\n",
    "        # Counts of unflagged instantaneous (20hz) eddy covariance measurements\n",
    "        # momentum flux flags   LHFlux flags   SH flux flags       \n",
    "        'counts_1m_c',    'counts_1m_c_1',    'counts_1m_c_2',    \n",
    "        'counts_2m_c',    'counts_2m_c_1',    'counts_2m_c_2',    \n",
    "        'counts_3m_c',    'counts_3m_c_1',    'counts_3m_c_2',    \n",
    "        'counts_5m_c',    'counts_5m_c_1',    'counts_5m_c_2',    \n",
    "        'counts_10m_c',   'counts_10m_c_1',   'counts_10m_c_2',   \n",
    "        'counts_15m_c',   'counts_15m_c_1',   'counts_15m_c_2',   \n",
    "        'counts_20m_c',   'counts_20m_c_1',   'counts_20m_c_2',   \n",
    "        'counts_1m_uw',   'counts_1m_uw_1',   'counts_1m_uw_2',   \n",
    "        'counts_3m_uw',   'counts_3m_uw_1',   'counts_3m_uw_2',   \n",
    "        'counts_10m_uw',  'counts_10m_uw_1',  'counts_10m_uw_2',  \n",
    "        'counts_1m_ue',   'counts_1m_ue_1',   'counts_1m_ue_2',   \n",
    "        'counts_3m_ue',   'counts_3m_ue_1',   'counts_3m_ue_2',   \n",
    "        'counts_10m_ue',  'counts_10m_ue_1',  'counts_10m_ue_2',  \n",
    "        'counts_1m_d',    'counts_1m_d_1',    'counts_1m_d_2',    \n",
    "        'counts_3m_d',    'counts_3m_d_1',    'counts_3m_d_2',    \n",
    "        'counts_10m_d',   'counts_10m_d_1',   'counts_10m_d_2',   \n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function for resampling covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_covariance_variable_name(cov_name):\n",
    "    \"\"\"Get the names of the two mean variables associated with a covariance variable. Built to use\n",
    "    with SOS datasets. For example, one might provide `w_h2o__3m_c` and this function will return\n",
    "    `w_3m_c` and `h2o_3m_c`.\n",
    "\n",
    "    Args:\n",
    "        cov_name (str): name of variable that you want to separate into the two names of the \n",
    "        asssociated mean variables.\n",
    "\n",
    "    Returns:\n",
    "        var1, var 2 (str, str): two strings with the names of the two mean variables\n",
    "    \"\"\"\n",
    "    [first_parts, second_part] = cov_name.split('__')\n",
    "    [var1, var2] = first_parts.split('_')\n",
    "    [var1, var2] = [\n",
    "        var1 +'_' + second_part,\n",
    "        var2 +'_' + second_part,\n",
    "    ]\n",
    "    return var1, var2\n",
    "\n",
    "def resample_moment(df, cov, mean1, mean2, new_frequency, n_in_new_re_length, skipna=True):\n",
    "    \"\"\"Combines moments into longer time periods, using reynolds averaging. Built to use with SOS \n",
    "    datasets. Resampling covariances which have been calculated for a specific Reynolds\n",
    "    averaging length (e.g. the SOS datasets are averaged to 5minutes), you need both the mean\n",
    "    values and covariance. For example, the variable `w_h2o__3m_c` is associated with mean values\n",
    "    `w_3m_c` and `h2o_3m_c`. To reasmple `w_h2o__3m_c` to another averaging length, we need the three\n",
    "    variables.\n",
    "\n",
    "    Args:\n",
    "        df (pd.Dataframe): Dataframe containing the three columns required for calculations (contains)\n",
    "                    the names supplied as parameters `cov`, `mean1`, and `mean2`.\n",
    "        cov (str): Name of covariance variable to resample using Reynolds averaing\n",
    "        mean1 (str): Name of one of the two mean variables associated with `cov`\n",
    "        mean2 (str): Name of the other mean variable associated with `cov`\n",
    "        new_frequency (str): String interpretable by pandas/xarray that describes the reynolds length you \n",
    "            are resampling to. EG: '60Min'\n",
    "        n_in_new_re_length (_type_): Number of 5 minute intervals that fit in the new_frequency. E.G. for\n",
    "            new_frequency='60Min', you would provide 12.\n",
    "        skipna (bool, optional): Whether to skip NaNs when calculating the new variables. Providing True\n",
    "            will allow more moments to be calculated, but those moments may be inaccurate/non-representative.\n",
    "            Providing False will result in more missing data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with resampled data.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame({\n",
    "            cov: df.groupby(pd.Grouper(freq=new_frequency)).apply(\n",
    "                lambda row: \n",
    "                    (1/n_in_new_re_length)*(row[cov] + row[mean1]*row[mean2]).sum(skipna=skipna)\n",
    "                    - (\n",
    "                        (1/n_in_new_re_length)*row[mean1].sum(skipna=skipna)\n",
    "                        * (1/n_in_new_re_length)*row[mean2].sum(skipna=skipna)\n",
    "                    )\n",
    "            )\n",
    "        })\n",
    "\n",
    "def resample(ds, new_frequency, n_in_new_re_length, skipna=True):\n",
    "    \"\"\"Resample SOS xarray datasets, applying the proper aggregation function\n",
    "    for different variables. Some are resampled by taking the mean, some by \n",
    "    summing, and others by Reynolds averaging. \n",
    "    \"\"\"\n",
    "    # Resample data vars that need to be averaged (plain old averaging)\n",
    "    # Use built in xarray functionality\n",
    "    resampled_averages = ds[\n",
    "        data_vars_processing_dict['average']\n",
    "    ].to_dataframe().resample(new_frequency).mean().to_xarray()\n",
    "\n",
    "    resampled_medians = ds[\n",
    "        data_vars_processing_dict['median']\n",
    "    ].to_dataframe().resample(new_frequency).median().to_xarray()\n",
    "    \n",
    "    # Resample data vars that need to be summed\n",
    "    # Use built in xarray functionality\n",
    "    resampled_sums = ds[\n",
    "        data_vars_processing_dict['sum']\n",
    "    ].to_dataframe().resample(new_frequency).sum().to_xarray()\n",
    "    \n",
    "    # Resample data vars that need to be summed using the rules of Reynolds Averaging\n",
    "    # Use our custom function defined above\n",
    "    resampled_reynolds_averages_list = []\n",
    "    def split_covariance_name_and_resample(name):\n",
    "        mean_var1, mean_var2 = separate_covariance_variable_name(name)\n",
    "        resampled = resample_moment(\n",
    "            ds[[mean_var1, mean_var2, name]].to_dataframe(), \n",
    "            name, \n",
    "            mean_var1, \n",
    "            mean_var2, \n",
    "            new_frequency, \n",
    "            n_in_new_re_length, \n",
    "            skipna=skipna\n",
    "        )\n",
    "        return resampled.to_xarray()\n",
    "    resampled_reynolds_averages_list =  Parallel(n_jobs = 8)(\n",
    "        delayed(split_covariance_name_and_resample)(name) \n",
    "        for name in tqdm(data_vars_processing_dict['reynolds_average'])\n",
    "    )\n",
    "    \n",
    "    new_ds = xr.merge(\n",
    "        [\n",
    "            resampled_sums, \n",
    "            resampled_medians,\n",
    "            resampled_averages\n",
    "        ] + resampled_reynolds_averages_list\n",
    "    )\n",
    "\n",
    "    ## Copy attributes from the original dataset\n",
    "    new_ds.attrs = ds.attrs\n",
    "    for var in new_ds:\n",
    "        new_ds[var].attrs = ds[var].attrs\n",
    "    return new_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds10min = resample(sos_ds, '10Min', 2, skipna=False)\n",
    "sos_ds30min = resample(sos_ds, '30Min', 6, skipna=False)\n",
    "sos_ds60min = resample(sos_ds, '60Min', 12, skipna=False)\n",
    "sos_ds120min = resample(sos_ds, '120Min', 24, skipna=False)\n",
    "\n",
    "sos_ds5min = sos_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sos_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count nans for lhflux variables in the 60min resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for suffix in ec_measurement_suffixes:\n",
    "    h2o_flux_var = 'w_h2o__' + suffix\n",
    "    print(h2o_flux_var, sos_ds60min[h2o_flux_var].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for stationarity\n",
    "\n",
    "Following https://link.springer.com/10.1007/978-3-030-52171-4_55, http://link.springer.com/10.1007/s10546-015-0103-zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data30min_matching_5min = (np.array([\n",
    "    sos_ds30min[var].values,\n",
    "    sos_ds30min[var].values,\n",
    "    sos_ds30min[var].values,\n",
    "    sos_ds30min[var].values,\n",
    "    sos_ds30min[var].values,\n",
    "    sos_ds30min[var].values,\n",
    "]).T).flatten()\n",
    "\n",
    "data30min_matching_5min\n",
    "stationary_stat =  100 * (data30min_matching_5min - sos_ds5min[var].values) / data30min_matching_5min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in limited_nan_counts_df.variable:\n",
    "    w_h2o__3m_c_stationarity_stat = 100*(\n",
    "        sos_ds30min[var].interp(time = sos_ds5min.time).to_dataframe()\n",
    "        -\n",
    "        sos_ds5min[var].to_dataframe()\n",
    "    ) / sos_ds5min[var].to_dataframe()\n",
    "    print(\n",
    "        var,\n",
    "        (w_h2o__3m_c_stationarity_stat > 30).sum().values, \n",
    "        (w_h2o__3m_c_stationarity_stat <= 30).sum().values,\n",
    "        round((((w_h2o__3m_c_stationarity_stat > 30).sum().values) / (\n",
    "            ((w_h2o__3m_c_stationarity_stat > 30).sum().values + (w_h2o__3m_c_stationarity_stat <= 30).sum().values)\n",
    "        ))[0], 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in limited_nan_counts_df.variable:\n",
    "    w_h2o__3m_c_stationarity_stat = 100*(\n",
    "        sos_ds5min[var].to_dataframe().resample('30Min').mean()\n",
    "        -\n",
    "        sos_ds30min[var].to_dataframe()\n",
    "    ) / sos_ds30min[var].to_dataframe()\n",
    "    print(\n",
    "        var,\n",
    "        (w_h2o__3m_c_stationarity_stat > 30).sum().values, \n",
    "        (w_h2o__3m_c_stationarity_stat <= 30).sum().values,\n",
    "        round((((w_h2o__3m_c_stationarity_stat > 30).sum().values) / (\n",
    "            ((w_h2o__3m_c_stationarity_stat > 30).sum().values + (w_h2o__3m_c_stationarity_stat <= 30).sum().values)\n",
    "        ))[0], 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot latent and sensible heat flux means for different resampling intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lhflux_vars =[\n",
    "    # 'w_h2o__1m_uw',\n",
    "    'w_h2o__3m_uw',\n",
    "    'w_h2o__10m_uw',\n",
    "    # 'w_h2o__1m_ue',\n",
    "    'w_h2o__3m_ue',\n",
    "    'w_h2o__10m_ue',\n",
    "    # 'w_h2o__1m_d',\n",
    "    'w_h2o__3m_d',\n",
    "    'w_h2o__10m_d',\n",
    "    'w_h2o__2m_c',\n",
    "    'w_h2o__3m_c',\n",
    "    'w_h2o__5m_c',\n",
    "    'w_h2o__10m_c',\n",
    "    'w_h2o__15m_c',\n",
    "    'w_h2o__20m_c',\n",
    "]\n",
    "sos_ds_5min_mean_lhflux = sos_ds5min[lhflux_vars].to_dataframe().dropna().mean()\n",
    "sos_ds_10min_mean_lhflux = sos_ds10min[lhflux_vars].to_dataframe().dropna().mean()\n",
    "sos_ds_30min_mean_lhflux = sos_ds30min[lhflux_vars].to_dataframe().dropna().mean()\n",
    "sos_ds_60min_mean_lhflux = sos_ds60min[lhflux_vars].to_dataframe().dropna().mean()\n",
    "sos_ds_120min_mean_lhflux = sos_ds120min[lhflux_vars].to_dataframe().dropna().mean()\n",
    "\n",
    "mean_lhflux_estimates = pd.concat([\n",
    "    pd.DataFrame(sos_ds_5min_mean_lhflux).rename(columns={0:'5min'}),\n",
    "    pd.DataFrame(sos_ds_10min_mean_lhflux).rename(columns={0:'10min'}),\n",
    "    pd.DataFrame(sos_ds_30min_mean_lhflux).rename(columns={0:'30min'}),\n",
    "    pd.DataFrame(sos_ds_60min_mean_lhflux).rename(columns={0:'60min'}),\n",
    "    pd.DataFrame(sos_ds_120min_mean_lhflux).rename(columns={0:'120min'}),\n",
    "], axis=1)\n",
    "mean_lhflux_estimates.index.name = 'location'\n",
    "mean_lhflux_estimates = mean_lhflux_estimates.reset_index()\n",
    "mean_lhflux_estimates\n",
    "mean_lhflux_estimates['height'] = mean_lhflux_estimates['location'].apply(lambda s: int(s.split('__')[1].split('m_')[0]))\n",
    "mean_lhflux_estimates['tower'] = mean_lhflux_estimates['location'].apply(lambda s: s.split('_')[-1])\n",
    "mean_lhflux_estimates\n",
    "\n",
    "lhflux_estimates_chart = alt.Chart(mean_lhflux_estimates).transform_fold(\n",
    "    ['5min', '10min', '30min', '60min', '120min']\n",
    ").mark_point().encode(\n",
    "    alt.X(\"key:O\").sort(['5min', '10min', '30min', '60min', '120min']).title(\"Reynolds Avg Length\").axis(None).title(None),\n",
    "    alt.Y(\"value:Q\").scale(zero=False).title(\"Mean w'q' (g/m^2/s)\"),\n",
    "    alt.Color(\"tower:N\"),\n",
    "    alt.Facet(\"height:O\").title(None)\n",
    ").properties(height = 100)\n",
    "\n",
    "lhflux_estimates_chart_limited = alt.Chart(mean_lhflux_estimates[mean_lhflux_estimates.height.isin([3,10,15,20])]).transform_fold(\n",
    "    ['5min', '10min', '30min', '60min', '120min']\n",
    ").mark_point().encode(\n",
    "    alt.X(\"key:O\").sort(['5min', '10min', '30min', '60min', '120min']).title(\"Reynolds Avg Length\"),\n",
    "    alt.Y(\"value:Q\").scale(zero=False).title(\"<w'q'> (g/m^2/s)\"),\n",
    "    alt.Color(\"tower:N\"),\n",
    "    alt.Facet(\"height:O\").title(\"Measurement height (m)\")\n",
    ").properties(height = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lhflux_estimates_chart_limited.save(\"generalexam_reavg_mean_lhflux.png\", ppi=200)\n",
    "lhflux_estimates_chart_limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds_5min_sum_lhflux = sos_ds5min[lhflux_vars].to_dataframe().sum()*60*5/density_water.magnitude\n",
    "sos_ds_10min_sum_lhflux = sos_ds10min[lhflux_vars].to_dataframe().sum()*60*10/density_water.magnitude\n",
    "sos_ds_30min_sum_lhflux = sos_ds30min[lhflux_vars].to_dataframe().sum()*60*30/density_water.magnitude\n",
    "sos_ds_60min_sum_lhflux = sos_ds60min[lhflux_vars].to_dataframe().sum()*60*60/density_water.magnitude\n",
    "sos_ds_120min_sum_lhflux = sos_ds120min[lhflux_vars].to_dataframe().sum()*60*120/density_water.magnitude\n",
    "\n",
    "sum_lhflux_estimates = pd.concat([\n",
    "    pd.DataFrame(sos_ds_5min_sum_lhflux).rename(columns={0:'5min'}),\n",
    "    pd.DataFrame(sos_ds_10min_sum_lhflux).rename(columns={0:'10min'}),\n",
    "    pd.DataFrame(sos_ds_30min_sum_lhflux).rename(columns={0:'30min'}),\n",
    "    pd.DataFrame(sos_ds_60min_sum_lhflux).rename(columns={0:'60min'}),\n",
    "    pd.DataFrame(sos_ds_120min_sum_lhflux).rename(columns={0:'120min'}),\n",
    "], axis=1)\n",
    "sum_lhflux_estimates.index.name = 'location'\n",
    "sum_lhflux_estimates = sum_lhflux_estimates.reset_index()\n",
    "sum_lhflux_estimates\n",
    "sum_lhflux_estimates['height'] = sum_lhflux_estimates['location'].apply(lambda s: int(s.split('__')[1].split('m_')[0]))\n",
    "sum_lhflux_estimates['tower'] = sum_lhflux_estimates['location'].apply(lambda s: s.split('_')[-1])\n",
    "sum_lhflux_estimates\n",
    "cumsub_estimates_chart = alt.Chart(sum_lhflux_estimates).transform_fold(\n",
    "    ['5min', '10min', '30min', '60min', '120min']\n",
    ").mark_point().encode(\n",
    "    alt.X(\"key:O\").sort(['5min', '10min', '30min', '60min', '120min']).title(\"Reynolds Avg Length\"),\n",
    "    alt.Y(\"value:Q\").scale(zero=True).title(\"Sublimation (mm)\"),\n",
    "    alt.Color(\"tower:N\"),\n",
    "    alt.Facet(\"height:O\", columns=3).title(\"Instrument height (m)\")\n",
    ").properties(height = 100, width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsub_estimates_chart.save(\"cum_sub_with_re_avg_length.png\", ppi = 200)\n",
    "cumsub_estimates_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shflux_vars =[\n",
    "    # 'w_tc__1m_uw',\n",
    "    'w_tc__3m_uw',\n",
    "    'w_tc__10m_uw',\n",
    "    # 'w_tc__1m_ue',\n",
    "    'w_tc__3m_ue',\n",
    "    'w_tc__10m_ue',\n",
    "    # 'w_tc__1m_d',\n",
    "    'w_tc__3m_d',\n",
    "    'w_tc__10m_d',\n",
    "    # 'w_tc__2m_c',\n",
    "    'w_tc__3m_c',\n",
    "    'w_tc__5m_c',\n",
    "    'w_tc__10m_c',\n",
    "    'w_tc__15m_c',\n",
    "    'w_tc__20m_c',\n",
    "]\n",
    "sos_ds_5min_mean_shflux = sos_ds5min[shflux_vars].to_dataframe().mean()\n",
    "sos_ds_10min_mean_shflux = sos_ds10min[shflux_vars].to_dataframe().mean()\n",
    "sos_ds_30min_mean_shflux = sos_ds30min[shflux_vars].to_dataframe().mean()\n",
    "sos_ds_60min_mean_shflux = sos_ds60min[shflux_vars].to_dataframe().mean()\n",
    "sos_ds_120min_mean_shflux = sos_ds120min[shflux_vars].to_dataframe().mean()\n",
    "\n",
    "mean_shflux_estimates = pd.concat([\n",
    "    pd.DataFrame(sos_ds_5min_mean_shflux).rename(columns={0:'5min'}),\n",
    "    pd.DataFrame(sos_ds_10min_mean_shflux).rename(columns={0:'10min'}),\n",
    "    pd.DataFrame(sos_ds_30min_mean_shflux).rename(columns={0:'30min'}),\n",
    "    pd.DataFrame(sos_ds_60min_mean_shflux).rename(columns={0:'60min'}),\n",
    "    pd.DataFrame(sos_ds_120min_mean_shflux).rename(columns={0:'120min'}),\n",
    "], axis=1)\n",
    "mean_shflux_estimates.index.name = 'location'\n",
    "mean_shflux_estimates = mean_shflux_estimates.reset_index()\n",
    "mean_shflux_estimates\n",
    "mean_shflux_estimates['height'] = mean_shflux_estimates['location'].apply(lambda s: int(s.split('__')[1].split('m_')[0]))\n",
    "mean_shflux_estimates['tower'] = mean_shflux_estimates['location'].apply(lambda s: s.split('_')[-1])\n",
    "mean_shflux_estimates\n",
    "shflux_estimates_chart = alt.Chart(mean_shflux_estimates).transform_fold(\n",
    "    ['5min', '10min', '30min', '60min', '120min']\n",
    ").mark_point().encode(\n",
    "    alt.X(\"key:O\").sort(['5min', '10min', '30min', '60min', '120min']).title(\"Reynolds Avg Length\"),\n",
    "    alt.Y(\"value:Q\").scale(zero=False).title(\"Mean w'T' (C*m/s)\"),\n",
    "    alt.Color(\"tower:N\"),\n",
    "    alt.Facet(\"height:O\").title(None).header(labelFontSize=0)\n",
    ").properties(height = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsub_estimates_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lhflux_estimates_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsub_estimates_chart & lhflux_estimates_chart & shflux_estimates_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "(sos_ds5min['w_h2o__3m_c'].to_dataframe().cumsum()*60*5/density_water.magnitude).plot(ax=ax)\n",
    "(sos_ds10min['w_h2o__3m_c'].to_dataframe().cumsum()*60*10/density_water.magnitude).plot(ax=ax)\n",
    "(sos_ds30min['w_h2o__3m_c'].to_dataframe().cumsum()*60*30/density_water.magnitude).plot(ax=ax)\n",
    "(sos_ds60min['w_h2o__3m_c'].to_dataframe().cumsum()*60*60/density_water.magnitude).plot(ax=ax)\n",
    "(sos_ds120min['w_h2o__3m_c'].to_dataframe().cumsum()*60*120/density_water.magnitude).plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "(sos_ds5min['w_h2o__10m_c'].to_dataframe().cumsum()*60*5/density_water.magnitude).plot(ax=ax)\n",
    "(sos_ds10min['w_h2o__10m_c'].to_dataframe().cumsum()*60*10/density_water.magnitude).plot(ax=ax)\n",
    "(sos_ds30min['w_h2o__10m_c'].to_dataframe().cumsum()*60*30/density_water.magnitude).plot(ax=ax)\n",
    "(sos_ds60min['w_h2o__10m_c'].to_dataframe().cumsum()*60*60/density_water.magnitude).plot(ax=ax)\n",
    "(sos_ds120min['w_h2o__10m_c'].to_dataframe().cumsum()*60*120/density_water.magnitude).plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add additional variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add snow depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open snow depth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "towerc_snowdepth_dataset = xr.open_dataset(\"~/Development/data/sublimationofsnow/lidar_snow_depth/C_l2.nc\")\n",
    "towerc_lidar_snowdepth_da = towerc_snowdepth_dataset.resample(time='1440Min').median()['surface']\n",
    "towerc_lidar_snowdepth_da = towerc_lidar_snowdepth_da.interpolate_na(dim = 'time', method='linear')\n",
    "towerc_lidar_snowdepth_da = towerc_lidar_snowdepth_da.where(towerc_lidar_snowdepth_da > 0, 0)\n",
    "towerc_lidar_snowdepth_upsample_da = towerc_lidar_snowdepth_da.resample(time = '30Min').pad()\n",
    "towerc_lidar_snowdepth_da.plot()\n",
    "towerc_lidar_snowdepth_upsample_da.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "towerc_snowdepth_df = towerc_lidar_snowdepth_upsample_da.loc[\n",
    "    sos_ds30min.time.min():sos_ds30min.time.max()\n",
    "].to_dataframe()\n",
    "towerc_snowdepth_df = towerc_snowdepth_df.reset_index()\n",
    "sos_ds30min['SnowDepth_c'] = (['time'],  towerc_snowdepth_df.surface.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "towerd_snowdepth_dataset = xr.open_dataset(\"~/Development/data/sublimationofsnow/lidar_snow_depth/D_from_D_l6.nc\")\n",
    "towerd_lidar_snowdepth_da = towerd_snowdepth_dataset.resample(time='1440Min').median()['surface']\n",
    "towerd_lidar_snowdepth_da = towerd_lidar_snowdepth_da.interpolate_na(dim = 'time', method='linear')\n",
    "towerd_lidar_snowdepth_da = towerd_lidar_snowdepth_da.where(towerd_lidar_snowdepth_da > 0, 0)\n",
    "towerd_lidar_snowdepth_upsample_da = towerd_lidar_snowdepth_da.resample(time = '30Min').pad()\n",
    "towerd_lidar_snowdepth_da.plot()\n",
    "towerd_lidar_snowdepth_upsample_da.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "towerd_snowdepth_df = towerd_lidar_snowdepth_upsample_da.loc[\n",
    "    sos_ds30min.time.min():sos_ds30min.time.max()\n",
    "].to_dataframe()\n",
    "towerd_snowdepth_df = towerd_snowdepth_df.reset_index()\n",
    "sos_ds30min['SnowDepth_d'] = (['time'],  towerd_snowdepth_df.surface.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds30min['SnowDepth_c'].plot(label='SnowDepth_c')\n",
    "sos_ds30min['SnowDepth_d'].plot(label='SnowDepth_d')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add/calculate longwave radiation and surface temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds30min = variables.add_longwave_radiation(sos_ds30min)\n",
    "sos_ds30min = variables.add_surface_temps(sos_ds30min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean $T_s$ variables before proceeding with other calculations\n",
    "\n",
    "(as of Feb 20, 2023, using NCAR's QC data release, we found a single $T_s$ outlier.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tsurf_vars = [v for v in sos_ds30min.data_vars if v.startswith('Tsurf')]\n",
    "Tsurf_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in Tsurf_vars:\n",
    "    print(f\"{var}\\t {round(sos_ds30min[var].min().item(), 1)}\\t{round(sos_ds30min[var].max().item(), 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in Tsurf_vars:\n",
    "    sos_ds30min[var] = sos_ds30min[var].where(\n",
    "        (sos_ds30min[var].values > -40)\n",
    "        &\n",
    "        (sos_ds30min[var].values < 40)\n",
    "    ).interpolate_na(\n",
    "        dim='time', \n",
    "        method='linear'\n",
    "    ).where(\n",
    "        ~ sos_ds30min[var].isnull()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in Tsurf_vars:\n",
    "    print(f\"{var}\\t {round(sos_ds30min[var].min().item(), 1)}\\t{round(sos_ds30min[var].max().item(), 1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2,sharex=True,sharey=True)\n",
    "sos_ds30min['Tsurf_c'].plot(ax=axes[0][0])\n",
    "sos_ds30min['Tsurf_d'].plot(ax=axes[0][1])\n",
    "sos_ds30min['Tsurf_ue'].plot(ax=axes[1][0])\n",
    "sos_ds30min['Tsurf_uw'].plot(ax=axes[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add $T_v, \\theta, \\theta_v, \\textbf{tke}, R_i, L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds30min = variables.add_potential_virtual_temperatures(sos_ds30min)\n",
    "sos_ds30min = variables.add_surface_potential_virtual_temperatures(sos_ds30min)\n",
    "sos_ds30min = variables.add_tke(sos_ds30min)\n",
    "sos_ds30min = variables.add_gradients_and_ri(sos_ds30min)\n",
    "sos_ds30min = variables.add_shear_velocity_and_obukhov_length(sos_ds30min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add decoupling metric, from Peltola et al. (2021).\n",
    "\n",
    "We adjust the height value for snow depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoupling_metric(z, sigma_w, N):\n",
    "    \"\"\"Calculate the decoupling metric as described in Peltola et al (2021).\n",
    "\n",
    "    Peltola, O., Lapo, K., & Thomas, C. K. (2021). A PhysicsBased Universal Indicator for Vertical Decoupling and Mixing Across Canopies Architectures and Dynamic Stabilities. Geophysical Research Letters, 48(5), e2020GL091615. https://doi.org/10.1029/2020GL091615\n",
    "    \n",
    "    Args:\n",
    "        z (float): height of measurements \n",
    "        sigma_w (float): standarad deviation of w, vertical velocity\n",
    "        N (float): Brunt-Vaisala frequency\n",
    "    \"\"\"\n",
    "    # Brunt Vaisala frequency estimated using the bulk theta gradient\n",
    "    # N = np.sqrt(\n",
    "    #     g * (theta_e - theta_mean) / theta_mean\n",
    "    # )\n",
    "\n",
    "    Lb = sigma_w / N\n",
    "    omega = Lb / ( np.sqrt(2)*z )\n",
    "    return omega\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "### OLD METHOD USING GRADIENT-BASED N\n",
    "#######################################################################\n",
    "from metpy.calc import brunt_vaisala_frequency\n",
    "\n",
    "sigma_w = np.sqrt(sos_ds30min['w_w__3m_c']).values\n",
    "pot_temps = sos_ds30min[[\n",
    "    'Tpot_2m_c', \n",
    "    'Tpot_3m_c', \n",
    "    'Tpot_4m_c', \n",
    "    'Tpot_5m_c', \n",
    "    'Tpot_6m_c'\n",
    "]].to_stacked_array(\n",
    "    'z', ['time']\n",
    ").values\n",
    "\n",
    "snow_depth_values = sos_ds30min['SnowDepth_c']\n",
    "snow_depth_values_reshaped = np.repeat(sos_ds30min['SnowDepth_c'].values, 5).reshape(-1, 5)\n",
    "\n",
    "heights = np.full(pot_temps.shape,  [ 2,    3,    4,    5,    6])\n",
    "heights_adjusted = heights - snow_depth_values_reshaped\n",
    "brunt_vaisala_values = [ Ns[1] for Ns in \n",
    "    brunt_vaisala_frequency( \n",
    "        heights_adjusted * units(\"meters\"),\n",
    "        pot_temps * units(\"celsius\"), \n",
    "        vertical_dim=1\n",
    "    ).magnitude   \n",
    "]\n",
    "z = np.full(sigma_w.shape, 3) - snow_depth_values.values\n",
    "\n",
    "# decoupling_metric(z, sigma_w, N)\n",
    "print(len(z))\n",
    "print(len(sigma_w))\n",
    "print(len(brunt_vaisala_values))\n",
    "\n",
    "omegas = decoupling_metric(z, sigma_w, brunt_vaisala_values)\n",
    "sos_ds30min['omega_3m_c'] = (['time'],  omegas)\n",
    "print(len(omegas))\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "### NEW METHOD USING BULK N\n",
    "#######################################################################\n",
    "# sigma_w = np.sqrt(sos_ds30min['w_w__3m_c']).values\n",
    "\n",
    "# snow_depth_values = sos_ds30min['SnowDepth_c'].values\n",
    "\n",
    "# air_temp_height = 3\n",
    "# heights_adjusted = air_temp_height - snow_depth_values\n",
    "# surf_pottemp = sos_ds30min['Tsurfpot_c'].values\n",
    "# air_pottemp = sos_ds30min['Tpot_3m_c'].values\n",
    "# surfacealyer_avg_pottemp = 0.5*(air_pottemp + surf_pottemp)\n",
    "# bulk_brunt_vaisala_value = np.sqrt(\n",
    "#     metpy.constants.earth_gravity.magnitude.item()*(air_pottemp - surfacealyer_avg_pottemp) \n",
    "#     /\n",
    "#     (surfacealyer_avg_pottemp * heights_adjusted)\n",
    "# )\n",
    "# z = np.full(sigma_w.shape, 3) - snow_depth_values\n",
    "\n",
    "# # decoupling_metric(z, sigma_w, N)\n",
    "# print(len(z))\n",
    "# print(len(sigma_w))\n",
    "# print(len(bulk_brunt_vaisala_value))\n",
    "\n",
    "# omegas = decoupling_metric(z, sigma_w, bulk_brunt_vaisala_value)\n",
    "# sos_ds30min['omega_3m_c'] = (['time'],  omegas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(sos_ds30min['omega_3m_c'])\n",
    "plt.xlim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net LW and Net SW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds30min['Rlw_net_9m_d'] = sos_ds30min['Rlw_in_9m_d'] - sos_ds30min['Rlw_out_9m_d']\n",
    "sos_ds30min['Rsw_net_9m_d'] = sos_ds30min['Rsw_in_9m_d'] - sos_ds30min['Rsw_out_9m_d']\n",
    "\n",
    "sos_ds30min['Rlw_net_9m_uw'] = sos_ds30min['Rlw_in_9m_uw'] - sos_ds30min['Rlw_out_9m_uw']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net Radiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds30min['Rnet_9m_d'] = (\n",
    "    (sos_ds30min['Rsw_in_9m_d'] + sos_ds30min['Rlw_in_9m_d'])\n",
    "    -\n",
    "    (sos_ds30min['Rsw_out_9m_d'] + sos_ds30min['Rlw_out_9m_d'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in [\n",
    "    'Tsurfmixingratio_c',\n",
    "    'mixingratio_2m_c',\n",
    "    'mixingratio_3m_c',\n",
    "    'mixingratio_4m_c',\n",
    "    'mixingratio_5m_c',\n",
    "    'mixingratio_6m_c',\n",
    "    'mixingratio_7m_c',\n",
    "    'mixingratio_8m_c',\n",
    "    'mixingratio_9m_c',\n",
    "    'mixingratio_10m_c',\n",
    "    'mixingratio_11m_c',\n",
    "    'mixingratio_12m_c',\n",
    "    'mixingratio_13m_c',\n",
    "    'mixingratio_14m_c',\n",
    "    'mixingratio_15m_c',\n",
    "    'mixingratio_16m_c',\n",
    "    'mixingratio_17m_c',\n",
    "    'mixingratio_18m_c',\n",
    "    'mixingratio_19m_c',\n",
    "    'mixingratio_20m_c',\n",
    "]:\n",
    "    new_var_name = var.replace('mixingratio', 'specifichumidity')\n",
    "    result = specific_humidity_from_mixing_ratio(\n",
    "        sos_ds30min[var]*units('g/g')\n",
    "    )\n",
    "    sos_ds30min[new_var_name] = (['time'], result.values)\n",
    "    sos_ds30min[new_var_name] = sos_ds30min[new_var_name].assign_attrs(units=str(result.pint.units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply sensible and latent heat flux corrections (lots of citations, see here: https://www.eol.ucar.edu/content/corrections-sensible-and-latent-heat-flux-measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rho_d = 1293*units('g/m^3') # density of dry air\n",
    "# rho_a = 1287*units('g/m^3') #density of moist air\n",
    "# rho_v = 600*units('g/m^3') #density of water vapor\n",
    "# mu = 1/0.622\n",
    "# rho_w = metpy.constants.density_water\n",
    "\n",
    "# def corrected_sensible_heat_flux(\n",
    "#     w_tc_, #sonic_temperature_flux\n",
    "#     w_h2o_, #h2o_flux\n",
    "#     Q, #specific_humidity\n",
    "#     T, #absolute_temperature\n",
    "#     A, #A\n",
    "#     Mr, #mean_mixing_ratio\n",
    "# ):\n",
    "#     Ctc = 0.51*(1 + Q*(mu-1))\n",
    "#     part1 = w_tc_ - Ctc*(T/rho_a)*A*w_h2o_\n",
    "#     part2 = 1 + Ctc*Q\n",
    "#     return part1 / part2\n",
    "\n",
    "# def corrected_latent_heat_flux(\n",
    "#     w_tc_, #sonic_temperature_flux\n",
    "#     w_h2o_, #h2o_flux\n",
    "#     Q, #specific_humidity\n",
    "#     T, #absolute_temperature\n",
    "#     A, #A\n",
    "#     Mr, #mean_mixing_ratio\n",
    "# ):\n",
    "#     Ctc = 0.51*(1 + Q*(mu-1))\n",
    "#     return (1 + mu*Mr)* (\n",
    "#         (A/rho_d)*w_h2o_ + (Mr/T)*(w_tc_)\n",
    "#     ) / (\n",
    "#         1 + Ctc*Q\n",
    "#     )\n",
    "\n",
    "# def wpl_corrected_latent_heat_flux(\n",
    "#     w_tc_, #sonic_temperature_flux\n",
    "#     w_h2o_, #h2o_flux\n",
    "#     Q, #specific_humidity\n",
    "#     T, #absolute_temperature\n",
    "#     A, #A\n",
    "#     Mr, #mean_mixing_ratio\n",
    "# ):\n",
    "#     return (\n",
    "#         1 + mu*Mr\n",
    "#     )*(\n",
    "#         w_h2o_ + (rho_v/T)*w_tc_\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sos_ds30min['w_tc__3m_c_corrected'] = corrected_sensible_heat_flux(\n",
    "#     sos_ds30min['w_tc__3m_c']*units(\"(m/s)(degC)\"),\n",
    "#     sos_ds30min['w_h2o__3m_c']*units(\"(m/s)(g/m^3)\"),\n",
    "#     sos_ds30min['specifichumidity_3m_c']*units(\"g/g\"),\n",
    "#     sos_ds30min['T_3m_c'] * units.degC,\n",
    "#     1, #A\n",
    "#     sos_ds30min['mixingratio_3m_c']*units(\"g/g\")\n",
    "# )\n",
    "# sos_ds30min['w_h2o__3m_c_corrected'] = rho_d*corrected_latent_heat_flux(\n",
    "#     sos_ds30min['w_tc__3m_c']*units(\"(m/s)(degC)\"),\n",
    "#     sos_ds30min['w_h2o__3m_c']*units(\"(m/s)(g/m^3)\"),\n",
    "#     sos_ds30min['specifichumidity_3m_c']*units(\"g/g\"),\n",
    "#     sos_ds30min['T_3m_c'] * units.degC,\n",
    "#     1, #A\n",
    "#     sos_ds30min['mixingratio_3m_c']*units(\"g/g\")\n",
    "# )\n",
    "\n",
    "# # WHY DO I HAVE TO MULTIPLE DENSITY_WATER HERE???\n",
    "# sos_ds30min['w_h2o__3m_c_wplcorrected'] = wpl_corrected_latent_heat_flux(\n",
    "#     sos_ds30min['w_tc__3m_c']*units(\"(m/s)(degC)\"),\n",
    "#     sos_ds30min['w_h2o__3m_c']*units(\"(m/s)(g/m^3)\"),\n",
    "#     sos_ds30min['specifichumidity_3m_c']*units(\"g/g\"),\n",
    "#     sos_ds30min['T_3m_c'] * units.degC,\n",
    "#     1, #A\n",
    "#     sos_ds30min['mixingratio_3m_c']*units(\"g/g\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (sos_ds30min['w_h2o__3m_c']).sel(time=slice('20221201', '20221215')).plot(label=\"Measured\")\n",
    "# (-0.1*sos_ds30min['w_h2o__3m_c_corrected']).sel(time=slice('20221201', '20221215')).plot(label='Corrected')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sos_ds30min['w_tc__3m_c'].sel(time=slice('20221201', '20221215')).plot()\n",
    "# sos_ds30min['w_tc__3m_c_corrected'].sel(time=slice('20221201', '20221215')).plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tidy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sublimpy import tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_df = tidy.get_tidy_dataset(sos_ds30min, list(sos_ds30min.data_vars))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which variables did not get a \"measurement\" name assigned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_with_no_measurement = tidy_df[tidy_df.measurement.apply(lambda x: x is None)].variable.unique()\n",
    "variables_with_no_measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(tidy_df.variable.unique()).difference(set(list(sos_ds30min.data_vars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penman-Monteith and Priestley-Taylor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather the data we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyet_src = tidy_df[tidy_df.variable.isin([\n",
    "    'Rnet_9m_d',\n",
    "    'airdensity_3m_c',\n",
    "    'Tsurfvaporpressure_c',\n",
    "    'vaporpressure_3m_c',\n",
    "    'T_3m_c',\n",
    "    'spd_3m_c',\n",
    "    'P_10m_c'\n",
    "])].pivot_table(\n",
    "    index=['time'],\n",
    "    values='value',\n",
    "    columns='variable'\n",
    "# Calculate means across 30min periods, mins and maxes for the required variables too\n",
    ").reset_index()\n",
    "pyet_src.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From PyET code\n",
    "# Equation for calculating the latent heat of vaporization from:\n",
    "# Richard G Allen, Luis S Pereira, Dirk Raes, Martin Smith, and others. Crop evapotranspiration-Guidelines for computing crop \n",
    "# water requirements-FAO Irrigation and drainage paper 56. Fao, Rome, 300(9):D05109, 1998.\n",
    "pyet_src['L_v estimated'] = 2.501 - 0.002361 * pyet_src['T_3m_c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_delta_alduchov(t_in_c):\n",
    "# Taking the derivative of the Alduchov curve\n",
    "# returns in kPa\n",
    "    numerator = 37836.5*np.exp((22.587*t_in_c)/(273.86+t_in_c))\n",
    "    denominator = (t_in_c + 273.86)**2\n",
    "    # 0.1 to convert from millibars to kPa\n",
    "    return 0.1*numerator/denominator\n",
    "\n",
    "def calc_psychrometric_constant(C_p, P, L_v):\n",
    "    # C_p in MJ/kg/C\n",
    "    # P in kPa\n",
    "    # L_v in MJ/kg\n",
    "    # From wikipedia\n",
    "    # https://en.wikipedia.org/wiki/Psychrometric_constant#cite_note-1\n",
    "    # which cites this:\n",
    "    #  Allen, R.G.; Pereira, L.S.; Raes, D.; Smith, M. (1998). Crop EvapotranspirationGuidelines for Computing Crop Water Requirements. \n",
    "    # FAO Irrigation and drainage paper 56. Rome, Italy: Food and Agriculture Organization of the United Nations. ISBN 92-5-104219-5. Retrieved 2007-10-08.\n",
    "    return C_p*P/(0.622 * L_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_p = 1.013 * 10 ** -3  # MJ/kg/K\n",
    "\n",
    "def penman_monteith(\n",
    "    energy_available, # MJ/m^2/day\n",
    "    density_air, # kg/m^3\n",
    "    vapor_pressure_surface, # kPa\n",
    "    vapor_pressure_air, # kPa\n",
    "    temp_air, # C\n",
    "    pressure, # kPa\n",
    "    wind_speed, # m/s\n",
    "    L_v\n",
    "):\n",
    "    A = energy_available\n",
    "    rho_a = density_air\n",
    "    e_s = vapor_pressure_surface\n",
    "    e_a = vapor_pressure_air\n",
    "    \n",
    "    # delta = pyet.calc_vpc(temp_air)\n",
    "    delta = calc_delta_alduchov(temp_air)\n",
    "    y = calc_psychrometric_constant(C_p, pressure, L_v)\n",
    "    r_a = pyet.calc_res_aero(wind_speed) # I should replace this with some better method for estimating atmospheric resistance - i.e. see Mahrt and Vickers, 2005, i.e. r_a = 1/(wind_speed * C_q)\n",
    "    r_s = 0\n",
    "\n",
    "    numerator = delta*A + rho_a*86400*C_p*(e_s - e_a)/r_a\n",
    "    denominator = delta + y*(1 + r_s / r_a)\n",
    "    return (1/L_v)*(numerator/denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyet_src['pm results'] = penman_monteith(\n",
    "    (pyet_src['Rnet_9m_d'].to_xarray() * units(\"W/m^2\")).pint.to(\"MJ/m^2/day\").values,\n",
    "    pyet_src['airdensity_3m_c'].values,\n",
    "    (pyet_src['Tsurfvaporpressure_c'].to_xarray() * units(\"Pa\")).pint.to(\"kPa\").values,\n",
    "    (pyet_src['vaporpressure_3m_c'].to_xarray() * units(\"Pa\")).pint.to(\"kPa\").values,\n",
    "    pyet_src['T_3m_c'].values,\n",
    "    (pyet_src['P_10m_c'].to_xarray() * units(\"millibars\")).pint.to(\"kPa\").values,\n",
    "    pyet_src['spd_3m_c'].values,\n",
    "    pyet_src['L_v estimated'].values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert from mm/day to g/m^2/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyet_src['pm results'] = pyet_src['pm results']/(86.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyet_src.set_index('time')['pm results'].plot(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyet_src.set_index('time')['pm results'].cumsum().plot(alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add to tidy df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill missing timestamps with NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_values = tidy_df.query(\"variable == 'spd_3m_c'\").merge(\n",
    "    pyet_src[['time', 'pm results']],\n",
    "    on='time',\n",
    "    how='left'\n",
    ")['pm results'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_df = tidy.tidy_df_add_variable(\n",
    "    tidy_df,\n",
    "    pm_values,\n",
    "    f'w_h2o__3m_c predicted (Penman Monteith)',\n",
    "    'w_h2o_',\n",
    "    3,\n",
    "    'c'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOST (my iterative solution, in most.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the input data we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT VARIABLES\n",
    "VARIABLES = [\n",
    "    ## Input Variables\n",
    "    'spd_3m_c',\n",
    "    'Tpot_3m_c',\n",
    "    'Tsurfpot_c',\n",
    "    'airdensity_3m_c',\n",
    "    'mixingratio_3m_c',\n",
    "    'Tsurfmixingratio_c',\n",
    "    'T_3m_c',\n",
    "    ## Measurement Variables\n",
    "    'w_h2o__3m_c',\n",
    "    'w_tc__3m_c',\n",
    "    'u*_3m_c',\n",
    "    'Ri_3m_c',\n",
    "]\n",
    "print([ v for v in tidy_df.variable.unique() if v in VARIABLES ])\n",
    "\n",
    "# CREATE WIDE DATAFRAME\n",
    "variables_df = tidy_df[tidy_df.variable.isin(VARIABLES)].pivot_table(\n",
    "    values = 'value',\n",
    "    index = 'time',\n",
    "    columns='variable'\n",
    ").reset_index()\n",
    "\n",
    "# MAKE CONVERSIONS\n",
    "# convert from C to K\n",
    "variables_df['T_3m_c'] = variables_df['T_3m_c'] + 273.15\n",
    "variables_df['Tpot_3m_c'] = variables_df['Tpot_3m_c'] + 273.15\n",
    "variables_df['Tsurfpot_c'] = variables_df['Tsurfpot_c'] + 273.15\n",
    "# comes in units of g/g\n",
    "variables_df['specifichumidity_3m_c'] = specific_humidity_from_mixing_ratio(\n",
    "    xr.DataArray(variables_df['mixingratio_3m_c'])*units('g/g')\n",
    ").pint.to('g/kg').values\n",
    "# comes in units of g/g,  solution requires it in units of g/kg\n",
    "variables_df['specifichumidity_surface_c'] = specific_humidity_from_mixing_ratio(\n",
    "    xr.DataArray(variables_df['Tsurfmixingratio_c'])*units('g/g')\n",
    ").pint.to('g/kg').values\n",
    "\n",
    "\n",
    "\n",
    "# CREATE MEASUREMENT HEIGHT VARIABLES\n",
    "variables_df['measurement_height'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../paper1/')\n",
    "from most import MOST, StabilityFunctionBrutsaert1982\n",
    "\n",
    "most_config_dict = {\n",
    "    0.00001: MOST(StabilityFunctionBrutsaert1982(), snow_surface_roughness = 0.00001),\n",
    "    0.00005: MOST(StabilityFunctionBrutsaert1982(), snow_surface_roughness = 0.00005),\n",
    "    0.0001: MOST(StabilityFunctionBrutsaert1982(), snow_surface_roughness = 0.0001),\n",
    "    0.0005: MOST(StabilityFunctionBrutsaert1982(), snow_surface_roughness = 0.0005),\n",
    "    0.001: MOST(StabilityFunctionBrutsaert1982(), snow_surface_roughness = 0.001),\n",
    "    0.005: MOST(StabilityFunctionBrutsaert1982(), snow_surface_roughness = 0.005),\n",
    "}\n",
    "\n",
    "for z0, this_most in most_config_dict.items():\n",
    "    L_solutions, u_friction_solutions, H_solutions, E_solutions = this_most.solve(\n",
    "        variables_df['spd_3m_c'],\n",
    "        variables_df['Tpot_3m_c'],\n",
    "        variables_df['Tsurfpot_c'],\n",
    "        variables_df['airdensity_3m_c'],\n",
    "        variables_df['specifichumidity_3m_c'],\n",
    "        variables_df['specifichumidity_surface_c'],\n",
    "        variables_df['T_3m_c'],\n",
    "        variables_df['measurement_height']\n",
    "    )\n",
    "\n",
    "    variables_df[f'L_solution_z0_{str(z0)}'] = L_solutions\n",
    "    variables_df[f'u_friction_solution_z0_{str(z0)}'] = u_friction_solutions\n",
    "    variables_df[f'H_solution_z0_{str(z0)}'] = H_solutions\n",
    "    variables_df[f'E_solution_z0_{str(z0)}'] = E_solutions\n",
    "\n",
    "    # convert from W/m^2 to C*m/s\n",
    "    variables_df[f'H_solution_z0_{str(z0)}'] = (variables_df[f'H_solution_z0_{str(z0)}']/(variables_df['airdensity_3m_c']*0.718*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add nans for the model results where there are missing timesteps (As compared to the tidy dataset of measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_df['time'] = pd.to_datetime(variables_df['time'])\n",
    "\n",
    "variables_df = pd.merge(\n",
    "    tidy_df.query(\"variable == 'w_h2o__3m_c'\")[['time']],\n",
    "    variables_df,\n",
    "    on='time',\n",
    "    how='outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add to tidy df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUR SOLUTIONS\n",
    "for z0 in most_config_dict.keys():\n",
    "    tidy_df = tidy.tidy_df_add_variable(\n",
    "        tidy_df,  \n",
    "        variables_df[f'L_solution_z0_{z0}'], \n",
    "        f'L predicted ({z0})',\n",
    "        'Obukhov length', \n",
    "        3,\n",
    "        'c'\n",
    "    )\n",
    "\n",
    "    tidy_df = tidy.tidy_df_add_variable(\n",
    "        tidy_df, \n",
    "        variables_df[f'u_friction_solution_z0_{z0}'], \n",
    "        f'u*_3m_c predicted ({z0})',\n",
    "        'shear velocity', \n",
    "        3,\n",
    "        'c'\n",
    "    )\n",
    "\n",
    "    tidy_df = tidy.tidy_df_add_variable(\n",
    "        tidy_df, \n",
    "        variables_df[f'H_solution_z0_{z0}'], \n",
    "        f'w_tc__3m_c predicted ({z0})',\n",
    "        'w_tc_', \n",
    "        3,\n",
    "        'c'\n",
    "    )\n",
    "\n",
    "    tidy_df = tidy.tidy_df_add_variable(\n",
    "        tidy_df, \n",
    "        variables_df[f'E_solution_z0_{z0}'], \n",
    "        f'w_h2o__3m_c predicted ({z0})',\n",
    "        'w_h2o_', \n",
    "        3,\n",
    "        'c'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = tidy_df[tidy_df.measurement == 'w_h2o_'].pivot_table(\n",
    "    values = 'value',\n",
    "    index = 'time',\n",
    "    columns='variable'\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "src['w_h2o__5m_c'].cumsum().plot(color='black', label='measured')\n",
    "for col in [c for c in src.columns if 'predicted' in c]:\n",
    "    src[col].cumsum().plot(label = f\"predicted, z0 == {col.split(' ')[-1][1:-1]}\")\n",
    "plt.ylim(-30,400)\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lah | grep parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLANAR_FIT:    \n",
    "    tidy_df.to_parquet(f'tidy_df_{start_date}_{end_date}_planar_fit.parquet', index=False)\n",
    "else:\n",
    "    tidy_df.to_parquet(f'tidy_df_{start_date}_{end_date}_noplanar_fit.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lah | grep parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_ds30min['w_h2o__3m_c'].isnull().sum().item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
